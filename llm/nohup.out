Inference:   3%|▎         | 1/33 [00:14<07:37, 14.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:25<06:23, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [00:38<06:24, 12.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [00:54<06:41, 13.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [01:24<09:17, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [01:35<07:35, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [01:44<06:08, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [02:12<07:46, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [02:36<08:06, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:02<08:24, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [03:13<06:52, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [03:35<06:51, 19.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [04:07<07:51, 23.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [04:47<09:01, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [05:18<08:42, 29.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [05:39<07:36, 26.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [05:59<06:36, 24.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [06:16<05:35, 22.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [06:34<04:55, 21.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [07:06<05:16, 24.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [07:24<04:27, 22.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [07:34<03:26, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [07:53<03:07, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [08:20<03:10, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [08:31<02:25, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [09:24<03:20, 28.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [10:24<03:48, 38.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [10:43<02:41, 32.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [11:15<02:08, 32.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [11:55<01:44, 34.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [12:17<01:01, 30.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [12:31<00:25, 25.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [13:03<00:00, 27.69s/it]Inference: 100%|██████████| 33/33 [13:03<00:00, 23.75s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:11<06:11, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:23<06:19, 11.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:51<09:57, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [00:54<06:28, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:09<06:31, 13.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [01:18<05:39, 12.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [01:36<06:16, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [02:03<07:52, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [02:19<07:17, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [02:23<05:17, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [03:24<10:39, 27.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [03:46<09:35, 26.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [04:11<09:03, 25.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [04:59<10:46, 32.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [05:11<08:17, 26.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [05:17<06:04, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [05:29<04:59, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [05:47<04:47, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [06:03<04:18, 17.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [06:18<03:50, 16.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [06:29<03:16, 15.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [06:45<03:04, 15.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [07:44<05:10, 28.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [08:29<05:34, 33.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [08:48<04:21, 29.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [08:58<03:07, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [09:21<02:42, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [09:40<02:12, 22.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [10:05<01:54, 22.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [10:34<01:38, 24.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [10:47<01:03, 21.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [11:19<00:48, 24.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [11:51<00:26, 26.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [12:06<00:00, 23.26s/it]Inference: 100%|██████████| 34/34 [12:06<00:00, 21.37s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:13<07:17, 13.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:27<07:27, 13.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:52<09:52, 19.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [01:25<12:09, 24.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:36<09:28, 19.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [02:02<10:07, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [02:14<08:21, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [02:28<07:27, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [03:23<12:06, 29.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [03:33<09:12, 23.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [04:02<09:34, 24.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [04:36<10:10, 27.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [05:07<10:00, 28.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [05:14<07:23, 22.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [05:46<07:57, 25.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [06:19<08:12, 27.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [06:29<06:15, 22.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [06:39<04:56, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [07:03<05:01, 20.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [07:25<04:53, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [08:22<06:52, 31.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [08:41<05:34, 27.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [09:02<04:41, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [09:18<03:48, 22.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [09:40<03:24, 22.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [10:07<03:11, 23.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [10:43<03:11, 27.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [11:43<03:44, 37.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [12:17<03:01, 36.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [12:46<02:16, 34.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [13:16<01:38, 32.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [14:02<01:13, 36.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [14:21<00:31, 31.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [14:43<00:00, 28.73s/it]Inference: 100%|██████████| 34/34 [14:43<00:00, 26.00s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:19<13:09, 19.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:51<17:48, 26.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [01:26<19:58, 30.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:45<16:25, 25.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [01:57<12:54, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [02:24<13:45, 22.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [02:46<13:14, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [02:59<11:04, 19.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [03:18<10:39, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:42<11:12, 21.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [04:14<12:27, 24.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:32<11:10, 22.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [05:03<12:07, 25.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [05:24<11:04, 23.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [05:51<11:07, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [06:11<10:09, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [06:47<11:15, 27.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [07:47<14:50, 37.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [08:10<12:33, 32.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [08:30<10:37, 28.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [08:44<08:35, 24.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [09:06<07:53, 23.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [09:55<09:55, 31.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [10:23<09:03, 30.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [12:17<15:40, 55.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [12:53<13:13, 49.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [13:07<09:43, 38.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [13:14<06:51, 29.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [13:38<06:02, 27.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [14:03<05:24, 27.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [14:24<04:36, 25.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [14:40<03:44, 22.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [15:01<03:16, 21.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [17:01<06:50, 51.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [17:20<04:51, 41.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [18:31<05:02, 50.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [19:14<04:00, 48.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [19:24<02:26, 36.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [19:51<01:41, 33.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [20:23<01:06, 33.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [20:59<00:34, 34.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [21:13<00:00, 27.93s/it]Inference: 100%|██████████| 42/42 [21:13<00:00, 30.31s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:23<16:06, 23.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:36<11:34, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [01:23<19:55, 30.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:42<16:33, 26.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [02:16<17:58, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [02:42<16:42, 27.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [02:55<13:30, 23.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [03:11<11:45, 20.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [03:17<08:59, 16.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:43<10:14, 19.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [04:02<09:55, 19.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:33<11:24, 22.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [05:08<12:46, 26.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [05:16<09:45, 20.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [05:40<09:46, 21.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [06:17<11:22, 26.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [06:38<10:20, 24.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [06:51<08:33, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [07:04<07:10, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [07:29<07:31, 20.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [07:58<08:03, 23.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [08:10<06:35, 19.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [08:19<05:13, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [08:35<04:58, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [08:42<03:51, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [08:57<03:43, 13.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [09:43<05:54, 23.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [10:09<05:39, 24.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [10:34<05:20, 24.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [10:45<04:05, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [11:13<04:09, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [11:38<03:52, 23.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [11:57<03:20, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [12:17<02:51, 21.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [12:29<02:10, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [13:06<02:24, 24.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [14:16<03:10, 38.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [15:06<02:45, 41.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [15:30<01:48, 36.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [15:51<01:03, 31.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [16:29<00:33, 33.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [17:53<00:00, 48.59s/it]Inference: 100%|██████████| 42/42 [17:53<00:00, 25.56s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/22 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:36<12:50, 36.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 2/22 [01:12<12:05, 36.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 3/22 [01:49<11:35, 36.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 4/22 [02:11<09:13, 30.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 5/22 [02:43<08:51, 31.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 6/22 [03:05<07:27, 27.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 7/22 [03:34<07:06, 28.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 8/22 [04:34<08:56, 38.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 9/22 [11:05<32:13, 148.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 10/22 [11:16<21:14, 106.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 11/22 [11:39<14:47, 80.68s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 12/22 [12:11<10:59, 65.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 13/22 [12:42<08:16, 55.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 14/22 [13:07<06:10, 46.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 15/22 [13:47<05:09, 44.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 16/22 [14:30<04:23, 43.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 17/22 [14:57<03:13, 38.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 18/22 [15:15<02:10, 32.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 19/22 [15:38<01:29, 29.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 20/22 [16:08<00:59, 29.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 21/22 [16:47<00:32, 32.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 22/22 [17:09<00:00, 29.55s/it]Inference: 100%|██████████| 22/22 [17:09<00:00, 46.82s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Inference:   0%|          | 0/27 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:29<12:41, 29.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 2/27 [00:50<10:17, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 3/27 [01:48<15:55, 39.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 4/27 [02:00<11:02, 28.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 5/27 [02:19<09:15, 25.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 6/27 [02:34<07:37, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 7/27 [03:10<08:50, 26.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 8/27 [03:27<07:22, 23.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 9/27 [03:57<07:41, 25.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 10/27 [04:33<08:09, 28.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 11/27 [05:30<09:58, 37.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 12/27 [06:07<09:17, 37.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 13/27 [06:22<07:08, 30.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 14/27 [07:07<07:34, 34.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 15/27 [07:44<07:06, 35.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 16/27 [08:23<06:41, 36.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 17/27 [08:41<05:11, 31.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 18/27 [08:53<03:46, 25.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 19/27 [09:09<03:00, 22.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 20/27 [09:40<02:54, 24.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 21/27 [10:07<02:34, 25.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 22/27 [10:22<01:51, 22.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 23/27 [10:38<01:21, 20.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 24/27 [11:11<01:13, 24.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 25/27 [11:26<00:43, 21.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 26/27 [11:55<00:23, 23.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 27/27 [12:10<00:00, 21.13s/it]Inference: 100%|██████████| 27/27 [12:10<00:00, 27.06s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Inference:   0%|          | 0/21 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [00:23<07:56, 23.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 2/21 [00:49<07:50, 24.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 3/21 [01:48<12:08, 40.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 4/21 [02:17<10:09, 35.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 5/21 [08:53<44:11, 165.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 6/21 [09:30<30:33, 122.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 7/21 [10:04<21:44, 93.17s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 8/21 [10:28<15:24, 71.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 9/21 [10:39<10:27, 52.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 10/21 [10:54<07:30, 40.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 11/21 [11:13<05:43, 34.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 12/21 [11:46<05:04, 33.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 13/21 [12:07<03:58, 29.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 14/21 [12:48<03:52, 33.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 15/21 [13:10<02:59, 29.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 16/21 [13:28<02:11, 26.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 17/21 [13:42<01:30, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 18/21 [14:03<01:06, 22.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 19/21 [14:32<00:48, 24.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 20/21 [15:06<00:27, 27.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 21/21 [15:29<00:00, 25.94s/it]Inference: 100%|██████████| 21/21 [15:29<00:00, 44.27s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/39 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:43<27:22, 43.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/39 [01:01<17:34, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/39 [01:35<18:34, 30.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|█         | 4/39 [01:57<16:06, 27.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 5/39 [02:31<16:59, 29.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 6/39 [02:56<15:29, 28.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 7/39 [03:16<13:38, 25.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 8/39 [03:34<11:58, 23.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 9/39 [04:40<18:16, 36.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 10/39 [05:04<15:48, 32.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 11/39 [05:25<13:32, 29.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 12/39 [06:08<14:54, 33.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 13/39 [06:29<12:50, 29.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 14/39 [06:52<11:31, 27.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 15/39 [07:09<09:42, 24.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 16/39 [07:30<08:55, 23.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▎     | 17/39 [07:43<07:23, 20.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 18/39 [07:54<06:10, 17.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 19/39 [08:35<08:10, 24.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 20/39 [09:13<09:03, 28.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 21/39 [09:27<07:12, 24.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▋    | 22/39 [09:56<07:17, 25.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 23/39 [10:23<06:56, 26.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 24/39 [10:50<06:37, 26.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 25/39 [11:06<05:27, 23.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 26/39 [11:20<04:26, 20.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 27/39 [12:04<05:27, 27.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 28/39 [12:22<04:30, 24.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 29/39 [12:59<04:42, 28.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 30/39 [13:23<04:03, 27.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 31/39 [13:34<02:58, 22.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 32/39 [14:17<03:18, 28.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 33/39 [14:45<02:51, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 34/39 [15:23<02:35, 31.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|████████▉ | 35/39 [15:32<01:38, 24.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 36/39 [16:08<01:23, 27.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 37/39 [16:18<00:45, 22.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 38/39 [16:56<00:27, 27.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 39/39 [17:12<00:00, 23.90s/it]Inference: 100%|██████████| 39/39 [17:12<00:00, 26.48s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:12<06:50, 12.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:31<08:24, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [00:45<07:38, 15.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [01:06<08:30, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [01:29<09:09, 19.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [01:48<08:40, 19.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [02:10<08:46, 20.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [02:48<10:44, 25.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [03:13<10:11, 25.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:29<08:43, 22.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [04:11<10:26, 28.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [04:22<08:04, 23.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [05:08<10:05, 30.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [05:27<08:30, 26.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [05:49<07:33, 25.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [06:07<06:34, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [06:43<07:13, 27.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [07:13<06:58, 27.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [07:36<06:09, 26.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [08:04<05:48, 26.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [08:26<05:03, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [08:41<04:05, 22.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [09:09<03:59, 23.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [09:30<03:27, 23.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [10:00<03:22, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [10:16<02:36, 22.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [10:38<02:13, 22.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [10:46<01:30, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [11:19<01:29, 22.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [11:31<00:57, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [12:01<00:45, 22.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [12:26<00:23, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [12:40<00:00, 20.37s/it]Inference: 100%|██████████| 33/33 [12:40<00:00, 23.03s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/43 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:22<15:28, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/43 [00:42<14:18, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/43 [00:59<12:45, 19.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 4/43 [01:19<12:40, 19.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/43 [01:33<11:10, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/43 [01:56<11:57, 19.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▋        | 7/43 [02:12<10:57, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 8/43 [02:33<11:06, 19.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 9/43 [02:42<09:05, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 10/43 [03:10<10:50, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/43 [04:00<15:32, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 12/43 [05:01<20:02, 38.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 13/43 [05:41<19:29, 39.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/43 [05:59<15:48, 32.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▍      | 15/43 [06:09<12:05, 25.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 16/43 [06:41<12:31, 27.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|███▉      | 17/43 [07:31<14:50, 34.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 18/43 [07:48<12:08, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 19/43 [08:07<10:26, 26.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 20/43 [08:12<07:37, 19.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 21/43 [08:30<07:00, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 22/43 [08:57<07:34, 21.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 23/43 [09:18<07:06, 21.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 24/43 [09:49<07:44, 24.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 25/43 [10:05<06:31, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|██████    | 26/43 [10:14<05:05, 17.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 27/43 [10:38<05:17, 19.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▌   | 28/43 [11:00<05:04, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 29/43 [11:25<05:06, 21.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 30/43 [11:35<03:55, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 31/43 [12:07<04:26, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 32/43 [12:20<03:35, 19.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 33/43 [13:03<04:26, 26.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 34/43 [13:26<03:50, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 35/43 [13:45<03:07, 23.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▎ | 36/43 [14:03<02:32, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 37/43 [14:13<01:50, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 38/43 [14:34<01:36, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 39/43 [14:43<01:04, 16.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 40/43 [15:03<00:51, 17.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 41/43 [15:20<00:34, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 42/43 [15:41<00:18, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 43/43 [16:01<00:00, 18.80s/it]Inference: 100%|██████████| 43/43 [16:01<00:00, 22.36s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/41 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:21<14:15, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/41 [00:41<13:32, 20.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/41 [00:59<12:15, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/41 [01:51<19:53, 32.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/41 [02:00<14:18, 23.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 6/41 [02:18<12:42, 21.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/41 [02:53<14:46, 26.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|█▉        | 8/41 [03:20<14:34, 26.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 9/41 [03:29<11:17, 21.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/41 [04:00<12:23, 23.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 11/41 [04:21<11:37, 23.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 12/41 [04:52<12:16, 25.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 13/41 [05:10<10:49, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 14/41 [05:27<09:37, 21.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 15/41 [05:57<10:26, 24.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 16/41 [06:06<08:02, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████▏     | 17/41 [06:16<06:37, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 18/41 [06:24<05:20, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 19/41 [06:38<05:08, 14.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 20/41 [06:46<04:17, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 21/41 [07:03<04:36, 13.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 22/41 [07:18<04:28, 14.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 23/41 [07:58<06:32, 21.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▊    | 24/41 [08:21<06:18, 22.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 25/41 [08:28<04:40, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 26/41 [09:49<09:09, 36.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 27/41 [10:25<08:30, 36.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 28/41 [10:38<06:21, 29.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 29/41 [10:52<04:57, 24.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 30/41 [11:04<03:50, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 31/41 [11:12<02:52, 17.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 32/41 [11:58<03:52, 25.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|████████  | 33/41 [12:05<02:41, 20.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 34/41 [12:28<02:26, 20.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 35/41 [13:01<02:26, 24.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 36/41 [13:20<01:54, 22.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 37/41 [14:09<02:03, 30.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 38/41 [14:37<01:29, 29.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 39/41 [15:10<01:01, 30.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 40/41 [15:23<00:25, 25.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 41/41 [15:48<00:00, 25.30s/it]Inference: 100%|██████████| 41/41 [15:48<00:00, 23.13s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]
Inference:   0%|          | 0/37 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:20<12:20, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/37 [00:42<12:37, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/37 [01:17<15:37, 27.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 4/37 [01:40<14:13, 25.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 5/37 [02:11<14:37, 27.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▌        | 6/37 [02:17<10:28, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 7/37 [02:53<12:48, 25.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 8/37 [03:21<12:36, 26.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 9/37 [03:31<09:51, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 10/37 [03:51<09:21, 20.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 11/37 [05:50<22:00, 50.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 12/37 [06:13<17:40, 42.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 13/37 [06:23<13:02, 32.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 14/37 [06:39<10:31, 27.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 15/37 [07:22<11:48, 32.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 16/37 [07:27<08:27, 24.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 17/37 [07:51<08:02, 24.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 18/37 [08:22<08:13, 26.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 19/37 [08:42<07:16, 24.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 20/37 [09:00<06:19, 22.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 21/37 [09:18<05:38, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 22/37 [09:43<05:33, 22.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 23/37 [10:03<05:01, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 24/37 [10:20<04:24, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 25/37 [12:14<09:39, 48.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 26/37 [12:26<06:50, 37.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 27/37 [13:00<06:03, 36.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 28/37 [13:27<05:04, 33.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 29/37 [14:06<04:42, 35.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 30/37 [14:31<03:44, 32.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▍ | 31/37 [14:43<02:35, 25.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 32/37 [15:00<01:56, 23.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 33/37 [15:17<01:25, 21.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 34/37 [15:42<01:07, 22.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 35/37 [16:19<00:54, 27.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 36/37 [16:28<00:21, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 37/37 [16:46<00:00, 20.52s/it]Inference: 100%|██████████| 37/37 [16:46<00:00, 27.21s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]
Inference:   0%|          | 0/47 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:18<13:52, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▍         | 2/47 [00:28<10:06, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▋         | 3/47 [00:49<12:30, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▊         | 4/47 [01:22<16:47, 23.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 5/47 [01:55<18:37, 26.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 6/47 [03:54<39:41, 58.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 7/47 [04:14<30:29, 45.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 8/47 [04:30<23:34, 36.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 9/47 [04:50<19:46, 31.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 10/47 [05:11<17:17, 28.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 11/47 [05:40<16:59, 28.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 12/47 [05:49<13:07, 22.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 13/47 [06:38<17:14, 30.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 14/47 [07:18<18:18, 33.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 15/47 [07:34<14:57, 28.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 16/47 [07:50<12:42, 24.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 17/47 [08:01<10:16, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 18/47 [08:12<08:27, 17.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 19/47 [08:25<07:29, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 20/47 [09:36<14:45, 32.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▍     | 21/47 [09:45<11:00, 25.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 22/47 [10:09<10:28, 25.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 23/47 [10:26<09:04, 22.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 24/47 [10:41<07:49, 20.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 25/47 [10:47<05:53, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▌    | 26/47 [10:54<04:41, 13.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 27/47 [11:22<05:53, 17.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 28/47 [11:37<05:23, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 29/47 [12:02<05:46, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 30/47 [12:37<06:47, 23.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 31/47 [13:00<06:21, 23.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 32/47 [13:11<04:58, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 33/47 [14:05<07:00, 30.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 34/47 [14:23<05:45, 26.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 35/47 [14:35<04:26, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 36/47 [15:21<05:21, 29.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 37/47 [15:33<03:59, 23.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 38/47 [15:44<03:02, 20.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 39/47 [16:18<03:14, 24.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 40/47 [16:37<02:38, 22.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 41/47 [16:52<02:03, 20.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 42/47 [17:14<01:45, 21.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████▏| 43/47 [17:48<01:38, 24.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▎| 44/47 [18:11<01:12, 24.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▌| 45/47 [18:45<00:54, 27.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 46/47 [19:04<00:24, 24.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 47/47 [19:21<00:00, 22.44s/it]Inference: 100%|██████████| 47/47 [19:21<00:00, 24.71s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]
Inference:   0%|          | 0/54 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:15<13:28, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▎         | 2/54 [00:39<17:45, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 3/54 [00:50<13:35, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 4/54 [01:12<15:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 5/54 [01:32<15:41, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 6/54 [01:52<15:28, 19.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 7/54 [02:16<16:27, 21.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 8/54 [02:41<17:01, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 9/54 [02:50<13:36, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 10/54 [03:03<11:58, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|██        | 11/54 [03:16<11:06, 15.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 12/54 [03:40<12:41, 18.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 13/54 [03:58<12:17, 17.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 14/54 [04:06<09:59, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 15/54 [04:15<08:36, 13.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 16/54 [04:23<07:21, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███▏      | 17/54 [04:38<07:46, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 18/54 [05:30<14:38, 24.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 19/54 [06:23<19:19, 33.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 20/54 [06:53<18:15, 32.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 21/54 [07:03<13:56, 25.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 22/54 [07:12<10:57, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 23/54 [07:45<12:28, 24.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 24/54 [07:57<10:16, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 25/54 [08:07<08:21, 17.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 26/54 [08:24<08:06, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 27/54 [08:36<07:01, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 28/54 [09:00<07:50, 18.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 29/54 [09:10<06:38, 15.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 30/54 [09:19<05:32, 13.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 31/54 [09:39<05:57, 15.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 32/54 [09:48<05:01, 13.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 33/54 [10:03<04:51, 13.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 34/54 [10:11<04:06, 12.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 35/54 [11:04<07:45, 24.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 36/54 [11:18<06:23, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▊   | 37/54 [11:40<06:03, 21.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 38/54 [12:05<06:03, 22.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 39/54 [12:20<05:04, 20.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 40/54 [12:38<04:36, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 41/54 [13:07<04:51, 22.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 42/54 [13:26<04:17, 21.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|███████▉  | 43/54 [13:43<03:38, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 44/54 [14:14<03:54, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 45/54 [14:33<03:18, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 46/54 [14:56<02:58, 22.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 47/54 [15:21<02:41, 23.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 48/54 [15:52<02:33, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 49/54 [16:22<02:14, 26.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 50/54 [16:41<01:37, 24.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 51/54 [16:53<01:02, 20.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 52/54 [17:00<00:33, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 53/54 [17:07<00:13, 13.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 54/54 [17:43<00:00, 20.50s/it]Inference: 100%|██████████| 54/54 [17:43<00:00, 19.70s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...                lora_path
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  ../finetune/is-all_gsat

[1 rows x 6 columns]
  dataset_name                             path  1_tile  ...  3_tile  4_tile  5_tile
0       113-ch  ../dataset/gsat/113_chinese.csv   63.48  ...   52.90   42.32   37.03
1       112-ch  ../dataset/gsat/112_chinese.csv   62.63  ...   52.19   41.75   36.54
2       111-ch  ../dataset/gsat/111_chinese.csv   64.08  ...   48.06   42.72   32.04
3       110-ch  ../dataset/gsat/110_chinese.csv   64.57  ...   53.81   43.05   37.67
4       109-ch  ../dataset/gsat/109_chinese.csv   65.65  ...   54.71   43.77   38.30

[5 rows x 7 columns]
Running dataset:113-ch on model:breeze-7b-is_gsat
Running dataset:112-ch on model:breeze-7b-is_gsat
Running dataset:111-ch on model:breeze-7b-is_gsat
Running dataset:110-ch on model:breeze-7b-is_gsat
Running dataset:109-ch on model:breeze-7b-is_gsat
Running dataset:113-ns on model:breeze-7b-is_gsat
Running dataset:112-ns on model:breeze-7b-is_gsat
Running dataset:111-ns on model:breeze-7b-is_gsat
Running dataset:110-ns on model:breeze-7b-is_gsat
Running dataset:109-ns on model:breeze-7b-is_gsat
Running dataset:113-ss on model:breeze-7b-is_gsat
Running dataset:112-ss on model:breeze-7b-is_gsat
Running dataset:111-ss on model:breeze-7b-is_gsat
Running dataset:110-ss on model:breeze-7b-is_gsat
Running dataset:109-ss on model:breeze-7b-is_gsat
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:11<06:00, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:25<06:52, 13.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [01:11<13:59, 27.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [01:24<10:46, 22.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [02:07<13:50, 29.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:10<05:58, 10.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:32<09:02, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:17<09:51, 17.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:11<07:56, 11.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:28<09:52, 14.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [00:55<13:18, 20.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:08<10:58, 17.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [01:25<10:40, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [01:38<09:29, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [01:50<08:30, 14.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [02:21<11:09, 19.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [02:58<13:54, 25.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:23<13:23, 25.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [03:39<11:35, 22.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:11<12:35, 25.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [04:41<12:51, 26.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [05:01<11:28, 24.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [05:09<08:52, 19.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:11<05:53, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:25<06:48, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [01:09<13:39, 27.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [01:23<10:31, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [02:05<13:36, 29.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [02:49<15:23, 34.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [03:04<12:10, 28.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [03:24<10:34, 25.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [03:36<08:27, 21.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:43<06:28, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [04:25<09:01, 24.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [04:38<07:21, 21.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [05:27<09:48, 29.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [06:05<10:09, 32.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [07:10<12:36, 42.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [07:25<09:34, 33.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [08:04<09:26, 35.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [08:19<07:20, 29.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [08:34<05:47, 24.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [08:54<05:04, 23.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [09:19<04:46, 23.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [09:33<03:50, 20.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [09:51<03:20, 20.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [10:12<03:02, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [10:30<02:38, 19.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [11:12<03:04, 26.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [11:22<02:09, 21.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [11:50<01:57, 23.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [13:23<02:56, 44.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [13:58<02:04, 41.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [15:59<02:10, 65.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [16:51<01:01, 61.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [17:19<00:00, 51.38s/it]Inference: 100%|██████████| 33/33 [17:19<00:00, 31.50s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:10<05:37, 10.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:29<08:22, 15.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:54<10:20, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [03:59<42:29, 84.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [04:10<28:07, 58.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [04:34<21:43, 46.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [04:49<16:21, 36.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [05:09<13:27, 31.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [05:40<13:00, 31.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [06:05<11:37, 29.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [10:30<38:51, 101.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [10:38<26:42, 72.86s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [10:52<19:20, 55.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [11:18<15:27, 46.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [11:29<11:18, 35.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [11:49<09:16, 30.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [12:29<09:32, 33.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [12:58<08:34, 32.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [13:27<07:48, 31.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [13:46<06:27, 27.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [13:56<04:50, 22.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [14:11<04:01, 20.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [14:36<03:56, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [15:03<03:53, 23.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [15:43<04:13, 28.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [16:09<03:41, 27.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [16:21<02:39, 22.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [17:14<03:11, 31.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [17:42<02:34, 30.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [17:59<01:46, 26.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [18:36<01:29, 29.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [19:22<01:09, 34.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [20:07<00:37, 37.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [20:30<00:00, 33.33s/it]Inference: 100%|██████████| 34/34 [20:30<00:00, 36.19s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:15<08:31, 15.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:25<06:27, 12.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:44<07:51, 15.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [01:05<08:52, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:30<09:47, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [01:50<09:25, 20.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [02:07<08:39, 19.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [02:22<07:38, 17.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [02:54<09:14, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [04:20<16:46, 41.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [04:39<13:24, 34.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [06:45<22:55, 62.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [07:23<19:18, 55.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [08:25<19:05, 57.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [09:20<17:57, 56.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [09:41<13:46, 45.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [10:34<13:36, 48.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [10:41<09:28, 35.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [11:14<08:41, 34.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [13:17<14:17, 61.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [14:28<13:54, 64.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [15:03<11:07, 55.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [15:28<08:30, 46.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [15:49<06:27, 38.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [16:25<05:39, 37.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [16:36<03:57, 29.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [17:20<03:58, 34.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [17:40<02:59, 29.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [18:26<02:54, 34.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [19:09<02:28, 37.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [20:23<02:24, 48.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [20:50<01:23, 42.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [21:10<00:35, 35.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [21:40<00:00, 33.64s/it]Inference: 100%|██████████| 34/34 [21:40<00:00, 38.24s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:11<07:32, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:27<09:26, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [00:53<12:42, 19.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:05<10:25, 16.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [01:21<10:08, 16.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [01:33<09:02, 15.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [01:45<08:06, 13.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [02:14<10:36, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [02:50<13:16, 24.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:14<12:48, 24.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [03:30<11:08, 21.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:00<12:06, 24.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [04:29<12:20, 25.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [04:48<10:59, 23.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [04:56<08:29, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [05:37<11:04, 25.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [06:04<10:55, 26.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [06:36<11:08, 27.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [06:53<09:23, 24.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [07:09<08:07, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [07:33<07:55, 22.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [08:01<08:02, 24.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [08:16<06:47, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [08:36<06:15, 20.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [08:57<05:58, 21.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [10:19<10:30, 39.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [12:30<16:43, 66.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [12:50<12:18, 52.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [13:07<09:05, 41.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [15:52<15:46, 78.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [16:12<11:15, 61.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [16:53<09:12, 55.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [17:24<07:11, 47.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [17:50<05:29, 41.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [18:04<03:52, 33.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [18:32<03:10, 31.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [18:44<02:07, 25.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [19:00<01:31, 22.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [19:24<01:09, 23.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [19:49<00:47, 23.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [20:07<00:21, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [20:24<00:00, 20.52s/it]Inference: 100%|██████████| 42/42 [20:24<00:00, 29.16s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:12<08:50, 12.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:27<09:15, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [00:55<13:17, 20.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:15<12:53, 20.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [01:34<12:02, 19.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [01:43<09:38, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [09:04<1:30:28, 155.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [09:16<1:01:57, 109.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [09:40<45:34, 82.88s/it]   Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [09:57<33:19, 62.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [10:17<25:31, 49.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [10:56<23:11, 46.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [11:12<17:56, 37.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [11:37<15:37, 33.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [11:56<13:05, 29.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [12:30<13:10, 30.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [12:39<10:04, 24.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [12:51<08:09, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [13:31<10:03, 26.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [14:34<13:42, 37.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [14:45<10:19, 29.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [15:14<09:47, 29.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [15:24<07:26, 23.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [16:00<08:13, 27.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [16:12<06:26, 22.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [16:39<06:20, 23.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [17:02<05:54, 23.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [17:53<07:27, 31.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [18:31<07:19, 33.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [20:16<11:00, 55.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [20:51<08:59, 49.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [21:11<06:42, 40.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [22:45<08:28, 56.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [23:04<06:01, 45.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [23:15<04:04, 34.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [23:44<03:18, 33.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [24:18<02:47, 33.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [24:58<02:21, 35.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [25:22<01:35, 31.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [26:38<01:30, 45.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [27:12<00:41, 41.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [27:58<00:00, 42.97s/it]Inference: 100%|██████████| 42/42 [27:58<00:00, 39.95s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]
Inference:   0%|          | 0/22 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:34<12:10, 34.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 2/22 [01:23<14:16, 42.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 3/22 [01:49<11:07, 35.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 4/22 [02:40<12:24, 41.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 5/22 [03:08<10:24, 36.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 6/22 [03:55<10:41, 40.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 7/22 [05:15<13:17, 53.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 8/22 [05:52<11:11, 47.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 9/22 [06:44<10:42, 49.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 10/22 [06:58<07:40, 38.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 11/22 [08:26<09:50, 53.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 12/22 [08:44<07:08, 42.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 13/22 [09:22<06:11, 41.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 14/22 [09:44<04:42, 35.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 15/22 [11:25<06:26, 55.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 16/22 [11:50<04:37, 46.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 17/22 [12:27<03:37, 43.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 18/22 [13:00<02:41, 40.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 19/22 [13:29<01:50, 36.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 20/22 [13:56<01:07, 33.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 21/22 [14:17<00:30, 30.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 22/22 [14:36<00:00, 26.73s/it]Inference: 100%|██████████| 22/22 [14:36<00:00, 39.84s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]
Inference:   0%|          | 0/27 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:12<05:23, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 2/27 [00:22<04:31, 10.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 3/27 [00:40<05:43, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 4/27 [01:04<06:52, 17.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 5/27 [01:41<09:07, 24.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 6/27 [01:55<07:23, 21.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 7/27 [02:37<09:18, 27.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 8/27 [02:56<07:57, 25.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 9/27 [03:23<07:41, 25.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 10/27 [04:09<09:05, 32.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 11/27 [04:29<07:34, 28.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 12/27 [04:48<06:22, 25.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 13/27 [05:09<05:38, 24.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 14/27 [06:10<07:40, 35.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 15/27 [06:34<06:22, 31.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 16/27 [07:09<06:01, 32.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 17/27 [07:33<05:01, 30.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 18/27 [07:54<04:07, 27.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 19/27 [08:27<03:52, 29.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 20/27 [09:22<04:17, 36.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 21/27 [09:45<03:15, 32.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 22/27 [09:58<02:13, 26.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 23/27 [10:38<02:03, 30.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 24/27 [10:59<01:23, 27.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 25/27 [11:13<00:47, 23.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 26/27 [11:34<00:22, 22.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 27/27 [12:09<00:00, 26.60s/it]Inference: 100%|██████████| 27/27 [12:09<00:00, 27.03s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Inference:   0%|          | 0/21 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [00:34<11:27, 34.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 2/21 [00:48<07:07, 22.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 3/21 [01:18<07:48, 26.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 4/21 [01:42<07:06, 25.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 5/21 [03:16<13:18, 49.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 6/21 [03:51<11:12, 44.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 7/21 [04:13<08:44, 37.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 8/21 [04:43<07:36, 35.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 9/21 [05:14<06:46, 33.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 10/21 [05:54<06:31, 35.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 11/21 [06:21<05:31, 33.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 12/21 [07:03<05:22, 35.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 13/21 [07:31<04:25, 33.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 14/21 [07:54<03:31, 30.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 15/21 [08:21<02:55, 29.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 16/21 [08:36<02:04, 24.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 17/21 [09:27<02:12, 33.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 18/21 [10:24<01:59, 39.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 19/21 [11:16<01:27, 43.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 20/21 [11:45<00:39, 39.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 21/21 [11:58<00:00, 31.30s/it]Inference: 100%|██████████| 21/21 [11:58<00:00, 34.20s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Inference:   0%|          | 0/39 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:06<03:50,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/39 [00:19<06:17, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/39 [00:55<13:09, 21.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|█         | 4/39 [01:11<11:36, 19.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 5/39 [01:26<10:14, 18.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 6/39 [02:16<15:50, 28.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 7/39 [02:28<12:26, 23.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 8/39 [02:43<10:38, 20.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 9/39 [03:05<10:38, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 10/39 [03:29<10:34, 21.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 11/39 [03:52<10:25, 22.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 12/39 [04:44<14:08, 31.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 13/39 [05:14<13:23, 30.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 14/39 [05:37<11:53, 28.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 15/39 [06:17<12:49, 32.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 16/39 [06:51<12:27, 32.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▎     | 17/39 [07:07<10:07, 27.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 18/39 [07:23<08:27, 24.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 19/39 [08:33<12:34, 37.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 20/39 [08:44<09:26, 29.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 21/39 [09:13<08:55, 29.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▋    | 22/39 [09:35<07:42, 27.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 23/39 [10:10<07:53, 29.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 24/39 [10:40<07:24, 29.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 25/39 [11:07<06:45, 28.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 26/39 [11:28<05:45, 26.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 27/39 [11:57<05:27, 27.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 28/39 [12:18<04:38, 25.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 29/39 [12:33<03:41, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 30/39 [13:16<04:17, 28.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 31/39 [13:38<03:31, 26.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 32/39 [20:14<16:01, 137.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 33/39 [20:40<10:24, 104.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 34/39 [21:22<07:06, 85.37s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|████████▉ | 35/39 [21:36<04:15, 63.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 36/39 [21:54<02:30, 50.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 37/39 [22:35<01:34, 47.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 38/39 [22:56<00:39, 39.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 39/39 [23:19<00:00, 34.66s/it]Inference: 100%|██████████| 39/39 [23:19<00:00, 35.89s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:24<12:53, 24.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:33<07:56, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [00:43<06:23, 12.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [00:54<06:00, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [01:33<10:11, 21.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [01:47<08:37, 19.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [02:08<08:32, 19.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [02:56<12:02, 28.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [03:16<10:22, 25.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:50<10:52, 28.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [04:06<09:02, 24.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [04:33<08:53, 25.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [04:58<08:24, 25.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [05:10<06:45, 21.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [05:48<07:54, 26.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [06:19<07:50, 27.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [06:40<06:53, 25.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [07:25<07:51, 31.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [08:55<11:25, 48.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [09:12<08:33, 39.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [09:26<06:21, 31.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [09:39<04:47, 26.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [10:07<04:28, 26.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [10:23<03:30, 23.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [10:51<03:19, 24.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [11:23<03:08, 26.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [11:39<02:23, 23.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [12:12<02:11, 26.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [18:46<09:07, 136.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [19:01<05:00, 100.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [19:18<02:30, 75.16s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [19:43<01:00, 60.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [20:20<00:00, 53.36s/it]Inference: 100%|██████████| 33/33 [20:20<00:00, 36.99s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/43 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:19<13:18, 19.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/43 [00:34<11:34, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/43 [00:48<10:28, 15.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 4/43 [01:17<13:28, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/43 [01:38<13:15, 20.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/43 [02:34<20:15, 32.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▋        | 7/43 [03:57<29:38, 49.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 8/43 [04:30<25:40, 44.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 9/43 [05:25<26:57, 47.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 10/43 [05:42<20:56, 38.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/43 [06:21<20:30, 38.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 12/43 [06:38<16:22, 31.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 13/43 [06:56<13:52, 27.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/43 [07:29<14:09, 29.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▍      | 15/43 [07:53<12:53, 27.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 16/43 [08:20<12:23, 27.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|███▉      | 17/43 [08:27<09:12, 21.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 18/43 [08:50<09:04, 21.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 19/43 [08:58<07:00, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 20/43 [09:26<07:58, 20.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 21/43 [09:43<07:11, 19.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 22/43 [09:50<05:34, 15.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 23/43 [10:11<05:45, 17.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 24/43 [10:28<05:28, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 25/43 [11:08<07:17, 24.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|██████    | 26/43 [11:19<05:43, 20.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 27/43 [11:34<04:59, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▌   | 28/43 [12:08<05:45, 23.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 29/43 [12:18<04:30, 19.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 30/43 [12:42<04:29, 20.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 31/43 [14:24<08:58, 44.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 32/43 [15:18<08:44, 47.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 33/43 [15:30<06:09, 36.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 34/43 [15:40<04:22, 29.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 35/43 [16:00<03:30, 26.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▎ | 36/43 [16:36<03:23, 29.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 37/43 [16:45<02:19, 23.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 38/43 [17:10<01:57, 23.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 39/43 [17:35<01:36, 24.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 40/43 [20:09<03:08, 62.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 41/43 [20:17<01:33, 46.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 42/43 [20:48<00:41, 41.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 43/43 [20:59<00:00, 32.55s/it]Inference: 100%|██████████| 43/43 [20:59<00:00, 29.28s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/41 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:26<17:34, 26.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/41 [00:48<15:41, 24.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/41 [01:09<14:20, 22.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/41 [01:21<11:15, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/41 [01:42<11:30, 19.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 6/41 [02:05<12:06, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/41 [02:42<14:41, 25.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|█▉        | 8/41 [03:16<15:43, 28.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 9/41 [03:45<15:15, 28.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/41 [04:16<15:07, 29.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 11/41 [04:47<14:56, 29.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 12/41 [05:05<12:40, 26.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 13/41 [07:52<32:10, 68.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 14/41 [08:11<24:10, 53.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 15/41 [08:52<21:39, 49.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 16/41 [09:15<17:27, 41.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████▏     | 17/41 [09:29<13:26, 33.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 18/41 [09:43<10:36, 27.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 19/41 [11:45<20:27, 55.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 20/41 [11:55<14:43, 42.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 21/41 [12:09<11:15, 33.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 22/41 [12:32<09:38, 30.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 23/41 [13:03<09:11, 30.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▊    | 24/41 [13:20<07:30, 26.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 25/41 [13:37<06:20, 23.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 26/41 [13:52<05:17, 21.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 27/41 [14:01<04:04, 17.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 28/41 [14:10<03:12, 14.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 29/41 [14:25<02:57, 14.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 30/41 [14:34<02:24, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 31/41 [15:44<05:02, 30.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 32/41 [15:54<03:36, 24.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|████████  | 33/41 [16:02<02:35, 19.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 34/41 [16:13<01:57, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 35/41 [16:54<02:24, 24.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 36/41 [17:30<02:18, 27.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 37/41 [18:02<01:55, 28.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 38/41 [18:36<01:31, 30.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 39/41 [19:16<01:06, 33.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 40/41 [19:24<00:25, 25.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 41/41 [20:42<00:00, 41.35s/it]Inference: 100%|██████████| 41/41 [20:42<00:00, 30.30s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Inference:   0%|          | 0/37 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [01:44<1:02:24, 104.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/37 [01:58<29:48, 51.11s/it]   Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/37 [02:08<18:29, 32.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 4/37 [02:40<17:50, 32.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 5/37 [03:04<15:31, 29.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▌        | 6/37 [03:21<12:57, 25.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 7/37 [03:26<09:21, 18.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 8/37 [03:45<08:57, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 9/37 [07:04<35:03, 75.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 10/37 [07:18<25:15, 56.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 11/37 [07:50<21:07, 48.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 12/37 [07:59<15:18, 36.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 13/37 [08:13<11:53, 29.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 14/37 [08:46<11:46, 30.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 15/37 [09:13<10:54, 29.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 16/37 [09:38<09:56, 28.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 17/37 [09:47<07:26, 22.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 18/37 [10:03<06:32, 20.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 19/37 [10:23<06:07, 20.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 20/37 [10:45<05:55, 20.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 21/37 [11:41<08:20, 31.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 22/37 [11:59<06:49, 27.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 23/37 [12:10<05:13, 22.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 24/37 [12:23<04:14, 19.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 25/37 [12:58<04:49, 24.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 26/37 [13:16<04:05, 22.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 27/37 [14:12<05:26, 32.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 28/37 [15:13<06:08, 40.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 29/37 [15:44<05:04, 38.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 30/37 [16:06<03:52, 33.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▍ | 31/37 [16:48<03:36, 36.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 32/37 [17:00<02:23, 28.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 33/37 [17:31<01:56, 29.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 34/37 [17:38<01:08, 22.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 35/37 [17:59<00:44, 22.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 36/37 [18:23<00:22, 22.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 37/37 [18:50<00:00, 24.12s/it]Inference: 100%|██████████| 37/37 [18:50<00:00, 30.56s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Inference:   0%|          | 0/47 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:56<43:25, 56.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▍         | 2/47 [03:42<1:30:27, 120.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▋         | 3/47 [04:00<54:13, 73.94s/it]   Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▊         | 4/47 [04:15<36:14, 50.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 5/47 [04:27<25:50, 36.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 6/47 [04:37<18:46, 27.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 7/47 [04:49<15:00, 22.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 8/47 [08:29<55:25, 85.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 9/47 [11:45<1:16:07, 120.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 10/47 [12:03<54:39, 88.63s/it]  Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 11/47 [12:24<40:36, 67.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 12/47 [12:36<29:44, 50.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 13/47 [13:05<25:01, 44.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 14/47 [13:31<21:18, 38.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 15/47 [13:43<16:21, 30.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 16/47 [14:15<16:04, 31.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 17/47 [14:37<14:12, 28.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 18/47 [14:55<12:14, 25.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 19/47 [15:14<10:55, 23.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 20/47 [15:25<08:46, 19.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▍     | 21/47 [15:50<09:11, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 22/47 [16:13<09:00, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 23/47 [16:23<07:17, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 24/47 [17:41<13:54, 36.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 25/47 [18:15<13:02, 35.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▌    | 26/47 [18:38<11:06, 31.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 27/47 [18:47<08:21, 25.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 28/47 [19:03<07:03, 22.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 29/47 [19:14<05:36, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 30/47 [19:25<04:41, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 31/47 [19:54<05:22, 20.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 32/47 [20:10<04:45, 19.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 33/47 [20:29<04:27, 19.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 34/47 [20:38<03:25, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 35/47 [27:16<26:07, 130.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 36/47 [27:31<17:36, 96.04s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 37/47 [29:42<17:45, 106.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 38/47 [29:58<11:52, 79.20s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 39/47 [30:11<07:55, 59.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 40/47 [30:29<05:28, 46.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 41/47 [30:56<04:05, 40.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 42/47 [31:15<02:52, 34.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████▏| 43/47 [32:40<03:18, 49.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▎| 44/47 [32:48<01:51, 37.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▌| 45/47 [32:55<00:56, 28.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 46/47 [33:08<00:23, 23.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 47/47 [33:18<00:00, 19.33s/it]Inference: 100%|██████████| 47/47 [33:18<00:00, 42.51s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Inference:   0%|          | 0/54 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:12<10:40, 12.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▎         | 2/54 [00:19<08:00,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 3/54 [00:31<08:56, 10.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 4/54 [00:46<10:16, 12.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 5/54 [00:59<10:17, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 6/54 [01:18<11:46, 14.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 7/54 [01:30<10:55, 13.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 8/54 [02:21<19:39, 25.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 9/54 [02:29<15:01, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 10/54 [03:10<19:26, 26.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|██        | 11/54 [03:25<16:39, 23.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 12/54 [03:51<16:50, 24.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 13/54 [05:39<33:42, 49.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 14/54 [05:47<24:33, 36.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 15/54 [06:21<23:19, 35.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 16/54 [06:45<20:31, 32.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███▏      | 17/54 [07:08<18:16, 29.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 18/54 [07:28<16:05, 26.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 19/54 [07:51<14:56, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 20/54 [08:55<20:59, 37.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 21/54 [09:10<16:42, 30.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 22/54 [09:21<13:12, 24.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 23/54 [10:08<16:08, 31.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 24/54 [10:28<13:59, 27.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 25/54 [10:36<10:39, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 26/54 [10:46<08:33, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 27/54 [11:05<08:20, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 28/54 [11:16<07:00, 16.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 29/54 [15:14<34:33, 82.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 30/54 [15:37<25:56, 64.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 31/54 [15:53<19:15, 50.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 32/54 [16:00<13:40, 37.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 33/54 [16:14<10:33, 30.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 34/54 [16:29<08:36, 25.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 35/54 [16:50<07:43, 24.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 36/54 [17:08<06:42, 22.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▊   | 37/54 [17:46<07:40, 27.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 38/54 [18:05<06:31, 24.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 39/54 [24:44<34:15, 137.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 40/54 [25:03<23:42, 101.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 41/54 [25:41<17:52, 82.49s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 42/54 [26:03<12:50, 64.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|███████▉  | 43/54 [26:25<09:28, 51.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 44/54 [26:44<06:58, 41.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 45/54 [26:56<04:56, 32.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 46/54 [27:09<03:36, 27.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 47/54 [27:34<03:04, 26.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 48/54 [27:47<02:13, 22.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 49/54 [27:56<01:32, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 50/54 [28:20<01:20, 20.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 51/54 [28:38<00:58, 19.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 52/54 [28:52<00:35, 17.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 53/54 [29:57<00:32, 32.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 54/54 [30:39<00:00, 34.80s/it]Inference: 100%|██████████| 54/54 [30:39<00:00, 34.06s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...                                lora_path
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  ../finetune/cp-all_textbook+is-all_gsat

[1 rows x 6 columns]
  dataset_name                             path  1_tile  ...  3_tile  4_tile  5_tile
0       113-ch  ../dataset/gsat/113_chinese.csv   63.48  ...   52.90   42.32   37.03
1       112-ch  ../dataset/gsat/112_chinese.csv   62.63  ...   52.19   41.75   36.54
2       111-ch  ../dataset/gsat/111_chinese.csv   64.08  ...   48.06   42.72   32.04
3       110-ch  ../dataset/gsat/110_chinese.csv   64.57  ...   53.81   43.05   37.67
4       109-ch  ../dataset/gsat/109_chinese.csv   65.65  ...   54.71   43.77   38.30

[5 rows x 7 columns]
Running dataset:113-ch on model:breeze-7b-cp+is_gsat
Running dataset:112-ch on model:breeze-7b-cp+is_gsat
Running dataset:111-ch on model:breeze-7b-cp+is_gsat
Running dataset:110-ch on model:breeze-7b-cp+is_gsat
Running dataset:109-ch on model:breeze-7b-cp+is_gsat
Running dataset:113-ns on model:breeze-7b-cp+is_gsat
Running dataset:112-ns on model:breeze-7b-cp+is_gsat
Running dataset:111-ns on model:breeze-7b-cp+is_gsat
Running dataset:110-ns on model:breeze-7b-cp+is_gsat
Running dataset:109-ns on model:breeze-7b-cp+is_gsat
Running dataset:113-ss on model:breeze-7b-cp+is_gsat
Running dataset:112-ss on model:breeze-7b-cp+is_gsat
Running dataset:111-ss on model:breeze-7b-cp+is_gsat
Running dataset:110-ss on model:breeze-7b-cp+is_gsat
Running dataset:109-ss on model:breeze-7b-cp+is_gsat
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [01:01<32:50, 61.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [01:28<21:22, 41.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [01:56<17:27, 34.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [02:30<16:42, 34.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [03:41<22:22, 47.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [04:12<18:52, 41.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [04:41<16:27, 37.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [05:07<14:14, 34.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [05:39<13:22, 33.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [06:09<12:24, 32.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [06:32<10:45, 29.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [07:07<10:56, 31.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [07:33<09:50, 29.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [08:28<11:46, 37.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [09:22<12:40, 42.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [10:00<11:38, 41.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [10:38<10:41, 40.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [11:07<09:09, 36.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [11:32<07:48, 33.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [12:03<07:03, 32.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [12:31<06:12, 31.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [13:02<05:43, 31.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [13:42<05:39, 33.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [14:25<05:29, 36.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [15:14<05:21, 40.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [16:07<05:08, 44.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [16:41<04:06, 41.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [17:10<03:07, 37.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [18:11<02:58, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [20:02<03:12, 64.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [21:50<02:35, 77.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [22:21<01:03, 63.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [22:53<00:00, 54.11s/it]Inference: 100%|██████████| 33/33 [22:53<00:00, 41.62s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:45<25:08, 45.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [01:16<19:43, 36.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [01:43<16:47, 32.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [02:27<18:22, 36.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [03:54<26:38, 55.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [05:33<32:39, 69.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [05:57<24:39, 54.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [06:26<20:15, 46.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [06:50<16:26, 39.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [07:17<14:15, 35.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [07:47<13:03, 34.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [08:17<11:59, 32.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [08:47<11:09, 31.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [09:32<11:55, 35.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [10:02<10:47, 34.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [10:30<09:43, 32.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [10:59<08:49, 31.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [11:34<08:40, 32.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [12:08<08:14, 32.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [12:33<07:05, 30.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [13:02<06:30, 30.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [13:35<06:10, 30.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [13:58<05:14, 28.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [14:21<04:29, 26.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [14:58<04:30, 30.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [15:26<03:53, 29.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [15:50<03:14, 27.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [16:53<03:50, 38.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [17:45<03:31, 42.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [18:09<02:28, 37.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [18:32<01:38, 32.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [18:52<00:57, 28.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [19:16<00:27, 27.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [19:45<00:00, 27.74s/it]Inference: 100%|██████████| 34/34 [19:45<00:00, 34.85s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:12<06:58, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:46<13:14, 24.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [01:01<10:37, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [01:23<10:39, 21.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:52<11:34, 23.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [04:24<31:27, 67.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [04:50<24:11, 53.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [05:25<20:45, 47.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [05:55<17:36, 42.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [06:32<16:17, 40.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [07:07<14:53, 38.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [07:27<12:08, 33.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [07:59<11:28, 32.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [08:22<09:59, 29.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [08:47<09:00, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [09:21<09:03, 30.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [10:04<09:38, 34.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [10:31<08:30, 31.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [10:57<07:29, 29.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [11:29<07:08, 30.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [12:04<06:55, 31.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [12:47<07:05, 35.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [13:16<06:06, 33.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [13:41<05:09, 30.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [14:13<04:42, 31.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [15:16<05:26, 40.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [15:40<04:09, 35.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [16:26<03:52, 38.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [17:18<03:34, 42.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [17:40<02:25, 36.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [18:05<01:38, 32.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [18:33<01:02, 31.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [19:05<00:31, 31.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [19:53<00:00, 36.58s/it]Inference: 100%|██████████| 34/34 [19:53<00:00, 35.10s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:45<31:10, 45.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [01:07<20:54, 31.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [02:15<31:16, 48.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [03:00<29:55, 47.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [03:24<23:50, 38.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [03:52<20:59, 34.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [04:40<22:58, 39.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [05:50<27:46, 49.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [06:51<29:05, 52.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [07:18<23:50, 44.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [08:04<23:25, 45.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [08:42<21:25, 42.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [09:04<17:41, 36.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [09:40<17:01, 36.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [10:04<14:39, 32.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [10:37<14:13, 32.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [11:04<13:00, 31.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [11:42<13:18, 33.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [12:14<12:33, 32.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [12:43<11:33, 31.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [13:22<11:48, 33.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [13:59<11:38, 34.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [14:24<10:06, 31.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [14:42<08:18, 27.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [15:24<09:01, 31.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [16:04<09:12, 34.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [16:39<08:40, 34.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [17:09<07:43, 33.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [17:35<06:43, 31.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [18:05<06:09, 30.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [18:34<05:30, 30.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [19:14<05:32, 33.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [19:41<04:42, 31.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [20:06<03:55, 29.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [20:26<03:06, 26.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [20:53<02:40, 26.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [21:45<02:51, 34.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [22:54<02:58, 44.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [23:49<02:22, 47.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [25:11<01:56, 58.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [26:16<01:00, 60.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [26:47<00:00, 51.47s/it]Inference: 100%|██████████| 42/42 [26:47<00:00, 38.28s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:31<21:51, 31.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [01:03<21:12, 31.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [01:53<26:07, 40.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [03:00<32:00, 50.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [03:23<24:59, 40.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [03:44<20:23, 33.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [04:09<18:09, 31.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [04:34<16:29, 29.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [05:30<20:38, 37.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [06:03<19:18, 36.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [06:34<17:49, 34.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [06:58<15:46, 31.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [07:26<14:39, 30.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [07:54<13:46, 29.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [08:38<15:14, 33.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [09:16<15:12, 35.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [10:00<15:44, 37.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [10:46<16:10, 40.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [11:22<14:54, 38.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [11:52<13:18, 36.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [12:19<11:43, 33.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [12:53<11:12, 33.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [13:28<10:51, 34.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [13:58<09:49, 32.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [14:19<08:18, 29.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [14:37<06:56, 26.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [15:17<07:31, 30.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [16:05<08:16, 35.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [17:01<09:00, 41.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [17:26<07:19, 36.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [17:54<06:15, 34.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [18:22<05:20, 32.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [19:01<05:09, 34.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [19:24<04:06, 30.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [19:55<03:35, 30.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [21:13<04:29, 44.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [22:27<04:28, 53.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [23:04<03:14, 48.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [24:03<02:35, 51.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [25:00<01:46, 53.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [25:46<00:51, 51.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [26:09<00:00, 42.74s/it]Inference: 100%|██████████| 42/42 [26:09<00:00, 37.37s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Inference:   0%|          | 0/22 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:34<12:02, 34.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 2/22 [01:42<18:04, 54.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 3/22 [02:39<17:38, 55.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 4/22 [03:25<15:29, 51.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 5/22 [04:01<13:04, 46.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 6/22 [04:38<11:26, 42.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 7/22 [06:43<17:24, 69.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 8/22 [07:43<15:32, 66.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 9/22 [10:01<19:18, 89.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 10/22 [10:36<14:28, 72.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 11/22 [11:25<11:56, 65.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 12/22 [11:53<08:58, 53.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 13/22 [13:08<09:03, 60.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 14/22 [16:16<13:10, 98.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 15/22 [17:01<09:39, 82.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 16/22 [17:41<06:58, 69.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 17/22 [18:58<05:59, 71.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 18/22 [19:47<04:20, 65.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 19/22 [20:28<02:53, 57.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 20/22 [20:54<01:36, 48.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 21/22 [21:48<00:50, 50.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 22/22 [22:13<00:00, 42.54s/it]Inference: 100%|██████████| 22/22 [22:13<00:00, 60.62s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
Inference:   0%|          | 0/27 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:41<17:47, 41.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 2/27 [01:12<14:49, 35.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 3/27 [02:08<17:49, 44.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 4/27 [03:16<20:38, 53.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 5/27 [03:48<16:53, 46.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 6/27 [04:46<17:31, 50.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 7/27 [05:32<16:13, 48.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 8/27 [05:49<12:14, 38.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 9/27 [06:21<11:01, 36.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 10/27 [07:28<13:01, 45.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 11/27 [08:11<11:59, 44.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 12/27 [09:07<12:05, 48.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 13/27 [09:41<10:15, 43.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 14/27 [11:10<12:29, 57.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 15/27 [11:57<10:52, 54.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 16/27 [12:52<10:00, 54.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 17/27 [13:27<08:06, 48.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 18/27 [15:38<11:02, 73.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 19/27 [16:25<08:42, 65.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 20/27 [17:31<07:40, 65.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 21/27 [18:15<05:54, 59.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 22/27 [18:40<04:05, 49.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 23/27 [19:14<02:57, 44.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 24/27 [21:00<03:09, 63.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 25/27 [22:07<02:08, 64.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 26/27 [23:32<01:10, 70.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 27/27 [24:58<00:00, 74.94s/it]Inference: 100%|██████████| 27/27 [24:58<00:00, 55.49s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]
Inference:   0%|          | 0/21 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [01:19<26:35, 79.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 2/21 [01:57<17:25, 55.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 3/21 [04:29<29:49, 99.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 4/21 [04:59<20:22, 71.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 5/21 [06:54<23:16, 87.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 6/21 [07:24<16:58, 67.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 7/21 [07:48<12:31, 53.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 8/21 [08:28<10:40, 49.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 9/21 [09:26<10:22, 51.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 10/21 [10:15<09:23, 51.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 11/21 [11:21<09:16, 55.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 12/21 [11:51<07:09, 47.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 13/21 [12:46<06:40, 50.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 14/21 [13:23<05:22, 46.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 15/21 [14:01<04:21, 43.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 16/21 [14:24<03:06, 37.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 17/21 [15:56<03:36, 54.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 18/21 [16:56<02:46, 55.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 19/21 [17:29<01:37, 48.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 20/21 [17:57<00:42, 42.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 21/21 [19:09<00:00, 51.39s/it]Inference: 100%|██████████| 21/21 [19:09<00:00, 54.73s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/39 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:29<18:42, 29.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/39 [00:59<18:13, 29.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/39 [01:21<15:40, 26.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|█         | 4/39 [01:41<13:49, 23.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 5/39 [02:06<13:48, 24.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 6/39 [02:23<12:02, 21.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 7/39 [03:29<19:19, 36.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 8/39 [03:58<17:26, 33.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 9/39 [04:42<18:37, 37.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 10/39 [05:04<15:35, 32.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 11/39 [06:06<19:24, 41.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 12/39 [06:30<16:16, 36.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 13/39 [07:28<18:28, 42.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 14/39 [08:38<21:15, 51.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 15/39 [09:23<19:39, 49.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 16/39 [10:11<18:46, 48.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▎     | 17/39 [10:58<17:44, 48.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 18/39 [11:48<17:04, 48.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 19/39 [12:53<17:51, 53.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 20/39 [13:52<17:28, 55.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 21/39 [14:20<14:05, 46.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▋    | 22/39 [15:20<14:27, 51.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 23/39 [16:09<13:27, 50.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 24/39 [16:53<12:07, 48.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 25/39 [17:42<11:19, 48.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 26/39 [18:25<10:10, 46.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 27/39 [19:26<10:15, 51.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 28/39 [19:51<07:55, 43.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 29/39 [20:18<06:24, 38.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 30/39 [20:52<05:34, 37.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 31/39 [21:16<04:24, 33.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 32/39 [22:16<04:49, 41.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 33/39 [22:59<04:09, 41.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 34/39 [25:51<06:44, 80.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|████████▉ | 35/39 [26:27<04:29, 67.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 36/39 [27:30<03:18, 66.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 37/39 [28:06<01:54, 57.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 38/39 [28:40<00:49, 49.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 39/39 [29:31<00:00, 50.40s/it]Inference: 100%|██████████| 39/39 [29:31<00:00, 45.43s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [01:03<33:50, 63.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [02:41<43:25, 84.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [03:18<31:10, 62.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [03:55<25:22, 52.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [04:21<20:02, 42.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [05:51<26:24, 58.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [06:32<23:02, 53.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [07:05<19:22, 46.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [07:30<15:55, 39.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [08:36<18:22, 47.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [09:09<15:52, 43.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [09:41<13:58, 39.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [10:46<15:52, 47.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [11:23<13:59, 44.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [12:11<13:37, 45.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [13:10<14:04, 49.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [13:54<12:44, 47.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [14:41<11:56, 47.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [15:07<09:34, 41.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [15:42<08:32, 39.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [16:09<07:07, 35.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [17:08<07:49, 42.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [18:08<07:56, 47.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [19:08<07:43, 51.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [19:54<06:37, 49.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [20:30<05:20, 45.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [21:50<05:36, 56.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [23:04<05:06, 61.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [23:38<03:33, 53.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [24:37<02:45, 55.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [25:02<01:31, 45.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [25:46<00:45, 45.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [32:24<00:00, 151.19s/it]Inference: 100%|██████████| 33/33 [32:24<00:00, 58.93s/it] 
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/43 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:28<19:46, 28.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/43 [01:00<20:42, 30.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/43 [01:28<19:35, 29.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 4/43 [01:54<18:11, 27.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/43 [02:13<15:45, 24.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/43 [02:36<15:03, 24.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▋        | 7/43 [03:15<17:18, 28.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 8/43 [03:44<16:57, 29.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 9/43 [04:17<17:11, 30.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 10/43 [05:00<18:46, 34.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/43 [05:25<16:48, 31.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 12/43 [05:55<15:58, 30.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 13/43 [06:12<13:25, 26.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/43 [06:43<13:33, 28.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▍      | 15/43 [07:29<15:33, 33.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 16/43 [08:01<14:48, 32.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|███▉      | 17/43 [08:33<14:09, 32.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 18/43 [08:57<12:32, 30.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 19/43 [09:23<11:35, 28.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 20/43 [09:51<10:59, 28.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 21/43 [10:18<10:16, 28.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 22/43 [10:43<09:28, 27.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 23/43 [11:04<08:28, 25.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 24/43 [12:05<11:24, 36.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 25/43 [12:39<10:34, 35.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|██████    | 26/43 [12:59<08:43, 30.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 27/43 [13:42<09:14, 34.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▌   | 28/43 [14:23<09:05, 36.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 29/43 [15:00<08:31, 36.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 30/43 [15:33<07:42, 35.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 31/43 [15:53<06:09, 30.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 32/43 [16:16<05:14, 28.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 33/43 [16:57<05:21, 32.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 34/43 [17:19<04:22, 29.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 35/43 [17:48<03:53, 29.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▎ | 36/43 [18:19<03:28, 29.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 37/43 [18:44<02:50, 28.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 38/43 [19:06<02:11, 26.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 39/43 [19:41<01:55, 28.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 40/43 [20:12<01:28, 29.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 41/43 [20:42<00:59, 29.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 42/43 [21:08<00:28, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 43/43 [21:46<00:00, 31.49s/it]Inference: 100%|██████████| 43/43 [21:46<00:00, 30.39s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/41 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:21<14:25, 21.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/41 [00:42<13:45, 21.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/41 [01:03<13:29, 21.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/41 [01:33<15:03, 24.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/41 [01:58<14:54, 24.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 6/41 [02:16<13:01, 22.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/41 [02:44<13:47, 24.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|█▉        | 8/41 [03:13<14:11, 25.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 9/41 [03:42<14:19, 26.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/41 [04:25<16:22, 31.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 11/41 [04:54<15:30, 31.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 12/41 [05:17<13:44, 28.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 13/41 [05:38<12:18, 26.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 14/41 [06:13<12:55, 28.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 15/41 [06:46<13:02, 30.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 16/41 [07:36<15:00, 36.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████▏     | 17/41 [08:05<13:37, 34.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 18/41 [08:21<10:59, 28.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 19/41 [08:53<10:52, 29.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 20/41 [09:21<10:08, 28.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 21/41 [09:43<09:02, 27.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 22/41 [10:06<08:06, 25.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 23/41 [10:26<07:12, 24.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▊    | 24/41 [10:58<07:29, 26.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 25/41 [11:32<07:37, 28.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 26/41 [12:08<07:44, 30.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 27/41 [12:35<06:56, 29.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 28/41 [13:23<07:37, 35.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 29/41 [13:46<06:18, 31.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 30/41 [14:09<05:20, 29.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 31/41 [14:37<04:47, 28.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 32/41 [14:58<03:57, 26.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|████████  | 33/41 [15:19<03:17, 24.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 34/41 [15:44<02:52, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 35/41 [16:06<02:23, 23.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 36/41 [16:36<02:09, 25.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 37/41 [17:05<01:46, 26.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 38/41 [18:06<01:51, 37.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 39/41 [18:24<01:02, 31.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 40/41 [18:50<00:29, 29.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 41/41 [19:15<00:00, 28.26s/it]Inference: 100%|██████████| 41/41 [19:15<00:00, 28.18s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]
Inference:   0%|          | 0/37 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:28<16:49, 28.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/37 [00:52<15:05, 25.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/37 [01:18<14:48, 26.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 4/37 [02:19<21:58, 39.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 5/37 [02:45<18:31, 34.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▌        | 6/37 [03:05<15:16, 29.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 7/37 [03:21<12:38, 25.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 8/37 [03:52<13:02, 27.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 9/37 [04:25<13:30, 28.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 10/37 [04:58<13:34, 30.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 11/37 [05:22<12:19, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 12/37 [05:52<12:02, 28.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 13/37 [06:18<11:07, 27.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 14/37 [07:21<14:49, 38.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 15/37 [08:01<14:14, 38.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 16/37 [08:32<12:48, 36.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 17/37 [09:14<12:44, 38.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 18/37 [10:02<13:04, 41.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 19/37 [10:34<11:30, 38.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 20/37 [10:55<09:26, 33.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 21/37 [11:33<09:14, 34.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 22/37 [12:09<08:46, 35.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 23/37 [12:32<07:18, 31.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 24/37 [13:07<07:03, 32.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 25/37 [14:23<09:05, 45.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 26/37 [14:43<06:54, 37.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 27/37 [15:17<06:07, 36.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 28/37 [15:49<05:16, 35.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 29/37 [16:12<04:14, 31.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 30/37 [16:40<03:32, 30.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▍ | 31/37 [17:01<02:46, 27.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 32/37 [17:39<02:33, 30.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 33/37 [17:57<01:47, 26.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 34/37 [18:22<01:19, 26.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 35/37 [18:53<00:55, 27.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 36/37 [19:20<00:27, 27.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 37/37 [19:48<00:00, 27.66s/it]Inference: 100%|██████████| 37/37 [19:48<00:00, 32.12s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/47 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:28<21:54, 28.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▍         | 2/47 [01:05<25:10, 33.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▋         | 3/47 [01:35<23:23, 31.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▊         | 4/47 [02:01<21:09, 29.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 5/47 [02:29<20:24, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 6/47 [02:51<18:07, 26.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 7/47 [03:17<17:33, 26.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 8/47 [03:44<17:24, 26.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 9/47 [04:09<16:34, 26.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 10/47 [04:32<15:26, 25.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 11/47 [04:54<14:28, 24.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 12/47 [05:20<14:25, 24.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 13/47 [05:45<14:07, 24.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 14/47 [06:11<13:48, 25.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 15/47 [06:40<13:58, 26.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 16/47 [07:05<13:21, 25.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 17/47 [07:32<13:07, 26.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 18/47 [08:00<12:56, 26.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 19/47 [08:51<15:53, 34.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 20/47 [09:18<14:26, 32.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▍     | 21/47 [09:59<15:02, 34.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 22/47 [10:29<13:47, 33.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 23/47 [10:49<11:41, 29.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 24/47 [11:36<13:16, 34.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 25/47 [11:55<11:01, 30.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▌    | 26/47 [12:22<10:09, 29.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 27/47 [12:56<10:10, 30.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 28/47 [13:23<09:21, 29.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 29/47 [14:01<09:34, 31.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 30/47 [14:20<07:58, 28.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 31/47 [14:41<06:54, 25.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 32/47 [15:01<06:04, 24.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 33/47 [16:01<08:09, 34.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 34/47 [16:24<06:48, 31.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 35/47 [16:48<05:49, 29.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 36/47 [17:23<05:40, 30.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 37/47 [17:44<04:39, 27.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 38/47 [18:17<04:24, 29.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 39/47 [18:41<03:41, 27.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 40/47 [19:05<03:07, 26.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 41/47 [19:36<02:48, 28.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 42/47 [20:09<02:26, 29.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████▏| 43/47 [20:32<01:50, 27.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▎| 44/47 [21:00<01:23, 27.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▌| 45/47 [21:29<00:56, 28.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 46/47 [22:00<00:28, 28.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 47/47 [22:16<00:00, 24.93s/it]Inference: 100%|██████████| 47/47 [22:16<00:00, 28.43s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Inference:   0%|          | 0/54 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:23<20:43, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▎         | 2/54 [00:48<21:19, 24.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 3/54 [01:12<20:41, 24.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 4/54 [01:36<20:06, 24.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 5/54 [02:12<23:02, 28.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 6/54 [02:34<21:02, 26.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 7/54 [02:56<19:19, 24.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 8/54 [03:47<25:30, 33.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 9/54 [04:11<22:42, 30.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 10/54 [04:34<20:34, 28.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|██        | 11/54 [04:53<18:05, 25.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 12/54 [05:25<19:05, 27.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 13/54 [05:48<17:46, 26.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 14/54 [06:16<17:41, 26.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 15/54 [06:53<19:22, 29.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 16/54 [07:23<18:58, 29.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███▏      | 17/54 [07:46<17:07, 27.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 18/54 [08:17<17:12, 28.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 19/54 [08:46<16:51, 28.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 20/54 [09:25<18:01, 31.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 21/54 [09:52<16:42, 30.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 22/54 [10:29<17:13, 32.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 23/54 [11:08<17:50, 34.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 24/54 [11:35<16:06, 32.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 25/54 [12:02<14:50, 30.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 26/54 [12:25<13:15, 28.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 27/54 [12:49<12:06, 26.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 28/54 [13:19<12:07, 27.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 29/54 [13:47<11:35, 27.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 30/54 [14:13<10:54, 27.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 31/54 [14:39<10:17, 26.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 32/54 [15:44<14:04, 38.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 33/54 [16:00<11:06, 31.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 34/54 [16:25<09:53, 29.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 35/54 [17:07<10:35, 33.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 36/54 [17:36<09:35, 31.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▊   | 37/54 [18:03<08:38, 30.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 38/54 [18:35<08:14, 30.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 39/54 [19:22<08:55, 35.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 40/54 [19:48<07:39, 32.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 41/54 [20:16<06:47, 31.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 42/54 [20:47<06:17, 31.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|███████▉  | 43/54 [21:19<05:47, 31.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 44/54 [21:48<05:08, 30.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 45/54 [22:12<04:17, 28.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 46/54 [22:34<03:33, 26.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 47/54 [23:24<03:56, 33.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 48/54 [23:45<03:00, 30.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 49/54 [24:47<03:17, 39.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 50/54 [25:11<02:19, 34.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 51/54 [25:38<01:37, 32.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 52/54 [26:05<01:01, 30.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 53/54 [26:33<00:30, 30.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 54/54 [26:53<00:00, 26.98s/it]Inference: 100%|██████████| 54/54 [26:53<00:00, 29.88s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...                lora_path
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  ../finetune/is-tmmlu_2k

[1 rows x 6 columns]
  dataset_name                             path  1_tile  ...  3_tile  4_tile  5_tile
0       113-ch  ../dataset/gsat/113_chinese.csv   63.48  ...   52.90   42.32   37.03
1       112-ch  ../dataset/gsat/112_chinese.csv   62.63  ...   52.19   41.75   36.54
2       111-ch  ../dataset/gsat/111_chinese.csv   64.08  ...   48.06   42.72   32.04
3       110-ch  ../dataset/gsat/110_chinese.csv   64.57  ...   53.81   43.05   37.67
4       109-ch  ../dataset/gsat/109_chinese.csv   65.65  ...   54.71   43.77   38.30

[5 rows x 7 columns]
Running dataset:113-ch on model:breeze-7b-is_tmmlu
Running dataset:112-ch on model:breeze-7b-is_tmmlu
Running dataset:111-ch on model:breeze-7b-is_tmmlu
Running dataset:110-ch on model:breeze-7b-is_tmmlu
Running dataset:109-ch on model:breeze-7b-is_tmmlu
Running dataset:113-ns on model:breeze-7b-is_tmmlu
Running dataset:112-ns on model:breeze-7b-is_tmmlu
Running dataset:111-ns on model:breeze-7b-is_tmmlu
Running dataset:110-ns on model:breeze-7b-is_tmmlu
Running dataset:109-ns on model:breeze-7b-is_tmmlu
Running dataset:113-ss on model:breeze-7b-is_tmmlu
Running dataset:112-ss on model:breeze-7b-is_tmmlu
Running dataset:111-ss on model:breeze-7b-is_tmmlu
Running dataset:110-ss on model:breeze-7b-is_tmmlu
Running dataset:109-ss on model:breeze-7b-is_tmmlu
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/22 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:23<08:19, 23.78s/it]Inference:   9%|▉         | 2/22 [00:58<09:58, 29.94s/it]Inference:  14%|█▎        | 3/22 [01:46<12:05, 38.20s/it]Inference:  18%|█▊        | 4/22 [02:15<10:27, 34.88s/it]Inference:  23%|██▎       | 5/22 [02:56<10:28, 36.95s/it]Inference:  27%|██▋       | 6/22 [02:58<06:43, 25.21s/it]Inference:  32%|███▏      | 7/22 [03:25<06:27, 25.82s/it]Inference:  36%|███▋      | 8/22 [03:51<06:02, 25.87s/it]Inference:  41%|████      | 9/22 [04:40<07:08, 32.96s/it]Inference:  45%|████▌     | 10/22 [05:13<06:36, 33.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  50%|█████     | 11/22 [05:29<05:04, 27.64s/it]Inference:  55%|█████▍    | 12/22 [05:51<04:21, 26.13s/it]Inference:  59%|█████▉    | 13/22 [06:21<04:06, 27.35s/it]Inference:  64%|██████▎   | 14/22 [06:59<04:03, 30.39s/it]Inference:  68%|██████▊   | 15/22 [07:16<03:05, 26.54s/it]Inference:  73%|███████▎  | 16/22 [07:37<02:28, 24.80s/it]Inference:  77%|███████▋  | 17/22 [08:03<02:05, 25.16s/it]Inference:  82%|████████▏ | 18/22 [08:31<01:43, 25.82s/it]Inference:  86%|████████▋ | 19/22 [08:57<01:18, 26.07s/it]Inference:  91%|█████████ | 20/22 [09:24<00:52, 26.30s/it]Inference:  95%|█████████▌| 21/22 [10:08<00:31, 31.51s/it]Inference: 100%|██████████| 22/22 [10:53<00:00, 35.77s/it]Inference: 100%|██████████| 22/22 [10:53<00:00, 29.72s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/27 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:28<12:09, 28.07s/it]Inference:   7%|▋         | 2/27 [01:13<15:56, 38.27s/it]Inference:  11%|█         | 3/27 [01:29<11:15, 28.14s/it]Inference:  15%|█▍        | 4/27 [01:52<10:04, 26.28s/it]Inference:  19%|█▊        | 5/27 [02:15<09:10, 25.02s/it]Inference:  22%|██▏       | 6/27 [02:34<08:02, 22.96s/it]Inference:  26%|██▌       | 7/27 [02:59<07:54, 23.71s/it]Inference:  30%|██▉       | 8/27 [03:19<07:02, 22.25s/it]Inference:  33%|███▎      | 9/27 [03:30<05:39, 18.87s/it]Inference:  37%|███▋      | 10/27 [03:56<05:57, 21.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  41%|████      | 11/27 [04:29<06:33, 24.62s/it]Inference:  44%|████▍     | 12/27 [04:47<05:41, 22.78s/it]Inference:  48%|████▊     | 13/27 [05:26<06:27, 27.65s/it]Inference:  52%|█████▏    | 14/27 [05:56<06:08, 28.34s/it]Inference:  56%|█████▌    | 15/27 [06:29<05:56, 29.72s/it]Inference:  59%|█████▉    | 16/27 [06:56<05:17, 28.85s/it]Inference:  63%|██████▎   | 17/27 [07:07<03:55, 23.54s/it]Inference:  67%|██████▋   | 18/27 [08:28<06:06, 40.73s/it]Inference:  70%|███████   | 19/27 [08:45<04:30, 33.80s/it]Inference:  74%|███████▍  | 20/27 [09:03<03:23, 29.03s/it]Inference:  78%|███████▊  | 21/27 [09:32<02:54, 29.08s/it]Inference:  81%|████████▏ | 22/27 [09:48<02:04, 24.90s/it]Inference:  85%|████████▌ | 23/27 [10:07<01:33, 23.26s/it]Inference:  89%|████████▉ | 24/27 [10:48<01:25, 28.59s/it]Inference:  93%|█████████▎| 25/27 [11:10<00:53, 26.50s/it]Inference:  96%|█████████▋| 26/27 [11:47<00:29, 29.66s/it]Inference: 100%|██████████| 27/27 [12:22<00:00, 31.43s/it]Inference: 100%|██████████| 27/27 [12:22<00:00, 27.51s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/21 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [00:19<06:35, 19.79s/it]Inference:  10%|▉         | 2/21 [00:32<05:02, 15.91s/it]Inference:  14%|█▍        | 3/21 [00:47<04:33, 15.20s/it]Inference:  19%|█▉        | 4/21 [01:07<04:51, 17.17s/it]Inference:  24%|██▍       | 5/21 [01:46<06:38, 24.90s/it]Inference:  29%|██▊       | 6/21 [02:05<05:46, 23.12s/it]Inference:  33%|███▎      | 7/21 [02:45<06:40, 28.59s/it]Inference:  38%|███▊      | 8/21 [03:08<05:49, 26.86s/it]Inference:  43%|████▎     | 9/21 [03:29<04:59, 25.00s/it]Inference:  48%|████▊     | 10/21 [04:01<04:58, 27.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  52%|█████▏    | 11/21 [04:26<04:23, 26.37s/it]Inference:  57%|█████▋    | 12/21 [05:01<04:21, 29.07s/it]Inference:  62%|██████▏   | 13/21 [05:28<03:47, 28.49s/it]Inference:  67%|██████▋   | 14/21 [05:52<03:10, 27.17s/it]Inference:  71%|███████▏  | 15/21 [06:15<02:34, 25.76s/it]Inference:  76%|███████▌  | 16/21 [06:57<02:34, 30.85s/it]Inference:  81%|████████  | 17/21 [07:35<02:11, 32.75s/it]Inference:  86%|████████▌ | 18/21 [08:23<01:52, 37.40s/it]Inference:  90%|█████████ | 19/21 [09:12<01:21, 40.84s/it]Inference:  95%|█████████▌| 20/21 [09:43<00:37, 37.85s/it]Inference: 100%|██████████| 21/21 [10:20<00:00, 37.71s/it]Inference: 100%|██████████| 21/21 [10:20<00:00, 29.55s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/39 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:36<22:49, 36.03s/it]Inference:   5%|▌         | 2/39 [01:08<20:53, 33.88s/it]Inference:   8%|▊         | 3/39 [01:30<17:13, 28.71s/it]Inference:  10%|█         | 4/39 [01:58<16:28, 28.24s/it]Inference:  13%|█▎        | 5/39 [02:35<17:51, 31.52s/it]Inference:  15%|█▌        | 6/39 [02:52<14:34, 26.51s/it]Inference:  18%|█▊        | 7/39 [03:23<14:58, 28.08s/it]Inference:  21%|██        | 8/39 [03:38<12:19, 23.87s/it]Inference:  23%|██▎       | 9/39 [03:52<10:20, 20.68s/it]Inference:  26%|██▌       | 10/39 [04:11<09:47, 20.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  28%|██▊       | 11/39 [04:26<08:40, 18.59s/it]Inference:  31%|███       | 12/39 [04:57<09:59, 22.20s/it]Inference:  33%|███▎      | 13/39 [05:31<11:17, 26.06s/it]Inference:  36%|███▌      | 14/39 [05:52<10:06, 24.27s/it]Inference:  38%|███▊      | 15/39 [06:09<08:50, 22.10s/it]Inference:  41%|████      | 16/39 [06:29<08:16, 21.60s/it]Inference:  44%|████▎     | 17/39 [06:52<08:02, 21.92s/it]Inference:  46%|████▌     | 18/39 [07:19<08:15, 23.58s/it]Inference:  49%|████▊     | 19/39 [07:48<08:25, 25.27s/it]Inference:  51%|█████▏    | 20/39 [08:17<08:17, 26.20s/it]Inference:  54%|█████▍    | 21/39 [08:38<07:22, 24.59s/it]Inference:  56%|█████▋    | 22/39 [09:02<06:56, 24.52s/it]Inference:  59%|█████▉    | 23/39 [09:40<07:38, 28.66s/it]Inference:  62%|██████▏   | 24/39 [10:02<06:39, 26.63s/it]Inference:  64%|██████▍   | 25/39 [10:16<05:20, 22.93s/it]Inference:  67%|██████▋   | 26/39 [11:30<08:13, 37.97s/it]Inference:  69%|██████▉   | 27/39 [11:59<07:04, 35.36s/it]Inference:  72%|███████▏  | 28/39 [12:17<05:33, 30.35s/it]Inference:  74%|███████▍  | 29/39 [12:52<05:16, 31.70s/it]Inference:  77%|███████▋  | 30/39 [13:17<04:25, 29.50s/it]Inference:  79%|███████▉  | 31/39 [13:37<03:35, 26.89s/it]Inference:  82%|████████▏ | 32/39 [14:09<03:18, 28.35s/it]Inference:  85%|████████▍ | 33/39 [14:43<02:59, 29.94s/it]Inference:  87%|████████▋ | 34/39 [15:00<02:10, 26.02s/it]Inference:  90%|████████▉ | 35/39 [15:15<01:31, 22.85s/it]Inference:  92%|█████████▏| 36/39 [15:26<00:57, 19.23s/it]Inference:  95%|█████████▍| 37/39 [15:50<00:41, 20.73s/it]Inference:  97%|█████████▋| 38/39 [16:19<00:23, 23.04s/it]Inference: 100%|██████████| 39/39 [16:52<00:00, 26.22s/it]Inference: 100%|██████████| 39/39 [16:52<00:00, 25.97s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/33 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:12<06:49, 12.80s/it]Inference:   6%|▌         | 2/33 [00:40<11:00, 21.32s/it]Inference:   9%|▉         | 3/33 [01:13<13:20, 26.68s/it]Inference:  12%|█▏        | 4/33 [01:32<11:32, 23.87s/it]Inference:  15%|█▌        | 5/33 [01:53<10:42, 22.93s/it]Inference:  18%|█▊        | 6/33 [02:21<10:56, 24.33s/it]Inference:  21%|██        | 7/33 [02:51<11:21, 26.22s/it]Inference:  24%|██▍       | 8/33 [03:14<10:31, 25.27s/it]Inference:  27%|██▋       | 9/33 [03:36<09:39, 24.16s/it]Inference:  30%|███       | 10/33 [03:55<08:45, 22.85s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  33%|███▎      | 11/33 [04:19<08:27, 23.07s/it]Inference:  36%|███▋      | 12/33 [04:40<07:53, 22.53s/it]Inference:  39%|███▉      | 13/33 [05:07<07:53, 23.66s/it]Inference:  42%|████▏     | 14/33 [05:42<08:35, 27.13s/it]Inference:  45%|████▌     | 15/33 [06:05<07:46, 25.89s/it]Inference:  48%|████▊     | 16/33 [06:25<06:49, 24.08s/it]Inference:  52%|█████▏    | 17/33 [06:49<06:28, 24.30s/it]Inference:  55%|█████▍    | 18/33 [07:13<05:58, 23.92s/it]Inference:  58%|█████▊    | 19/33 [08:13<08:09, 34.96s/it]Inference:  61%|██████    | 20/33 [09:14<09:15, 42.70s/it]Inference:  64%|██████▎   | 21/33 [09:33<07:08, 35.68s/it]Inference:  67%|██████▋   | 22/33 [09:45<05:14, 28.58s/it]Inference:  70%|██████▉   | 23/33 [10:16<04:52, 29.24s/it]Inference:  73%|███████▎  | 24/33 [10:28<03:35, 23.99s/it]Inference:  76%|███████▌  | 25/33 [10:49<03:04, 23.01s/it]Inference:  79%|███████▉  | 26/33 [11:17<02:53, 24.78s/it]Inference:  82%|████████▏ | 27/33 [11:51<02:43, 27.29s/it]Inference:  85%|████████▍ | 28/33 [12:12<02:07, 25.53s/it]Inference:  88%|████████▊ | 29/33 [12:27<01:29, 22.36s/it]Inference:  91%|█████████ | 30/33 [13:02<01:18, 26.22s/it]Inference:  94%|█████████▍| 31/33 [13:25<00:50, 25.14s/it]Inference:  97%|█████████▋| 32/33 [14:20<00:34, 34.23s/it]Inference: 100%|██████████| 33/33 [14:42<00:00, 30.48s/it]Inference: 100%|██████████| 33/33 [14:42<00:00, 26.74s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/43 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:35<25:03, 35.80s/it]Inference:   5%|▍         | 2/43 [00:53<17:13, 25.20s/it]Inference:   7%|▋         | 3/43 [01:25<18:57, 28.44s/it]Inference:   9%|▉         | 4/43 [01:51<17:52, 27.50s/it]Inference:  12%|█▏        | 5/43 [02:24<18:31, 29.24s/it]Inference:  14%|█▍        | 6/43 [02:49<17:13, 27.94s/it]Inference:  16%|█▋        | 7/43 [04:04<26:02, 43.41s/it]Inference:  19%|█▊        | 8/43 [04:39<23:37, 40.49s/it]Inference:  21%|██        | 9/43 [04:58<19:14, 33.96s/it]Inference:  23%|██▎       | 10/43 [05:23<17:03, 31.00s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/43 [05:46<15:12, 28.53s/it]Inference:  28%|██▊       | 12/43 [06:11<14:10, 27.44s/it]Inference:  30%|███       | 13/43 [06:20<10:59, 21.97s/it]Inference:  33%|███▎      | 14/43 [06:49<11:38, 24.07s/it]Inference:  35%|███▍      | 15/43 [07:05<10:07, 21.71s/it]Inference:  37%|███▋      | 16/43 [07:42<11:53, 26.42s/it]Inference:  40%|███▉      | 17/43 [08:26<13:40, 31.55s/it]Inference:  42%|████▏     | 18/43 [08:47<11:52, 28.51s/it]Inference:  44%|████▍     | 19/43 [09:11<10:50, 27.12s/it]Inference:  47%|████▋     | 20/43 [10:05<13:29, 35.20s/it]Inference:  49%|████▉     | 21/43 [10:59<14:53, 40.63s/it]Inference:  51%|█████     | 22/43 [11:17<11:50, 33.84s/it]Inference:  53%|█████▎    | 23/43 [11:47<10:58, 32.90s/it]Inference:  56%|█████▌    | 24/43 [12:05<08:58, 28.34s/it]Inference:  58%|█████▊    | 25/43 [13:00<10:55, 36.41s/it]Inference:  60%|██████    | 26/43 [13:10<08:05, 28.56s/it]Inference:  63%|██████▎   | 27/43 [14:11<10:10, 38.13s/it]Inference:  65%|██████▌   | 28/43 [14:35<08:29, 33.98s/it]Inference:  67%|██████▋   | 29/43 [14:58<07:09, 30.70s/it]Inference:  70%|██████▉   | 30/43 [16:26<10:20, 47.76s/it]Inference:  72%|███████▏  | 31/43 [16:51<08:13, 41.14s/it]Inference:  74%|███████▍  | 32/43 [18:01<09:05, 49.63s/it]Inference:  77%|███████▋  | 33/43 [19:03<08:54, 53.45s/it]Inference:  79%|███████▉  | 34/43 [19:16<06:10, 41.21s/it]Inference:  81%|████████▏ | 35/43 [20:18<06:18, 47.34s/it]Inference:  84%|████████▎ | 36/43 [21:25<06:13, 53.37s/it]Inference:  86%|████████▌ | 37/43 [21:55<04:38, 46.46s/it]Inference:  88%|████████▊ | 38/43 [22:19<03:18, 39.74s/it]Inference:  91%|█████████ | 39/43 [23:32<03:18, 49.58s/it]Inference:  93%|█████████▎| 40/43 [23:51<02:01, 40.45s/it]Inference:  95%|█████████▌| 41/43 [24:04<01:04, 32.29s/it]Inference:  98%|█████████▊| 42/43 [24:31<00:30, 30.51s/it]Inference: 100%|██████████| 43/43 [24:52<00:00, 27.80s/it]Inference: 100%|██████████| 43/43 [24:52<00:00, 34.71s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/41 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:27<18:09, 27.24s/it]Inference:   5%|▍         | 2/41 [01:01<20:13, 31.12s/it]Inference:   7%|▋         | 3/41 [01:30<19:04, 30.13s/it]Inference:  10%|▉         | 4/41 [02:10<20:58, 34.02s/it]Inference:  12%|█▏        | 5/41 [02:28<17:00, 28.33s/it]Inference:  15%|█▍        | 6/41 [02:52<15:42, 26.92s/it]Inference:  17%|█▋        | 7/41 [03:07<13:06, 23.13s/it]Inference:  20%|█▉        | 8/41 [03:44<15:10, 27.59s/it]Inference:  22%|██▏       | 9/41 [04:07<13:48, 25.90s/it]Inference:  24%|██▍       | 10/41 [04:32<13:20, 25.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  27%|██▋       | 11/41 [04:51<11:49, 23.66s/it]Inference:  29%|██▉       | 12/41 [05:30<13:44, 28.42s/it]Inference:  32%|███▏      | 13/41 [05:49<11:50, 25.36s/it]Inference:  34%|███▍      | 14/41 [06:26<13:02, 28.97s/it]Inference:  37%|███▋      | 15/41 [07:05<13:50, 31.96s/it]Inference:  39%|███▉      | 16/41 [07:17<10:46, 25.87s/it]Inference:  41%|████▏     | 17/41 [07:33<09:09, 22.92s/it]Inference:  44%|████▍     | 18/41 [08:59<16:07, 42.06s/it]Inference:  46%|████▋     | 19/41 [09:27<13:52, 37.83s/it]Inference:  49%|████▉     | 20/41 [10:21<14:52, 42.49s/it]Inference:  51%|█████     | 21/41 [11:25<16:24, 49.23s/it]Inference:  54%|█████▎    | 22/41 [11:39<12:11, 38.51s/it]Inference:  56%|█████▌    | 23/41 [12:02<10:10, 33.90s/it]Inference:  59%|█████▊    | 24/41 [12:31<09:09, 32.31s/it]Inference:  61%|██████    | 25/41 [12:43<07:02, 26.39s/it]Inference:  63%|██████▎   | 26/41 [13:05<06:14, 24.97s/it]Inference:  66%|██████▌   | 27/41 [14:00<07:53, 33.84s/it]Inference:  68%|██████▊   | 28/41 [14:14<06:02, 27.91s/it]Inference:  71%|███████   | 29/41 [14:42<05:35, 27.97s/it]Inference:  73%|███████▎  | 30/41 [14:53<04:12, 22.94s/it]Inference:  76%|███████▌  | 31/41 [15:14<03:42, 22.26s/it]Inference:  78%|███████▊  | 32/41 [15:40<03:31, 23.47s/it]Inference:  80%|████████  | 33/41 [15:46<02:27, 18.41s/it]Inference:  83%|████████▎ | 34/41 [16:29<02:59, 25.68s/it]Inference:  85%|████████▌ | 35/41 [16:57<02:38, 26.39s/it]Inference:  88%|████████▊ | 36/41 [18:20<03:36, 43.23s/it]Inference:  90%|█████████ | 37/41 [19:11<03:02, 45.63s/it]Inference:  93%|█████████▎| 38/41 [20:09<02:28, 49.51s/it]Inference:  95%|█████████▌| 39/41 [21:16<01:49, 54.51s/it]Inference:  98%|█████████▊| 40/41 [21:33<00:43, 43.50s/it]Inference: 100%|██████████| 41/41 [22:25<00:00, 45.94s/it]Inference: 100%|██████████| 41/41 [22:25<00:00, 32.82s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/37 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:38<22:53, 38.15s/it]Inference:   5%|▌         | 2/37 [01:02<17:27, 29.94s/it]Inference:   8%|▊         | 3/37 [01:35<17:48, 31.42s/it]Inference:  11%|█         | 4/37 [02:12<18:25, 33.50s/it]Inference:  14%|█▎        | 5/37 [02:31<15:10, 28.45s/it]Inference:  16%|█▌        | 6/37 [02:50<13:03, 25.26s/it]Inference:  19%|█▉        | 7/37 [03:09<11:33, 23.11s/it]Inference:  22%|██▏       | 8/37 [03:30<10:50, 22.44s/it]Inference:  24%|██▍       | 9/37 [03:51<10:18, 22.10s/it]Inference:  27%|██▋       | 10/37 [04:32<12:28, 27.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  30%|██▉       | 11/37 [05:02<12:24, 28.62s/it]Inference:  32%|███▏      | 12/37 [05:43<13:26, 32.27s/it]Inference:  35%|███▌      | 13/37 [05:58<10:52, 27.20s/it]Inference:  38%|███▊      | 14/37 [06:34<11:22, 29.68s/it]Inference:  41%|████      | 15/37 [07:04<10:55, 29.80s/it]Inference:  43%|████▎     | 16/37 [07:50<12:08, 34.71s/it]Inference:  46%|████▌     | 17/37 [08:18<10:50, 32.54s/it]Inference:  49%|████▊     | 18/37 [08:50<10:17, 32.48s/it]Inference:  51%|█████▏    | 19/37 [09:36<11:00, 36.71s/it]Inference:  54%|█████▍    | 20/37 [10:05<09:42, 34.26s/it]Inference:  57%|█████▋    | 21/37 [10:32<08:34, 32.17s/it]Inference:  59%|█████▉    | 22/37 [11:06<08:09, 32.61s/it]Inference:  62%|██████▏   | 23/37 [11:22<06:25, 27.56s/it]Inference:  65%|██████▍   | 24/37 [12:11<07:23, 34.09s/it]Inference:  68%|██████▊   | 25/37 [12:37<06:20, 31.73s/it]Inference:  70%|███████   | 26/37 [12:54<04:59, 27.25s/it]Inference:  73%|███████▎  | 27/37 [13:43<05:37, 33.75s/it]Inference:  76%|███████▌  | 28/37 [14:02<04:23, 29.28s/it]Inference:  78%|███████▊  | 29/37 [14:52<04:45, 35.67s/it]Inference:  81%|████████  | 30/37 [15:15<03:43, 31.90s/it]Inference:  84%|████████▍ | 31/37 [15:35<02:49, 28.30s/it]Inference:  86%|████████▋ | 32/37 [15:53<02:05, 25.18s/it]Inference:  89%|████████▉ | 33/37 [16:22<01:44, 26.23s/it]Inference:  92%|█████████▏| 34/37 [17:14<01:42, 34.05s/it]Inference:  95%|█████████▍| 35/37 [17:52<01:10, 35.29s/it]Inference:  97%|█████████▋| 36/37 [18:46<00:40, 40.77s/it]Inference: 100%|██████████| 37/37 [19:07<00:00, 34.92s/it]Inference: 100%|██████████| 37/37 [19:07<00:00, 31.02s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/47 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:36<27:38, 36.06s/it]Inference:   4%|▍         | 2/47 [01:15<28:42, 38.27s/it]Inference:   6%|▋         | 3/47 [01:48<26:08, 35.66s/it]Inference:   9%|▊         | 4/47 [02:15<23:11, 32.37s/it]Inference:  11%|█         | 5/47 [02:45<21:54, 31.29s/it]Inference:  13%|█▎        | 6/47 [03:13<20:38, 30.22s/it]Inference:  15%|█▍        | 7/47 [03:30<17:20, 26.01s/it]Inference:  17%|█▋        | 8/47 [04:05<18:50, 28.98s/it]Inference:  19%|█▉        | 9/47 [04:45<20:25, 32.24s/it]Inference:  21%|██▏       | 10/47 [05:14<19:21, 31.40s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  23%|██▎       | 11/47 [05:43<18:17, 30.47s/it]Inference:  26%|██▌       | 12/47 [06:36<21:48, 37.39s/it]Inference:  28%|██▊       | 13/47 [07:01<18:59, 33.51s/it]Inference:  30%|██▉       | 14/47 [07:22<16:23, 29.80s/it]Inference:  32%|███▏      | 15/47 [07:45<14:48, 27.76s/it]Inference:  34%|███▍      | 16/47 [08:29<16:52, 32.66s/it]Inference:  36%|███▌      | 17/47 [08:53<15:01, 30.06s/it]Inference:  38%|███▊      | 18/47 [09:36<16:24, 33.96s/it]Inference:  40%|████      | 19/47 [10:04<15:02, 32.24s/it]Inference:  43%|████▎     | 20/47 [10:35<14:19, 31.85s/it]Inference:  45%|████▍     | 21/47 [11:04<13:24, 30.93s/it]Inference:  47%|████▋     | 22/47 [11:37<13:08, 31.56s/it]Inference:  49%|████▉     | 23/47 [12:14<13:15, 33.16s/it]Inference:  51%|█████     | 24/47 [12:39<11:47, 30.76s/it]Inference:  53%|█████▎    | 25/47 [13:01<10:20, 28.20s/it]Inference:  55%|█████▌    | 26/47 [13:20<08:52, 25.36s/it]Inference:  57%|█████▋    | 27/47 [13:48<08:44, 26.22s/it]Inference:  60%|█████▉    | 28/47 [14:21<08:57, 28.28s/it]Inference:  62%|██████▏   | 29/47 [14:33<07:00, 23.35s/it]Inference:  64%|██████▍   | 30/47 [14:46<05:44, 20.28s/it]Inference:  66%|██████▌   | 31/47 [15:04<05:14, 19.63s/it]Inference:  68%|██████▊   | 32/47 [15:23<04:51, 19.44s/it]Inference:  70%|███████   | 33/47 [15:47<04:52, 20.87s/it]Inference:  72%|███████▏  | 34/47 [16:10<04:37, 21.35s/it]Inference:  74%|███████▍  | 35/47 [16:47<05:12, 26.04s/it]Inference:  77%|███████▋  | 36/47 [17:05<04:21, 23.79s/it]Inference:  79%|███████▊  | 37/47 [17:16<03:16, 19.70s/it]Inference:  81%|████████  | 38/47 [17:28<02:36, 17.44s/it]Inference:  83%|████████▎ | 39/47 [17:40<02:07, 15.97s/it]Inference:  85%|████████▌ | 40/47 [19:09<04:23, 37.68s/it]Inference:  87%|████████▋ | 41/47 [19:33<03:22, 33.80s/it]Inference:  89%|████████▉ | 42/47 [20:34<03:29, 41.82s/it]Inference:  91%|█████████▏| 43/47 [21:05<02:34, 38.55s/it]Inference:  94%|█████████▎| 44/47 [21:25<01:38, 32.96s/it]Inference:  96%|█████████▌| 45/47 [21:59<01:06, 33.30s/it]Inference:  98%|█████████▊| 46/47 [22:11<00:27, 27.09s/it]Inference: 100%|██████████| 47/47 [22:20<00:00, 21.63s/it]Inference: 100%|██████████| 47/47 [22:20<00:00, 28.53s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/54 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:31<27:26, 31.06s/it]Inference:   4%|▎         | 2/54 [00:51<21:26, 24.75s/it]Inference:   6%|▌         | 3/54 [01:08<17:52, 21.04s/it]Inference:   7%|▋         | 4/54 [01:47<23:35, 28.31s/it]Inference:   9%|▉         | 5/54 [02:22<25:08, 30.78s/it]Inference:  11%|█         | 6/54 [02:55<25:12, 31.51s/it]Inference:  13%|█▎        | 7/54 [03:25<24:10, 30.86s/it]Inference:  15%|█▍        | 8/54 [04:23<30:21, 39.60s/it]Inference:  17%|█▋        | 9/54 [05:05<30:17, 40.39s/it]Inference:  19%|█▊        | 10/54 [05:14<22:31, 30.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  20%|██        | 11/54 [06:05<26:33, 37.05s/it]Inference:  22%|██▏       | 12/54 [06:52<27:51, 39.80s/it]Inference:  24%|██▍       | 13/54 [07:15<23:46, 34.80s/it]Inference:  26%|██▌       | 14/54 [07:55<24:14, 36.35s/it]Inference:  28%|██▊       | 15/54 [08:31<23:38, 36.38s/it]Inference:  30%|██▉       | 16/54 [08:56<20:44, 32.75s/it]Inference:  31%|███▏      | 17/54 [09:22<19:00, 30.82s/it]Inference:  33%|███▎      | 18/54 [09:49<17:48, 29.67s/it]Inference:  35%|███▌      | 19/54 [10:23<18:00, 30.87s/it]Inference:  37%|███▋      | 20/54 [10:51<17:08, 30.25s/it]Inference:  39%|███▉      | 21/54 [11:30<18:03, 32.83s/it]Inference:  41%|████      | 22/54 [11:52<15:44, 29.50s/it]Inference:  43%|████▎     | 23/54 [12:31<16:44, 32.41s/it]Inference:  44%|████▍     | 24/54 [12:43<13:09, 26.33s/it]Inference:  46%|████▋     | 25/54 [13:05<11:59, 24.83s/it]Inference:  48%|████▊     | 26/54 [13:25<10:55, 23.41s/it]Inference:  50%|█████     | 27/54 [13:32<08:21, 18.56s/it]Inference:  52%|█████▏    | 28/54 [13:54<08:32, 19.72s/it]Inference:  54%|█████▎    | 29/54 [14:38<11:09, 26.78s/it]Inference:  56%|█████▌    | 30/54 [14:59<10:00, 25.02s/it]Inference:  57%|█████▋    | 31/54 [15:05<07:24, 19.31s/it]Inference:  59%|█████▉    | 32/54 [15:33<08:05, 22.05s/it]Inference:  61%|██████    | 33/54 [15:43<06:27, 18.47s/it]Inference:  63%|██████▎   | 34/54 [15:53<05:18, 15.93s/it]Inference:  65%|██████▍   | 35/54 [16:17<05:48, 18.34s/it]Inference:  67%|██████▋   | 36/54 [17:13<08:54, 29.69s/it]Inference:  69%|██████▊   | 37/54 [17:26<06:59, 24.67s/it]Inference:  70%|███████   | 38/54 [17:55<06:53, 25.82s/it]Inference:  72%|███████▏  | 39/54 [18:44<08:10, 32.72s/it]Inference:  74%|███████▍  | 40/54 [19:09<07:06, 30.49s/it]Inference:  76%|███████▌  | 41/54 [20:01<08:01, 37.05s/it]Inference:  78%|███████▊  | 42/54 [20:18<06:13, 31.11s/it]Inference:  80%|███████▉  | 43/54 [20:33<04:46, 26.05s/it]Inference:  81%|████████▏ | 44/54 [21:02<04:29, 26.99s/it]Inference:  83%|████████▎ | 45/54 [21:26<03:55, 26.15s/it]Inference:  85%|████████▌ | 46/54 [21:44<03:09, 23.75s/it]Inference:  87%|████████▋ | 47/54 [22:14<02:57, 25.43s/it]Inference:  89%|████████▉ | 48/54 [22:37<02:29, 24.91s/it]Inference:  91%|█████████ | 49/54 [22:59<02:00, 24.10s/it]Inference:  93%|█████████▎| 50/54 [24:14<02:36, 39.20s/it]Inference:  94%|█████████▍| 51/54 [24:37<01:42, 34.28s/it]Inference:  96%|█████████▋| 52/54 [25:02<01:03, 31.62s/it]Inference:  98%|█████████▊| 53/54 [25:10<00:24, 24.62s/it]Inference: 100%|██████████| 54/54 [25:31<00:00, 23.49s/it]Inference: 100%|██████████| 54/54 [25:31<00:00, 28.37s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/33 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:11<06:11, 11.60s/it]Inference:   6%|▌         | 2/33 [00:32<08:45, 16.94s/it]Inference:   9%|▉         | 3/33 [00:56<10:04, 20.14s/it]Inference:  12%|█▏        | 4/33 [01:25<11:31, 23.85s/it]Inference:  15%|█▌        | 5/33 [01:50<11:16, 24.15s/it]Inference:  18%|█▊        | 6/33 [03:07<19:01, 42.27s/it]Inference:  21%|██        | 7/33 [04:36<24:51, 57.38s/it]Inference:  24%|██▍       | 8/33 [06:05<28:03, 67.35s/it]Inference:  27%|██▋       | 9/33 [06:18<20:14, 50.61s/it]Inference:  30%|███       | 10/33 [07:09<19:22, 50.56s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  33%|███▎      | 11/33 [07:45<16:52, 46.02s/it]Inference:  36%|███▋      | 12/33 [08:05<13:22, 38.20s/it]Inference:  39%|███▉      | 13/33 [08:27<11:09, 33.47s/it]Inference:  42%|████▏     | 14/33 [09:03<10:48, 34.14s/it]Inference:  45%|████▌     | 15/33 [09:59<12:14, 40.82s/it]Inference:  48%|████▊     | 16/33 [10:18<09:42, 34.24s/it]Inference:  52%|█████▏    | 17/33 [10:26<07:01, 26.34s/it]Inference:  55%|█████▍    | 18/33 [10:32<05:00, 20.03s/it]Inference:  58%|█████▊    | 19/33 [11:16<06:23, 27.41s/it]Inference:  61%|██████    | 20/33 [12:33<09:08, 42.19s/it]Inference:  64%|██████▎   | 21/33 [12:58<07:22, 36.91s/it]Inference:  67%|██████▋   | 22/33 [13:25<06:16, 34.22s/it]Inference:  70%|██████▉   | 23/33 [13:50<05:12, 31.29s/it]Inference:  73%|███████▎  | 24/33 [15:17<07:13, 48.16s/it]Inference:  76%|███████▌  | 25/33 [15:49<05:44, 43.08s/it]Inference:  79%|███████▉  | 26/33 [16:10<04:16, 36.65s/it]Inference:  82%|████████▏ | 27/33 [16:20<02:51, 28.60s/it]Inference:  85%|████████▍ | 28/33 [16:46<02:19, 27.88s/it]Inference:  88%|████████▊ | 29/33 [18:14<03:03, 45.83s/it]Inference:  91%|█████████ | 30/33 [18:42<02:01, 40.46s/it]Inference:  94%|█████████▍| 31/33 [19:09<01:12, 36.42s/it]Inference:  97%|█████████▋| 32/33 [19:12<00:26, 26.54s/it]Inference: 100%|██████████| 33/33 [19:27<00:00, 22.88s/it]Inference: 100%|██████████| 33/33 [19:27<00:00, 35.37s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:13<07:16, 13.24s/it]Inference:   6%|▌         | 2/34 [00:29<08:06, 15.20s/it]Inference:   9%|▉         | 3/34 [00:50<09:03, 17.54s/it]Inference:  12%|█▏        | 4/34 [01:57<18:34, 37.14s/it]Inference:  15%|█▍        | 5/34 [02:22<15:51, 32.80s/it]Inference:  18%|█▊        | 6/34 [02:59<15:59, 34.28s/it]Inference:  21%|██        | 7/34 [03:23<13:52, 30.83s/it]Inference:  24%|██▎       | 8/34 [04:30<18:22, 42.39s/it]Inference:  26%|██▋       | 9/34 [05:07<16:59, 40.78s/it]Inference:  29%|██▉       | 10/34 [06:14<19:28, 48.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [06:34<15:20, 40.04s/it]Inference:  35%|███▌      | 12/34 [08:01<19:57, 54.45s/it]Inference:  38%|███▊      | 13/34 [08:08<13:58, 39.92s/it]Inference:  41%|████      | 14/34 [08:27<11:12, 33.62s/it]Inference:  44%|████▍     | 15/34 [08:52<09:49, 31.03s/it]Inference:  47%|████▋     | 16/34 [09:16<08:41, 28.99s/it]Inference:  50%|█████     | 17/34 [09:30<06:54, 24.35s/it]Inference:  53%|█████▎    | 18/34 [09:59<06:51, 25.69s/it]Inference:  56%|█████▌    | 19/34 [10:23<06:19, 25.28s/it]Inference:  59%|█████▉    | 20/34 [10:54<06:17, 26.97s/it]Inference:  62%|██████▏   | 21/34 [11:13<05:18, 24.52s/it]Inference:  65%|██████▍   | 22/34 [11:39<05:00, 25.04s/it]Inference:  68%|██████▊   | 23/34 [13:07<08:02, 43.84s/it]Inference:  71%|███████   | 24/34 [14:35<09:32, 57.21s/it]Inference:  74%|███████▎  | 25/34 [14:53<06:50, 45.56s/it]Inference:  76%|███████▋  | 26/34 [15:24<05:28, 41.07s/it]Inference:  79%|███████▉  | 27/34 [15:39<03:51, 33.13s/it]Inference:  82%|████████▏ | 28/34 [16:39<04:07, 41.20s/it]Inference:  85%|████████▌ | 29/34 [17:06<03:05, 37.08s/it]Inference:  88%|████████▊ | 30/34 [17:42<02:27, 36.76s/it]Inference:  91%|█████████ | 31/34 [19:11<02:37, 52.51s/it]Inference:  94%|█████████▍| 32/34 [19:48<01:35, 47.71s/it]Inference:  97%|█████████▋| 33/34 [19:54<00:35, 35.17s/it]Inference: 100%|██████████| 34/34 [20:08<00:00, 28.82s/it]Inference: 100%|██████████| 34/34 [20:08<00:00, 35.54s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:15<08:18, 15.11s/it]Inference:   6%|▌         | 2/34 [00:38<10:46, 20.22s/it]Inference:   9%|▉         | 3/34 [01:00<10:49, 20.94s/it]Inference:  12%|█▏        | 4/34 [02:05<19:06, 38.21s/it]Inference:  15%|█▍        | 5/34 [02:18<14:08, 29.27s/it]Inference:  18%|█▊        | 6/34 [02:40<12:25, 26.64s/it]Inference:  21%|██        | 7/34 [03:00<10:59, 24.44s/it]Inference:  24%|██▎       | 8/34 [03:26<10:51, 25.08s/it]Inference:  26%|██▋       | 9/34 [03:31<07:51, 18.87s/it]Inference:  29%|██▉       | 10/34 [03:53<07:56, 19.84s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [05:16<15:01, 39.17s/it]Inference:  35%|███▌      | 12/34 [05:30<11:28, 31.29s/it]Inference:  38%|███▊      | 13/34 [05:58<10:36, 30.32s/it]Inference:  41%|████      | 14/34 [06:20<09:17, 27.89s/it]Inference:  44%|████▍     | 15/34 [06:43<08:22, 26.44s/it]Inference:  47%|████▋     | 16/34 [07:12<08:08, 27.15s/it]Inference:  50%|█████     | 17/34 [07:55<09:03, 31.98s/it]Inference:  53%|█████▎    | 18/34 [08:54<10:38, 39.91s/it]Inference:  56%|█████▌    | 19/34 [09:14<08:30, 34.04s/it]Inference:  59%|█████▉    | 20/34 [10:41<11:40, 50.04s/it]Inference:  62%|██████▏   | 21/34 [11:17<09:52, 45.61s/it]Inference:  65%|██████▍   | 22/34 [12:45<11:41, 58.45s/it]Inference:  68%|██████▊   | 23/34 [13:39<10:29, 57.20s/it]Inference:  71%|███████   | 24/34 [14:45<09:58, 59.83s/it]Inference:  74%|███████▎  | 25/34 [15:09<07:21, 49.02s/it]Inference:  76%|███████▋  | 26/34 [15:17<04:54, 36.86s/it]Inference:  79%|███████▉  | 27/34 [15:51<04:12, 36.02s/it]Inference:  82%|████████▏ | 28/34 [16:58<04:30, 45.11s/it]Inference:  85%|████████▌ | 29/34 [17:52<03:59, 47.82s/it]Inference:  88%|████████▊ | 30/34 [18:24<02:52, 43.18s/it]Inference:  91%|█████████ | 31/34 [18:34<01:39, 33.08s/it]Inference:  94%|█████████▍| 32/34 [19:41<01:26, 43.46s/it]Inference:  97%|█████████▋| 33/34 [20:56<00:52, 52.72s/it]Inference: 100%|██████████| 34/34 [21:11<00:00, 41.48s/it]Inference: 100%|██████████| 34/34 [21:11<00:00, 37.40s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:16<11:35, 16.96s/it]Inference:   5%|▍         | 2/42 [00:41<14:06, 21.16s/it]Inference:   7%|▋         | 3/42 [00:53<11:10, 17.19s/it]Inference:  10%|▉         | 4/42 [01:05<09:33, 15.09s/it]Inference:  12%|█▏        | 5/42 [01:22<09:43, 15.76s/it]Inference:  14%|█▍        | 6/42 [01:43<10:29, 17.49s/it]Inference:  17%|█▋        | 7/42 [02:10<12:03, 20.66s/it]Inference:  19%|█▉        | 8/42 [02:45<14:23, 25.41s/it]Inference:  21%|██▏       | 9/42 [03:09<13:35, 24.72s/it]Inference:  24%|██▍       | 10/42 [03:53<16:21, 30.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [04:13<14:13, 27.53s/it]Inference:  29%|██▊       | 12/42 [04:58<16:24, 32.80s/it]Inference:  31%|███       | 13/42 [05:23<14:40, 30.36s/it]Inference:  33%|███▎      | 14/42 [06:09<16:23, 35.12s/it]Inference:  36%|███▌      | 15/42 [06:44<15:46, 35.05s/it]Inference:  38%|███▊      | 16/42 [07:11<14:09, 32.69s/it]Inference:  40%|████      | 17/42 [08:18<17:54, 42.98s/it]Inference:  43%|████▎     | 18/42 [08:50<15:51, 39.66s/it]Inference:  45%|████▌     | 19/42 [09:05<12:24, 32.39s/it]Inference:  48%|████▊     | 20/42 [10:27<17:17, 47.14s/it]Inference:  50%|█████     | 21/42 [10:39<12:52, 36.76s/it]Inference:  52%|█████▏    | 22/42 [11:24<13:00, 39.02s/it]Inference:  55%|█████▍    | 23/42 [11:48<10:57, 34.61s/it]Inference:  57%|█████▋    | 24/42 [12:14<09:37, 32.06s/it]Inference:  60%|█████▉    | 25/42 [12:34<08:04, 28.49s/it]Inference:  62%|██████▏   | 26/42 [13:35<10:10, 38.16s/it]Inference:  64%|██████▍   | 27/42 [15:03<13:18, 53.24s/it]Inference:  67%|██████▋   | 28/42 [15:30<10:33, 45.25s/it]Inference:  69%|██████▉   | 29/42 [15:45<07:48, 36.06s/it]Inference:  71%|███████▏  | 30/42 [16:01<06:02, 30.22s/it]Inference:  74%|███████▍  | 31/42 [16:18<04:49, 26.31s/it]Inference:  76%|███████▌  | 32/42 [16:39<04:05, 24.58s/it]Inference:  79%|███████▊  | 33/42 [16:54<03:15, 21.75s/it]Inference:  81%|████████  | 34/42 [17:10<02:39, 19.96s/it]Inference:  83%|████████▎ | 35/42 [17:27<02:13, 19.11s/it]Inference:  86%|████████▌ | 36/42 [18:13<02:43, 27.31s/it]Inference:  88%|████████▊ | 37/42 [18:43<02:20, 28.12s/it]Inference:  90%|█████████ | 38/42 [19:10<01:50, 27.68s/it]Inference:  93%|█████████▎| 39/42 [19:43<01:28, 29.36s/it]Inference:  95%|█████████▌| 40/42 [20:17<01:01, 30.78s/it]Inference:  98%|█████████▊| 41/42 [20:40<00:28, 28.38s/it]Inference: 100%|██████████| 42/42 [21:20<00:00, 31.94s/it]Inference: 100%|██████████| 42/42 [21:20<00:00, 30.50s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:19<13:32, 19.82s/it]Inference:   5%|▍         | 2/42 [00:43<14:52, 22.32s/it]Inference:   7%|▋         | 3/42 [01:10<15:49, 24.36s/it]Inference:  10%|▉         | 4/42 [01:50<19:20, 30.54s/it]Inference:  12%|█▏        | 5/42 [02:21<18:50, 30.56s/it]Inference:  14%|█▍        | 6/42 [02:52<18:22, 30.63s/it]Inference:  17%|█▋        | 7/42 [03:47<22:39, 38.83s/it]Inference:  19%|█▉        | 8/42 [04:21<21:05, 37.23s/it]Inference:  21%|██▏       | 9/42 [05:06<21:44, 39.53s/it]Inference:  24%|██▍       | 10/42 [05:22<17:19, 32.49s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [05:48<15:39, 30.31s/it]Inference:  29%|██▊       | 12/42 [06:35<17:47, 35.59s/it]Inference:  31%|███       | 13/42 [07:07<16:36, 34.35s/it]Inference:  33%|███▎      | 14/42 [07:17<12:33, 26.91s/it]Inference:  36%|███▌      | 15/42 [07:31<10:24, 23.13s/it]Inference:  38%|███▊      | 16/42 [07:58<10:34, 24.40s/it]Inference:  40%|████      | 17/42 [08:47<13:10, 31.64s/it]Inference:  43%|████▎     | 18/42 [09:05<11:00, 27.50s/it]Inference:  45%|████▌     | 19/42 [09:12<08:09, 21.29s/it]Inference:  48%|████▊     | 20/42 [09:30<07:28, 20.39s/it]Inference:  50%|█████     | 21/42 [09:54<07:32, 21.54s/it]Inference:  52%|█████▏    | 22/42 [10:02<05:52, 17.60s/it]Inference:  55%|█████▍    | 23/42 [10:26<06:08, 19.37s/it]Inference:  57%|█████▋    | 24/42 [10:43<05:35, 18.64s/it]Inference:  60%|█████▉    | 25/42 [10:59<05:02, 17.81s/it]Inference:  62%|██████▏   | 26/42 [11:21<05:05, 19.09s/it]Inference:  64%|██████▍   | 27/42 [11:52<05:42, 22.84s/it]Inference:  67%|██████▋   | 28/42 [12:06<04:42, 20.17s/it]Inference:  69%|██████▉   | 29/42 [12:28<04:29, 20.73s/it]Inference:  71%|███████▏  | 30/42 [13:57<08:11, 40.95s/it]Inference:  74%|███████▍  | 31/42 [14:17<06:22, 34.74s/it]Inference:  76%|███████▌  | 32/42 [14:42<05:18, 31.89s/it]Inference:  79%|███████▊  | 33/42 [15:12<04:41, 31.31s/it]Inference:  81%|████████  | 34/42 [15:35<03:51, 28.94s/it]Inference:  83%|████████▎ | 35/42 [16:13<03:40, 31.54s/it]Inference:  86%|████████▌ | 36/42 [16:40<03:00, 30.12s/it]Inference:  88%|████████▊ | 37/42 [17:52<03:33, 42.73s/it]Inference:  90%|█████████ | 38/42 [18:41<02:58, 44.72s/it]Inference:  93%|█████████▎| 39/42 [19:35<02:21, 47.30s/it]Inference:  95%|█████████▌| 40/42 [20:30<01:39, 49.79s/it]Inference:  98%|█████████▊| 41/42 [21:13<00:47, 47.81s/it]Inference: 100%|██████████| 42/42 [22:08<00:00, 49.73s/it]Inference: 100%|██████████| 42/42 [22:08<00:00, 31.62s/it]
/home/namwoam/dl-final/llm/translate.py:74: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "translated_questions"] = translated_questions
  dataset_name                                      path  ...  4_tile  5_tile
0       113-ns  ../dataset/gsat/113_natural_sciences.csv  ...   31.41   25.13
1       112-ns  ../dataset/gsat/112_natural_sciences.csv  ...   29.58   23.66
2       111-ns  ../dataset/gsat/111_natural_sciences.csv  ...   37.74   25.16
3       110-ns  ../dataset/gsat/110_natural_sciences.csv  ...   32.31   25.85
4       109-ns  ../dataset/gsat/109_natural_sciences.csv  ...   32.63   26.10

[5 rows x 7 columns]
Running dataset:113-ns on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:112-ns on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:111-ns on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:110-ns on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:109-ns on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:113-ss on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:112-ss on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:111-ss on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:110-ss on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:109-ss on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:113-ch on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:112-ch on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:111-ch on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:110-ch on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
Running dataset:109-ch on model:MediaTek-Research/Breeze-7B-32k-Instruct-v1_0
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/22 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:21<07:32, 21.56s/it]Inference:   9%|▉         | 2/22 [00:44<07:30, 22.53s/it]Inference:  14%|█▎        | 3/22 [01:08<07:17, 23.03s/it]Inference:  18%|█▊        | 4/22 [01:27<06:30, 21.67s/it]Inference:  23%|██▎       | 5/22 [01:57<06:54, 24.40s/it]Inference:  27%|██▋       | 6/22 [02:02<04:46, 17.88s/it]Inference:  32%|███▏      | 7/22 [03:08<08:26, 33.78s/it]Inference:  36%|███▋      | 8/22 [03:44<07:59, 34.23s/it]Inference:  41%|████      | 9/22 [04:10<06:52, 31.69s/it]Inference:  45%|████▌     | 10/22 [04:55<07:11, 35.97s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  50%|█████     | 11/22 [05:26<06:18, 34.45s/it]Inference:  55%|█████▍    | 12/22 [05:50<05:10, 31.04s/it]Inference:  59%|█████▉    | 13/22 [06:28<05:00, 33.34s/it]Inference:  64%|██████▎   | 14/22 [07:34<05:44, 43.02s/it]Inference:  68%|██████▊   | 15/22 [08:07<04:41, 40.26s/it]Inference:  73%|███████▎  | 16/22 [08:41<03:49, 38.20s/it]Inference:  77%|███████▋  | 17/22 [09:00<02:41, 32.40s/it]Inference:  82%|████████▏ | 18/22 [09:39<02:18, 34.55s/it]Inference:  86%|████████▋ | 19/22 [10:08<01:38, 32.70s/it]Inference:  91%|█████████ | 20/22 [10:22<00:54, 27.24s/it]Inference:  95%|█████████▌| 21/22 [10:38<00:23, 23.81s/it]Inference: 100%|██████████| 22/22 [10:51<00:00, 20.68s/it]Inference: 100%|██████████| 22/22 [10:51<00:00, 29.63s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/27 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:16<07:04, 16.34s/it]Inference:   7%|▋         | 2/27 [00:31<06:33, 15.73s/it]Inference:  11%|█         | 3/27 [00:52<07:14, 18.12s/it]Inference:  15%|█▍        | 4/27 [01:27<09:26, 24.63s/it]Inference:  19%|█▊        | 5/27 [01:54<09:19, 25.44s/it]Inference:  22%|██▏       | 6/27 [02:24<09:33, 27.30s/it]Inference:  26%|██▌       | 7/27 [02:56<09:33, 28.67s/it]Inference:  30%|██▉       | 8/27 [03:29<09:32, 30.13s/it]Inference:  33%|███▎      | 9/27 [03:50<08:08, 27.14s/it]Inference:  37%|███▋      | 10/27 [04:26<08:28, 29.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  41%|████      | 11/27 [04:44<06:58, 26.17s/it]Inference:  44%|████▍     | 12/27 [05:06<06:16, 25.07s/it]Inference:  48%|████▊     | 13/27 [05:25<05:22, 23.06s/it]Inference:  52%|█████▏    | 14/27 [05:56<05:32, 25.56s/it]Inference:  56%|█████▌    | 15/27 [06:36<05:59, 29.97s/it]Inference:  59%|█████▉    | 16/27 [07:00<05:09, 28.18s/it]Inference:  63%|██████▎   | 17/27 [07:23<04:24, 26.44s/it]Inference:  67%|██████▋   | 18/27 [08:24<05:33, 37.03s/it]Inference:  70%|███████   | 19/27 [08:41<04:08, 31.07s/it]Inference:  74%|███████▍  | 20/27 [09:07<03:27, 29.58s/it]Inference:  78%|███████▊  | 21/27 [09:30<02:44, 27.42s/it]Inference:  81%|████████▏ | 22/27 [10:14<02:42, 32.54s/it]Inference:  85%|████████▌ | 23/27 [10:32<01:52, 28.07s/it]Inference:  89%|████████▉ | 24/27 [11:13<01:35, 31.90s/it]Inference:  93%|█████████▎| 25/27 [11:39<01:00, 30.11s/it]Inference:  96%|█████████▋| 26/27 [12:01<00:27, 27.60s/it]Inference: 100%|██████████| 27/27 [12:48<00:00, 33.51s/it]Inference: 100%|██████████| 27/27 [12:48<00:00, 28.46s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/21 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [01:06<22:09, 66.50s/it]Inference:  10%|▉         | 2/21 [01:26<12:22, 39.10s/it]Inference:  14%|█▍        | 3/21 [02:10<12:22, 41.28s/it]Inference:  19%|█▉        | 4/21 [02:37<10:04, 35.55s/it]Inference:  24%|██▍       | 5/21 [03:53<13:24, 50.27s/it]Inference:  29%|██▊       | 6/21 [04:30<11:26, 45.77s/it]Inference:  33%|███▎      | 7/21 [04:51<08:48, 37.78s/it]Inference:  38%|███▊      | 8/21 [05:19<07:28, 34.48s/it]Inference:  43%|████▎     | 9/21 [05:41<06:09, 30.80s/it]Inference:  48%|████▊     | 10/21 [06:18<05:57, 32.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  52%|█████▏    | 11/21 [06:54<05:37, 33.79s/it]Inference:  57%|█████▋    | 12/21 [07:40<05:36, 37.42s/it]Inference:  62%|██████▏   | 13/21 [08:06<04:30, 33.75s/it]Inference:  67%|██████▋   | 14/21 [08:52<04:22, 37.57s/it]Inference:  71%|███████▏  | 15/21 [09:32<03:49, 38.32s/it]Inference:  76%|███████▌  | 16/21 [10:07<03:06, 37.29s/it]Inference:  81%|████████  | 17/21 [10:31<02:13, 33.32s/it]Inference:  86%|████████▌ | 18/21 [11:09<01:44, 34.71s/it]Inference:  90%|█████████ | 19/21 [11:46<01:10, 35.49s/it]Inference:  95%|█████████▌| 20/21 [12:09<00:31, 31.73s/it]Inference: 100%|██████████| 21/21 [12:36<00:00, 30.15s/it]Inference: 100%|██████████| 21/21 [12:36<00:00, 36.01s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/39 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:22<14:31, 22.93s/it]Inference:   5%|▌         | 2/39 [00:51<16:18, 26.45s/it]Inference:   8%|▊         | 3/39 [01:12<14:12, 23.69s/it]Inference:  10%|█         | 4/39 [01:45<16:06, 27.61s/it]Inference:  13%|█▎        | 5/39 [02:32<19:34, 34.54s/it]Inference:  15%|█▌        | 6/39 [03:04<18:24, 33.46s/it]Inference:  18%|█▊        | 7/39 [03:32<17:02, 31.97s/it]Inference:  21%|██        | 8/39 [04:09<17:17, 33.48s/it]Inference:  23%|██▎       | 9/39 [04:34<15:23, 30.80s/it]Inference:  26%|██▌       | 10/39 [05:04<14:44, 30.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  28%|██▊       | 11/39 [05:28<13:15, 28.39s/it]Inference:  31%|███       | 12/39 [05:54<12:33, 27.89s/it]Inference:  33%|███▎      | 13/39 [06:07<10:06, 23.34s/it]Inference:  36%|███▌      | 14/39 [06:25<09:01, 21.65s/it]Inference:  38%|███▊      | 15/39 [06:46<08:35, 21.46s/it]Inference:  41%|████      | 16/39 [07:22<09:55, 25.87s/it]Inference:  44%|████▎     | 17/39 [07:58<10:34, 28.86s/it]Inference:  46%|████▌     | 18/39 [08:29<10:22, 29.62s/it]Inference:  49%|████▊     | 19/39 [08:54<09:24, 28.22s/it]Inference:  51%|█████▏    | 20/39 [09:18<08:30, 26.89s/it]Inference:  54%|█████▍    | 21/39 [09:41<07:45, 25.88s/it]Inference:  56%|█████▋    | 22/39 [10:19<08:18, 29.31s/it]Inference:  59%|█████▉    | 23/39 [10:41<07:16, 27.28s/it]Inference:  62%|██████▏   | 24/39 [11:18<07:33, 30.24s/it]Inference:  64%|██████▍   | 25/39 [11:40<06:26, 27.62s/it]Inference:  67%|██████▋   | 26/39 [12:03<05:41, 26.23s/it]Inference:  69%|██████▉   | 27/39 [12:39<05:49, 29.12s/it]Inference:  72%|███████▏  | 28/39 [13:07<05:17, 28.86s/it]Inference:  74%|███████▍  | 29/39 [13:39<04:58, 29.81s/it]Inference:  77%|███████▋  | 30/39 [14:18<04:54, 32.67s/it]Inference:  79%|███████▉  | 31/39 [14:34<03:39, 27.45s/it]Inference:  82%|████████▏ | 32/39 [15:28<04:08, 35.43s/it]Inference:  85%|████████▍ | 33/39 [15:53<03:13, 32.26s/it]Inference:  87%|████████▋ | 34/39 [17:04<03:40, 44.07s/it]Inference:  90%|████████▉ | 35/39 [17:42<02:48, 42.15s/it]Inference:  92%|█████████▏| 36/39 [17:56<01:40, 33.63s/it]Inference:  95%|█████████▍| 37/39 [18:20<01:01, 30.81s/it]Inference:  97%|█████████▋| 38/39 [18:42<00:28, 28.24s/it]Inference: 100%|██████████| 39/39 [19:09<00:00, 27.69s/it]Inference: 100%|██████████| 39/39 [19:09<00:00, 29.46s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/33 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:26<14:09, 26.56s/it]Inference:   6%|▌         | 2/33 [01:16<20:50, 40.35s/it]Inference:   9%|▉         | 3/33 [01:42<16:48, 33.62s/it]Inference:  12%|█▏        | 4/33 [01:59<13:11, 27.28s/it]Inference:  15%|█▌        | 5/33 [02:24<12:22, 26.53s/it]Inference:  18%|█▊        | 6/33 [02:51<11:52, 26.39s/it]Inference:  21%|██        | 7/33 [03:13<10:53, 25.13s/it]Inference:  24%|██▍       | 8/33 [03:48<11:43, 28.15s/it]Inference:  27%|██▋       | 9/33 [04:10<10:34, 26.45s/it]Inference:  30%|███       | 10/33 [04:35<09:53, 25.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  33%|███▎      | 11/33 [04:57<09:03, 24.71s/it]Inference:  36%|███▋      | 12/33 [05:34<09:58, 28.49s/it]Inference:  39%|███▉      | 13/33 [06:03<09:29, 28.47s/it]Inference:  42%|████▏     | 14/33 [06:32<09:05, 28.70s/it]Inference:  45%|████▌     | 15/33 [06:58<08:22, 27.92s/it]Inference:  48%|████▊     | 16/33 [07:34<08:36, 30.36s/it]Inference:  52%|█████▏    | 17/33 [08:02<07:55, 29.74s/it]Inference:  55%|█████▍    | 18/33 [08:31<07:22, 29.53s/it]Inference:  58%|█████▊    | 19/33 [08:52<06:15, 26.85s/it]Inference:  61%|██████    | 20/33 [09:22<06:01, 27.78s/it]Inference:  64%|██████▎   | 21/33 [10:02<06:18, 31.52s/it]Inference:  67%|██████▋   | 22/33 [10:28<05:27, 29.76s/it]Inference:  70%|██████▉   | 23/33 [11:06<05:23, 32.38s/it]Inference:  73%|███████▎  | 24/33 [11:27<04:20, 28.98s/it]Inference:  76%|███████▌  | 25/33 [11:56<03:51, 28.90s/it]Inference:  79%|███████▉  | 26/33 [12:12<02:56, 25.14s/it]Inference:  82%|████████▏ | 27/33 [12:47<02:48, 28.14s/it]Inference:  85%|████████▍ | 28/33 [13:30<02:42, 32.55s/it]Inference:  88%|████████▊ | 29/33 [13:53<01:58, 29.50s/it]Inference:  91%|█████████ | 30/33 [14:34<01:39, 33.11s/it]Inference:  94%|█████████▍| 31/33 [14:49<00:55, 27.70s/it]Inference:  97%|█████████▋| 32/33 [15:19<00:28, 28.41s/it]Inference: 100%|██████████| 33/33 [15:41<00:00, 26.51s/it]Inference: 100%|██████████| 33/33 [15:41<00:00, 28.54s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/43 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:15<10:52, 15.54s/it]Inference:   5%|▍         | 2/43 [00:28<09:30, 13.91s/it]Inference:   7%|▋         | 3/43 [00:55<13:25, 20.13s/it]Inference:   9%|▉         | 4/43 [01:15<12:53, 19.84s/it]Inference:  12%|█▏        | 5/43 [01:55<17:20, 27.38s/it]Inference:  14%|█▍        | 6/43 [02:14<14:55, 24.20s/it]Inference:  16%|█▋        | 7/43 [02:28<12:31, 20.88s/it]Inference:  19%|█▊        | 8/43 [02:51<12:38, 21.68s/it]Inference:  21%|██        | 9/43 [03:04<10:43, 18.94s/it]Inference:  23%|██▎       | 10/43 [03:31<11:50, 21.54s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/43 [03:58<12:16, 23.01s/it]Inference:  28%|██▊       | 12/43 [04:14<10:49, 20.96s/it]Inference:  30%|███       | 13/43 [04:53<13:09, 26.33s/it]Inference:  33%|███▎      | 14/43 [05:15<12:12, 25.27s/it]Inference:  35%|███▍      | 15/43 [05:29<10:12, 21.88s/it]Inference:  37%|███▋      | 16/43 [05:53<10:08, 22.54s/it]Inference:  40%|███▉      | 17/43 [06:20<10:18, 23.77s/it]Inference:  42%|████▏     | 18/43 [06:35<08:47, 21.10s/it]Inference:  44%|████▍     | 19/43 [06:59<08:45, 21.88s/it]Inference:  47%|████▋     | 20/43 [07:20<08:19, 21.71s/it]Inference:  49%|████▉     | 21/43 [07:36<07:20, 20.03s/it]Inference:  51%|█████     | 22/43 [07:54<06:49, 19.48s/it]Inference:  53%|█████▎    | 23/43 [08:31<08:11, 24.60s/it]Inference:  56%|█████▌    | 24/43 [08:46<06:53, 21.78s/it]Inference:  58%|█████▊    | 25/43 [09:07<06:26, 21.48s/it]Inference:  60%|██████    | 26/43 [09:28<06:01, 21.26s/it]Inference:  63%|██████▎   | 27/43 [09:50<05:45, 21.61s/it]Inference:  65%|██████▌   | 28/43 [10:16<05:44, 22.98s/it]Inference:  67%|██████▋   | 29/43 [10:35<05:05, 21.79s/it]Inference:  70%|██████▉   | 30/43 [10:52<04:22, 20.16s/it]Inference:  72%|███████▏  | 31/43 [11:12<04:03, 20.26s/it]Inference:  74%|███████▍  | 32/43 [11:27<03:25, 18.65s/it]Inference:  77%|███████▋  | 33/43 [12:00<03:51, 23.11s/it]Inference:  79%|███████▉  | 34/43 [12:16<03:08, 20.90s/it]Inference:  81%|████████▏ | 35/43 [13:01<03:45, 28.22s/it]Inference:  84%|████████▎ | 36/43 [13:18<02:53, 24.75s/it]Inference:  86%|████████▌ | 37/43 [13:34<02:12, 22.15s/it]Inference:  88%|████████▊ | 38/43 [13:49<01:40, 20.09s/it]Inference:  91%|█████████ | 39/43 [14:16<01:27, 21.91s/it]Inference:  93%|█████████▎| 40/43 [14:32<01:00, 20.19s/it]Inference:  95%|█████████▌| 41/43 [15:13<00:53, 26.63s/it]Inference:  98%|█████████▊| 42/43 [15:31<00:23, 23.84s/it]Inference: 100%|██████████| 43/43 [15:50<00:00, 22.42s/it]Inference: 100%|██████████| 43/43 [15:50<00:00, 22.10s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/41 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:17<11:40, 17.51s/it]Inference:   5%|▍         | 2/41 [00:33<10:56, 16.83s/it]Inference:   7%|▋         | 3/41 [00:47<09:39, 15.25s/it]Inference:  10%|▉         | 4/41 [01:03<09:38, 15.64s/it]Inference:  12%|█▏        | 5/41 [01:22<10:06, 16.84s/it]Inference:  15%|█▍        | 6/41 [01:37<09:26, 16.18s/it]Inference:  17%|█▋        | 7/41 [01:57<09:50, 17.37s/it]Inference:  20%|█▉        | 8/41 [02:17<10:08, 18.44s/it]Inference:  22%|██▏       | 9/41 [02:37<10:04, 18.89s/it]Inference:  24%|██▍       | 10/41 [02:56<09:45, 18.87s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  27%|██▋       | 11/41 [03:22<10:30, 21.01s/it]Inference:  29%|██▉       | 12/41 [03:45<10:22, 21.48s/it]Inference:  32%|███▏      | 13/41 [04:05<09:56, 21.29s/it]Inference:  34%|███▍      | 14/41 [04:26<09:28, 21.04s/it]Inference:  37%|███▋      | 15/41 [04:44<08:48, 20.32s/it]Inference:  39%|███▉      | 16/41 [05:05<08:32, 20.51s/it]Inference:  41%|████▏     | 17/41 [05:26<08:15, 20.66s/it]Inference:  44%|████▍     | 18/41 [05:49<08:05, 21.11s/it]Inference:  46%|████▋     | 19/41 [06:06<07:23, 20.14s/it]Inference:  49%|████▉     | 20/41 [06:22<06:34, 18.77s/it]Inference:  51%|█████     | 21/41 [07:08<09:01, 27.06s/it]Inference:  54%|█████▎    | 22/41 [07:27<07:44, 24.45s/it]Inference:  56%|█████▌    | 23/41 [07:45<06:48, 22.68s/it]Inference:  59%|█████▊    | 24/41 [08:21<07:33, 26.65s/it]Inference:  61%|██████    | 25/41 [08:58<07:56, 29.79s/it]Inference:  63%|██████▎   | 26/41 [09:15<06:25, 25.70s/it]Inference:  66%|██████▌   | 27/41 [09:36<05:42, 24.45s/it]Inference:  68%|██████▊   | 28/41 [10:12<06:02, 27.86s/it]Inference:  71%|███████   | 29/41 [10:32<05:08, 25.68s/it]Inference:  73%|███████▎  | 30/41 [10:56<04:36, 25.10s/it]Inference:  76%|███████▌  | 31/41 [11:14<03:50, 23.04s/it]Inference:  78%|███████▊  | 32/41 [11:32<03:13, 21.51s/it]Inference:  80%|████████  | 33/41 [11:44<02:28, 18.61s/it]Inference:  83%|████████▎ | 34/41 [12:08<02:20, 20.04s/it]Inference:  85%|████████▌ | 35/41 [12:25<01:55, 19.20s/it]Inference:  88%|████████▊ | 36/41 [12:42<01:32, 18.60s/it]Inference:  90%|█████████ | 37/41 [13:03<01:17, 19.34s/it]Inference:  93%|█████████▎| 38/41 [13:24<00:59, 19.85s/it]Inference:  95%|█████████▌| 39/41 [13:40<00:37, 18.57s/it]Inference:  98%|█████████▊| 40/41 [13:58<00:18, 18.41s/it]Inference: 100%|██████████| 41/41 [14:16<00:00, 18.47s/it]Inference: 100%|██████████| 41/41 [14:16<00:00, 20.90s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/37 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:34<20:39, 34.42s/it]Inference:   5%|▌         | 2/37 [00:49<13:25, 23.03s/it]Inference:   8%|▊         | 3/37 [01:08<12:00, 21.19s/it]Inference:  11%|█         | 4/37 [01:30<11:46, 21.41s/it]Inference:  14%|█▎        | 5/37 [01:51<11:20, 21.28s/it]Inference:  16%|█▌        | 6/37 [02:10<10:39, 20.62s/it]Inference:  19%|█▉        | 7/37 [02:23<09:04, 18.15s/it]Inference:  22%|██▏       | 8/37 [02:41<08:45, 18.11s/it]Inference:  24%|██▍       | 9/37 [03:08<09:39, 20.69s/it]Inference:  27%|██▋       | 10/37 [03:26<09:01, 20.04s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  30%|██▉       | 11/37 [03:42<08:11, 18.88s/it]Inference:  32%|███▏      | 12/37 [03:55<07:00, 16.84s/it]Inference:  35%|███▌      | 13/37 [04:08<06:16, 15.68s/it]Inference:  38%|███▊      | 14/37 [04:29<06:43, 17.55s/it]Inference:  41%|████      | 15/37 [04:53<07:07, 19.42s/it]Inference:  43%|████▎     | 16/37 [05:10<06:29, 18.55s/it]Inference:  46%|████▌     | 17/37 [05:37<07:04, 21.21s/it]Inference:  49%|████▊     | 18/37 [06:10<07:50, 24.76s/it]Inference:  51%|█████▏    | 19/37 [06:28<06:50, 22.80s/it]Inference:  54%|█████▍    | 20/37 [07:08<07:50, 27.70s/it]Inference:  57%|█████▋    | 21/37 [07:29<06:52, 25.78s/it]Inference:  59%|█████▉    | 22/37 [07:46<05:49, 23.30s/it]Inference:  62%|██████▏   | 23/37 [08:04<05:03, 21.68s/it]Inference:  65%|██████▍   | 24/37 [08:21<04:23, 20.29s/it]Inference:  68%|██████▊   | 25/37 [08:37<03:45, 18.80s/it]Inference:  70%|███████   | 26/37 [08:55<03:26, 18.79s/it]Inference:  73%|███████▎  | 27/37 [09:15<03:10, 19.06s/it]Inference:  76%|███████▌  | 28/37 [09:34<02:52, 19.11s/it]Inference:  78%|███████▊  | 29/37 [09:47<02:18, 17.31s/it]Inference:  81%|████████  | 30/37 [10:04<01:59, 17.06s/it]Inference:  84%|████████▍ | 31/37 [10:23<01:45, 17.59s/it]Inference:  86%|████████▋ | 32/37 [10:34<01:18, 15.64s/it]Inference:  89%|████████▉ | 33/37 [11:12<01:29, 22.43s/it]Inference:  92%|█████████▏| 34/37 [11:38<01:10, 23.42s/it]Inference:  95%|█████████▍| 35/37 [11:58<00:44, 22.36s/it]Inference:  97%|█████████▋| 36/37 [12:17<00:21, 21.48s/it]Inference: 100%|██████████| 37/37 [12:34<00:00, 20.21s/it]Inference: 100%|██████████| 37/37 [12:34<00:00, 20.40s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/47 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:27<21:06, 27.52s/it]Inference:   4%|▍         | 2/47 [00:46<16:49, 22.42s/it]Inference:   6%|▋         | 3/47 [01:18<19:51, 27.08s/it]Inference:   9%|▊         | 4/47 [01:43<18:34, 25.92s/it]Inference:  11%|█         | 5/47 [01:57<15:16, 21.81s/it]Inference:  13%|█▎        | 6/47 [02:13<13:24, 19.62s/it]Inference:  15%|█▍        | 7/47 [02:40<14:44, 22.11s/it]Inference:  17%|█▋        | 8/47 [03:03<14:31, 22.35s/it]Inference:  19%|█▉        | 9/47 [03:49<18:56, 29.92s/it]Inference:  21%|██▏       | 10/47 [04:09<16:28, 26.73s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  23%|██▎       | 11/47 [04:27<14:26, 24.07s/it]Inference:  26%|██▌       | 12/47 [04:44<12:50, 22.02s/it]Inference:  28%|██▊       | 13/47 [04:59<11:10, 19.72s/it]Inference:  30%|██▉       | 14/47 [05:22<11:28, 20.87s/it]Inference:  32%|███▏      | 15/47 [05:43<11:04, 20.76s/it]Inference:  34%|███▍      | 16/47 [06:30<14:51, 28.76s/it]Inference:  36%|███▌      | 17/47 [06:53<13:31, 27.06s/it]Inference:  38%|███▊      | 18/47 [07:20<13:04, 27.06s/it]Inference:  40%|████      | 19/47 [07:44<12:07, 25.97s/it]Inference:  43%|████▎     | 20/47 [08:07<11:22, 25.27s/it]Inference:  45%|████▍     | 21/47 [08:30<10:34, 24.41s/it]Inference:  47%|████▋     | 22/47 [08:56<10:23, 24.95s/it]Inference:  49%|████▉     | 23/47 [09:35<11:43, 29.33s/it]Inference:  51%|█████     | 24/47 [09:54<10:03, 26.25s/it]Inference:  53%|█████▎    | 25/47 [10:13<08:47, 23.96s/it]Inference:  55%|█████▌    | 26/47 [10:30<07:40, 21.91s/it]Inference:  57%|█████▋    | 27/47 [10:51<07:10, 21.54s/it]Inference:  60%|█████▉    | 28/47 [11:07<06:21, 20.07s/it]Inference:  62%|██████▏   | 29/47 [11:30<06:14, 20.83s/it]Inference:  64%|██████▍   | 30/47 [11:48<05:41, 20.08s/it]Inference:  66%|██████▌   | 31/47 [12:32<07:11, 26.99s/it]Inference:  68%|██████▊   | 32/47 [12:49<06:03, 24.22s/it]Inference:  70%|███████   | 33/47 [13:06<05:08, 22.01s/it]Inference:  72%|███████▏  | 34/47 [13:23<04:26, 20.47s/it]Inference:  74%|███████▍  | 35/47 [13:45<04:10, 20.91s/it]Inference:  77%|███████▋  | 36/47 [13:59<03:28, 18.98s/it]Inference:  79%|███████▊  | 37/47 [14:16<03:01, 18.13s/it]Inference:  81%|████████  | 38/47 [14:37<02:51, 19.09s/it]Inference:  83%|████████▎ | 39/47 [14:58<02:38, 19.84s/it]Inference:  85%|████████▌ | 40/47 [15:24<02:31, 21.59s/it]Inference:  87%|████████▋ | 41/47 [15:52<02:19, 23.32s/it]Inference:  89%|████████▉ | 42/47 [16:10<01:49, 21.98s/it]Inference:  91%|█████████▏| 43/47 [16:28<01:22, 20.67s/it]Inference:  94%|█████████▎| 44/47 [16:45<00:58, 19.55s/it]Inference:  96%|█████████▌| 45/47 [17:01<00:37, 18.62s/it]Inference:  98%|█████████▊| 46/47 [17:19<00:18, 18.24s/it]Inference: 100%|██████████| 47/47 [17:40<00:00, 19.01s/it]Inference: 100%|██████████| 47/47 [17:40<00:00, 22.55s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/54 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:22<19:29, 22.07s/it]Inference:   4%|▎         | 2/54 [00:42<18:16, 21.09s/it]Inference:   6%|▌         | 3/54 [01:25<26:16, 30.91s/it]Inference:   7%|▋         | 4/54 [01:51<24:11, 29.04s/it]Inference:   9%|▉         | 5/54 [02:11<20:59, 25.70s/it]Inference:  11%|█         | 6/54 [02:36<20:37, 25.78s/it]Inference:  13%|█▎        | 7/54 [03:12<22:46, 29.06s/it]Inference:  15%|█▍        | 8/54 [03:30<19:37, 25.60s/it]Inference:  17%|█▋        | 9/54 [04:05<21:20, 28.46s/it]Inference:  19%|█▊        | 10/54 [04:17<17:10, 23.42s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  20%|██        | 11/54 [04:46<17:52, 24.94s/it]Inference:  22%|██▏       | 12/54 [05:10<17:24, 24.86s/it]Inference:  24%|██▍       | 13/54 [05:40<17:56, 26.26s/it]Inference:  26%|██▌       | 14/54 [06:12<18:35, 27.88s/it]Inference:  28%|██▊       | 15/54 [06:36<17:30, 26.94s/it]Inference:  30%|██▉       | 16/54 [06:54<15:15, 24.10s/it]Inference:  31%|███▏      | 17/54 [07:18<14:57, 24.27s/it]Inference:  33%|███▎      | 18/54 [07:38<13:47, 22.98s/it]Inference:  35%|███▌      | 19/54 [07:56<12:27, 21.37s/it]Inference:  37%|███▋      | 20/54 [08:21<12:47, 22.57s/it]Inference:  39%|███▉      | 21/54 [08:57<14:30, 26.38s/it]Inference:  41%|████      | 22/54 [09:14<12:37, 23.67s/it]Inference:  43%|████▎     | 23/54 [09:45<13:19, 25.78s/it]Inference:  44%|████▍     | 24/54 [10:02<11:35, 23.18s/it]Inference:  46%|████▋     | 25/54 [10:16<09:57, 20.61s/it]Inference:  48%|████▊     | 26/54 [10:39<09:54, 21.22s/it]Inference:  50%|█████     | 27/54 [11:05<10:09, 22.56s/it]Inference:  52%|█████▏    | 28/54 [11:27<09:40, 22.32s/it]Inference:  54%|█████▎    | 29/54 [11:56<10:10, 24.43s/it]Inference:  56%|█████▌    | 30/54 [12:22<09:57, 24.90s/it]Inference:  57%|█████▋    | 31/54 [12:48<09:42, 25.34s/it]Inference:  59%|█████▉    | 32/54 [13:08<08:43, 23.80s/it]Inference:  61%|██████    | 33/54 [13:34<08:33, 24.45s/it]Inference:  63%|██████▎   | 34/54 [13:56<07:51, 23.56s/it]Inference:  65%|██████▍   | 35/54 [14:42<09:33, 30.19s/it]Inference:  67%|██████▋   | 36/54 [15:01<08:07, 27.06s/it]Inference:  69%|██████▊   | 37/54 [15:19<06:50, 24.14s/it]Inference:  70%|███████   | 38/54 [15:38<06:04, 22.77s/it]Inference:  72%|███████▏  | 39/54 [15:59<05:33, 22.20s/it]Inference:  74%|███████▍  | 40/54 [16:23<05:17, 22.64s/it]Inference:  76%|███████▌  | 41/54 [16:49<05:07, 23.62s/it]Inference:  78%|███████▊  | 42/54 [17:14<04:49, 24.10s/it]Inference:  80%|███████▉  | 43/54 [17:28<03:51, 21.08s/it]Inference:  81%|████████▏ | 44/54 [17:45<03:17, 19.77s/it]Inference:  83%|████████▎ | 45/54 [17:58<02:39, 17.77s/it]Inference:  85%|████████▌ | 46/54 [18:21<02:36, 19.56s/it]Inference:  87%|████████▋ | 47/54 [19:01<02:57, 25.43s/it]Inference:  89%|████████▉ | 48/54 [19:17<02:16, 22.72s/it]Inference:  91%|█████████ | 49/54 [19:25<01:31, 18.39s/it]Inference:  93%|█████████▎| 50/54 [19:59<01:32, 23.11s/it]Inference:  94%|█████████▍| 51/54 [20:25<01:11, 23.82s/it]Inference:  96%|█████████▋| 52/54 [20:48<00:47, 23.60s/it]Inference:  98%|█████████▊| 53/54 [21:24<00:27, 27.23s/it]Inference: 100%|██████████| 54/54 [21:39<00:00, 23.55s/it]Inference: 100%|██████████| 54/54 [21:39<00:00, 24.06s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/33 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:26<13:54, 26.08s/it]Inference:   6%|▌         | 2/33 [00:48<12:24, 24.01s/it]Inference:   9%|▉         | 3/33 [01:03<09:48, 19.63s/it]Inference:  12%|█▏        | 4/33 [01:17<08:28, 17.52s/it]Inference:  15%|█▌        | 5/33 [02:01<12:39, 27.13s/it]Inference:  18%|█▊        | 6/33 [02:34<13:04, 29.06s/it]Inference:  21%|██        | 7/33 [02:55<11:25, 26.35s/it]Inference:  24%|██▍       | 8/33 [03:12<09:50, 23.60s/it]Inference:  27%|██▋       | 9/33 [03:25<08:00, 20.04s/it]Inference:  30%|███       | 10/33 [03:54<08:45, 22.87s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  33%|███▎      | 11/33 [04:20<08:49, 24.05s/it]Inference:  36%|███▋      | 12/33 [04:35<07:24, 21.15s/it]Inference:  39%|███▉      | 13/33 [04:51<06:31, 19.58s/it]Inference:  42%|████▏     | 14/33 [05:04<05:34, 17.60s/it]Inference:  45%|████▌     | 15/33 [05:26<05:41, 18.97s/it]Inference:  48%|████▊     | 16/33 [05:57<06:21, 22.45s/it]Inference:  52%|█████▏    | 17/33 [06:20<06:02, 22.65s/it]Inference:  55%|█████▍    | 18/33 [06:39<05:26, 21.75s/it]Inference:  58%|█████▊    | 19/33 [06:56<04:43, 20.24s/it]Inference:  61%|██████    | 20/33 [07:17<04:26, 20.53s/it]Inference:  64%|██████▎   | 21/33 [08:04<05:40, 28.39s/it]Inference:  67%|██████▋   | 22/33 [08:17<04:22, 23.87s/it]Inference:  70%|██████▉   | 23/33 [08:42<04:02, 24.21s/it]Inference:  73%|███████▎  | 24/33 [09:09<03:44, 24.96s/it]Inference:  76%|███████▌  | 25/33 [09:49<03:56, 29.54s/it]Inference:  79%|███████▉  | 26/33 [10:09<03:06, 26.64s/it]Inference:  82%|████████▏ | 27/33 [10:46<02:57, 29.61s/it]Inference:  85%|████████▍ | 28/33 [11:11<02:21, 28.29s/it]Inference:  88%|████████▊ | 29/33 [11:32<01:44, 26.22s/it]Inference:  91%|█████████ | 30/33 [11:53<01:13, 24.41s/it]Inference:  94%|█████████▍| 31/33 [12:10<00:44, 22.46s/it]Inference:  97%|█████████▋| 32/33 [12:16<00:17, 17.42s/it]Inference: 100%|██████████| 33/33 [12:30<00:00, 16.47s/it]Inference: 100%|██████████| 33/33 [12:30<00:00, 22.75s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:17<09:53, 17.99s/it]Inference:   6%|▌         | 2/34 [00:37<10:08, 19.01s/it]Inference:   9%|▉         | 3/34 [00:56<09:46, 18.90s/it]Inference:  12%|█▏        | 4/34 [01:22<10:46, 21.56s/it]Inference:  15%|█▍        | 5/34 [01:39<09:37, 19.90s/it]Inference:  18%|█▊        | 6/34 [01:59<09:26, 20.22s/it]Inference:  21%|██        | 7/34 [02:14<08:14, 18.30s/it]Inference:  24%|██▎       | 8/34 [02:41<09:08, 21.11s/it]Inference:  26%|██▋       | 9/34 [03:08<09:31, 22.86s/it]Inference:  29%|██▉       | 10/34 [03:34<09:35, 23.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [03:49<08:07, 21.22s/it]Inference:  35%|███▌      | 12/34 [04:27<09:41, 26.45s/it]Inference:  38%|███▊      | 13/34 [04:42<07:57, 22.72s/it]Inference:  41%|████      | 14/34 [05:05<07:39, 22.97s/it]Inference:  44%|████▍     | 15/34 [05:33<07:43, 24.39s/it]Inference:  47%|████▋     | 16/34 [06:00<07:35, 25.29s/it]Inference:  50%|█████     | 17/34 [06:17<06:24, 22.59s/it]Inference:  53%|█████▎    | 18/34 [06:47<06:40, 25.03s/it]Inference:  56%|█████▌    | 19/34 [07:16<06:33, 26.21s/it]Inference:  59%|█████▉    | 20/34 [07:31<05:19, 22.83s/it]Inference:  62%|██████▏   | 21/34 [07:59<05:17, 24.46s/it]Inference:  65%|██████▍   | 22/34 [08:21<04:42, 23.56s/it]Inference:  68%|██████▊   | 23/34 [09:02<05:18, 28.96s/it]Inference:  71%|███████   | 24/34 [09:34<04:57, 29.72s/it]Inference:  74%|███████▎  | 25/34 [09:57<04:10, 27.83s/it]Inference:  76%|███████▋  | 26/34 [10:16<03:21, 25.14s/it]Inference:  79%|███████▉  | 27/34 [10:37<02:46, 23.84s/it]Inference:  82%|████████▏ | 28/34 [11:02<02:24, 24.07s/it]Inference:  85%|████████▌ | 29/34 [11:18<01:48, 21.63s/it]Inference:  88%|████████▊ | 30/34 [11:42<01:29, 22.36s/it]Inference:  91%|█████████ | 31/34 [12:09<01:11, 23.76s/it]Inference:  94%|█████████▍| 32/34 [12:53<00:59, 29.90s/it]Inference:  97%|█████████▋| 33/34 [13:10<00:26, 26.14s/it]Inference: 100%|██████████| 34/34 [13:29<00:00, 23.94s/it]Inference: 100%|██████████| 34/34 [13:29<00:00, 23.81s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:42<23:09, 42.09s/it]Inference:   6%|▌         | 2/34 [01:08<17:24, 32.63s/it]Inference:   9%|▉         | 3/34 [01:22<12:27, 24.11s/it]Inference:  12%|█▏        | 4/34 [01:39<10:47, 21.57s/it]Inference:  15%|█▍        | 5/34 [01:53<09:08, 18.92s/it]Inference:  18%|█▊        | 6/34 [02:39<13:07, 28.12s/it]Inference:  21%|██        | 7/34 [02:55<10:46, 23.93s/it]Inference:  24%|██▎       | 8/34 [03:13<09:34, 22.11s/it]Inference:  26%|██▋       | 9/34 [03:25<07:55, 19.01s/it]Inference:  29%|██▉       | 10/34 [03:41<07:13, 18.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [04:01<07:09, 18.69s/it]Inference:  35%|███▌      | 12/34 [04:27<07:38, 20.82s/it]Inference:  38%|███▊      | 13/34 [04:44<06:50, 19.54s/it]Inference:  41%|████      | 14/34 [05:10<07:13, 21.66s/it]Inference:  44%|████▍     | 15/34 [05:24<06:07, 19.35s/it]Inference:  47%|████▋     | 16/34 [05:52<06:33, 21.87s/it]Inference:  50%|█████     | 17/34 [06:15<06:19, 22.35s/it]Inference:  53%|█████▎    | 18/34 [06:37<05:56, 22.28s/it]Inference:  56%|█████▌    | 19/34 [06:55<05:13, 20.88s/it]Inference:  59%|█████▉    | 20/34 [07:19<05:04, 21.76s/it]Inference:  62%|██████▏   | 21/34 [07:45<04:58, 23.00s/it]Inference:  65%|██████▍   | 22/34 [08:09<04:40, 23.38s/it]Inference:  68%|██████▊   | 23/34 [08:34<04:23, 23.99s/it]Inference:  71%|███████   | 24/34 [09:30<05:35, 33.58s/it]Inference:  74%|███████▎  | 25/34 [09:54<04:36, 30.70s/it]Inference:  76%|███████▋  | 26/34 [10:26<04:06, 30.87s/it]Inference:  79%|███████▉  | 27/34 [10:47<03:17, 28.15s/it]Inference:  82%|████████▏ | 28/34 [11:06<02:30, 25.15s/it]Inference:  85%|████████▌ | 29/34 [11:21<01:50, 22.16s/it]Inference:  88%|████████▊ | 30/34 [11:33<01:17, 19.33s/it]Inference:  91%|█████████ | 31/34 [11:47<00:53, 17.71s/it]Inference:  94%|█████████▍| 32/34 [12:10<00:38, 19.18s/it]Inference:  97%|█████████▋| 33/34 [12:38<00:21, 21.73s/it]Inference: 100%|██████████| 34/34 [12:55<00:00, 20.34s/it]Inference: 100%|██████████| 34/34 [12:55<00:00, 22.80s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:20<14:07, 20.68s/it]Inference:   5%|▍         | 2/42 [00:38<12:43, 19.09s/it]Inference:   7%|▋         | 3/42 [00:53<11:16, 17.34s/it]Inference:  10%|▉         | 4/42 [01:14<11:45, 18.57s/it]Inference:  12%|█▏        | 5/42 [01:23<09:20, 15.14s/it]Inference:  14%|█▍        | 6/42 [01:37<08:48, 14.69s/it]Inference:  17%|█▋        | 7/42 [01:59<10:04, 17.26s/it]Inference:  19%|█▉        | 8/42 [02:24<11:07, 19.62s/it]Inference:  21%|██▏       | 9/42 [02:50<11:52, 21.59s/it]Inference:  24%|██▍       | 10/42 [03:11<11:28, 21.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [03:25<09:53, 19.14s/it]Inference:  29%|██▊       | 12/42 [03:40<08:56, 17.90s/it]Inference:  31%|███       | 13/42 [04:01<09:06, 18.84s/it]Inference:  33%|███▎      | 14/42 [04:33<10:36, 22.72s/it]Inference:  36%|███▌      | 15/42 [05:00<10:53, 24.21s/it]Inference:  38%|███▊      | 16/42 [05:17<09:26, 21.79s/it]Inference:  40%|████      | 17/42 [05:38<08:59, 21.57s/it]Inference:  43%|████▎     | 18/42 [05:57<08:20, 20.84s/it]Inference:  45%|████▌     | 19/42 [06:14<07:34, 19.74s/it]Inference:  48%|████▊     | 20/42 [06:37<07:38, 20.84s/it]Inference:  50%|█████     | 21/42 [06:49<06:22, 18.20s/it]Inference:  52%|█████▏    | 22/42 [07:12<06:28, 19.43s/it]Inference:  55%|█████▍    | 23/42 [07:38<06:48, 21.51s/it]Inference:  57%|█████▋    | 24/42 [07:59<06:24, 21.35s/it]Inference:  60%|█████▉    | 25/42 [08:12<05:22, 18.98s/it]Inference:  62%|██████▏   | 26/42 [08:35<05:20, 20.02s/it]Inference:  64%|██████▍   | 27/42 [08:56<05:03, 20.24s/it]Inference:  67%|██████▋   | 28/42 [09:28<05:33, 23.83s/it]Inference:  69%|██████▉   | 29/42 [09:43<04:37, 21.35s/it]Inference:  71%|███████▏  | 30/42 [10:13<04:45, 23.80s/it]Inference:  74%|███████▍  | 31/42 [10:37<04:21, 23.76s/it]Inference:  76%|███████▌  | 32/42 [10:54<03:39, 21.90s/it]Inference:  79%|███████▊  | 33/42 [11:15<03:14, 21.64s/it]Inference:  81%|████████  | 34/42 [11:28<02:31, 18.96s/it]Inference:  83%|████████▎ | 35/42 [11:49<02:16, 19.55s/it]Inference:  86%|████████▌ | 36/42 [12:29<02:33, 25.58s/it]Inference:  88%|████████▊ | 37/42 [12:43<01:51, 22.32s/it]Inference:  90%|█████████ | 38/42 [12:59<01:21, 20.46s/it]Inference:  93%|█████████▎| 39/42 [13:30<01:10, 23.39s/it]Inference:  95%|█████████▌| 40/42 [14:05<00:54, 27.02s/it]Inference:  98%|█████████▊| 41/42 [14:36<00:28, 28.26s/it]Inference: 100%|██████████| 42/42 [15:05<00:00, 28.45s/it]Inference: 100%|██████████| 42/42 [15:05<00:00, 21.56s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:10<07:03, 10.32s/it]Inference:   5%|▍         | 2/42 [00:29<10:31, 15.78s/it]Inference:   7%|▋         | 3/42 [00:59<14:20, 22.07s/it]Inference:  10%|▉         | 4/42 [01:25<14:55, 23.57s/it]Inference:  12%|█▏        | 5/42 [01:46<14:05, 22.85s/it]Inference:  14%|█▍        | 6/42 [02:00<11:45, 19.59s/it]Inference:  17%|█▋        | 7/42 [02:24<12:14, 20.98s/it]Inference:  19%|█▉        | 8/42 [02:36<10:21, 18.27s/it]Inference:  21%|██▏       | 9/42 [02:56<10:20, 18.81s/it]Inference:  24%|██▍       | 10/42 [03:09<08:59, 16.87s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [03:24<08:32, 16.53s/it]Inference:  29%|██▊       | 12/42 [03:49<09:29, 18.99s/it]Inference:  31%|███       | 13/42 [04:13<09:59, 20.69s/it]Inference:  33%|███▎      | 14/42 [04:26<08:26, 18.09s/it]Inference:  36%|███▌      | 15/42 [04:57<09:54, 22.02s/it]Inference:  38%|███▊      | 16/42 [05:27<10:41, 24.65s/it]Inference:  40%|████      | 17/42 [05:54<10:27, 25.12s/it]Inference:  43%|████▎     | 18/42 [06:16<09:39, 24.15s/it]Inference:  45%|████▌     | 19/42 [06:26<07:43, 20.13s/it]Inference:  48%|████▊     | 20/42 [06:42<06:54, 18.84s/it]Inference:  50%|█████     | 21/42 [06:54<05:52, 16.78s/it]Inference:  52%|█████▏    | 22/42 [07:09<05:23, 16.19s/it]Inference:  55%|█████▍    | 23/42 [07:31<05:39, 17.86s/it]Inference:  57%|█████▋    | 24/42 [07:52<05:38, 18.78s/it]Inference:  60%|█████▉    | 25/42 [08:14<05:36, 19.81s/it]Inference:  62%|██████▏   | 26/42 [08:41<05:49, 21.87s/it]Inference:  64%|██████▍   | 27/42 [09:28<07:23, 29.59s/it]Inference:  67%|██████▋   | 28/42 [09:45<05:59, 25.69s/it]Inference:  69%|██████▉   | 29/42 [09:58<04:44, 21.85s/it]Inference:  71%|███████▏  | 30/42 [10:10<03:49, 19.11s/it]Inference:  74%|███████▍  | 31/42 [10:30<03:32, 19.30s/it]Inference:  76%|███████▌  | 32/42 [10:53<03:22, 20.30s/it]Inference:  79%|███████▊  | 33/42 [11:09<02:51, 19.11s/it]Inference:  81%|████████  | 34/42 [11:32<02:43, 20.39s/it]Inference:  83%|████████▎ | 35/42 [11:51<02:18, 19.76s/it]Inference:  86%|████████▌ | 36/42 [12:11<01:58, 19.79s/it]Inference:  88%|████████▊ | 37/42 [12:32<01:42, 20.41s/it]Inference:  90%|█████████ | 38/42 [12:49<01:16, 19.15s/it]Inference:  93%|█████████▎| 39/42 [13:08<00:57, 19.16s/it]Inference:  95%|█████████▌| 40/42 [13:41<00:46, 23.25s/it]Inference:  98%|█████████▊| 41/42 [14:06<00:23, 23.89s/it]Inference: 100%|██████████| 42/42 [14:40<00:00, 26.78s/it]Inference: 100%|██████████| 42/42 [14:40<00:00, 20.95s/it]
/home/namwoam/dl-final/llm/inference-translated.py:139: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...             icl_type
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  translated_question

[1 rows x 8 columns]
  dataset_name                                      path  ...  4_tile  5_tile
0       113-ns  ../dataset/gsat/113_natural_sciences.csv  ...   31.41   25.13
1       112-ns  ../dataset/gsat/112_natural_sciences.csv  ...   29.58   23.66
2       111-ns  ../dataset/gsat/111_natural_sciences.csv  ...   37.74   25.16
3       110-ns  ../dataset/gsat/110_natural_sciences.csv  ...   32.31   25.85
4       109-ns  ../dataset/gsat/109_natural_sciences.csv  ...   32.63   26.10

[5 rows x 7 columns]
Running dataset:113-ns on model:breeze-7b-icl_en
Running dataset:112-ns on model:breeze-7b-icl_en
Running dataset:111-ns on model:breeze-7b-icl_en
Running dataset:110-ns on model:breeze-7b-icl_en
Running dataset:109-ns on model:breeze-7b-icl_en
Running dataset:113-ss on model:breeze-7b-icl_en
Running dataset:112-ss on model:breeze-7b-icl_en
Running dataset:111-ss on model:breeze-7b-icl_en
Running dataset:110-ss on model:breeze-7b-icl_en
Running dataset:109-ss on model:breeze-7b-icl_en
Running dataset:113-ch on model:breeze-7b-icl_en
Running dataset:112-ch on model:breeze-7b-icl_en
Running dataset:111-ch on model:breeze-7b-icl_en
Running dataset:110-ch on model:breeze-7b-icl_en
Running dataset:109-ch on model:breeze-7b-icl_en
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/namwoam/dl-final/llm/inference-reverse.py", line 132, in <module>
    main(model_id=args.model_id, dataset_paths=args.dataset_paths,
  File "/home/namwoam/dl-final/llm/inference-reverse.py", line 20, in main
    pipeline = transformers.pipeline(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/pipelines/__init__.py", line 906, in pipeline
    framework, model = infer_framework_load_model(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/pipelines/base.py", line 283, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3754, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4237, in _load_pretrained_model
    gc.collect()
KeyboardInterrupt
                                           model  ...        icl_type
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  prompt_reverse

[1 rows x 8 columns]
  dataset_name                                    path  ...  4_tile  5_tile
0       113-ss  ../dataset/gsat/113_social_studies.csv  ...   39.56   28.25
1       112-ss  ../dataset/gsat/112_social_studies.csv  ...   38.13   27.24
2       111-ss  ../dataset/gsat/111_social_studies.csv  ...   45.90   34.42
3       110-ss  ../dataset/gsat/110_social_studies.csv  ...   40.50   34.72
4       109-ss  ../dataset/gsat/109_social_studies.csv  ...   49.01   36.76

[5 rows x 7 columns]
Running dataset:113-ss on model:breeze-7b-icl_reverse
Traceback (most recent call last):
  File "/home/namwoam/dl-final/llm/./super-inference.py", line 31, in <module>
    subprocess.run(['python', 'inference-reverse.py', '-i', f'{os.path.join(os.path.dirname(__file__) ,dataset_row["path"] ) }',
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/43 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:19<13:33, 19.36s/it]Inference:   5%|▍         | 2/43 [00:37<12:38, 18.50s/it]Inference:   7%|▋         | 3/43 [00:49<10:30, 15.77s/it]Inference:   9%|▉         | 4/43 [01:22<14:37, 22.49s/it]Inference:  12%|█▏        | 5/43 [01:50<15:23, 24.29s/it]Inference:  14%|█▍        | 6/43 [02:05<13:11, 21.38s/it]Inference:  16%|█▋        | 7/43 [02:30<13:34, 22.62s/it]Inference:  19%|█▊        | 8/43 [03:04<15:12, 26.07s/it]Inference:  21%|██        | 9/43 [03:32<15:03, 26.56s/it]Inference:  23%|██▎       | 10/43 [04:18<17:54, 32.56s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/43 [04:56<18:19, 34.37s/it]Inference:  28%|██▊       | 12/43 [05:13<15:05, 29.21s/it]Inference:  30%|███       | 13/43 [05:29<12:34, 25.15s/it]Inference:  33%|███▎      | 14/43 [05:54<12:09, 25.16s/it]Inference:  35%|███▍      | 15/43 [06:21<11:54, 25.51s/it]Inference:  37%|███▋      | 16/43 [06:42<10:57, 24.37s/it]Inference:  40%|███▉      | 17/43 [07:06<10:25, 24.06s/it]Inference:  42%|████▏     | 18/43 [07:41<11:24, 27.36s/it]Inference:  44%|████▍     | 19/43 [08:04<10:22, 25.95s/it]Inference:  47%|████▋     | 20/43 [08:33<10:24, 27.15s/it]Inference:  49%|████▉     | 21/43 [09:04<10:17, 28.07s/it]Inference:  51%|█████     | 22/43 [09:25<09:09, 26.19s/it]Inference:  53%|█████▎    | 23/43 [09:47<08:14, 24.71s/it]Inference:  56%|█████▌    | 24/43 [10:17<08:20, 26.34s/it]Inference:  58%|█████▊    | 25/43 [10:37<07:20, 24.48s/it]Inference:  60%|██████    | 26/43 [11:01<06:52, 24.24s/it]Inference:  63%|██████▎   | 27/43 [11:15<05:38, 21.15s/it]Inference:  65%|██████▌   | 28/43 [11:51<06:25, 25.70s/it]Inference:  67%|██████▋   | 29/43 [12:00<04:50, 20.72s/it]Inference:  70%|██████▉   | 30/43 [12:55<06:43, 31.01s/it]Inference:  72%|███████▏  | 31/43 [13:11<05:18, 26.53s/it]Inference:  74%|███████▍  | 32/43 [13:48<05:25, 29.58s/it]Inference:  77%|███████▋  | 33/43 [14:04<04:14, 25.41s/it]Inference:  79%|███████▉  | 34/43 [14:16<03:12, 21.40s/it]Inference:  81%|████████▏ | 35/43 [14:30<02:34, 19.33s/it]Inference:  84%|████████▎ | 36/43 [15:06<02:50, 24.41s/it]Inference:  86%|████████▌ | 37/43 [15:35<02:34, 25.82s/it]Inference:  88%|████████▊ | 38/43 [15:57<02:02, 24.43s/it]Inference:  91%|█████████ | 39/43 [16:09<01:23, 20.94s/it]Inference:  93%|█████████▎| 40/43 [16:32<01:04, 21.52s/it]Inference:  95%|█████████▌| 41/43 [16:55<00:43, 21.89s/it]Inference:  98%|█████████▊| 42/43 [17:32<00:26, 26.55s/it]Inference: 100%|██████████| 43/43 [18:00<00:00, 26.88s/it]Inference: 100%|██████████| 43/43 [18:00<00:00, 25.13s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/41 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:20<13:36, 20.41s/it]Inference:   5%|▍         | 2/41 [00:50<17:03, 26.25s/it]Inference:   7%|▋         | 3/41 [01:22<18:17, 28.89s/it]Inference:  10%|▉         | 4/41 [02:02<20:24, 33.09s/it]Inference:  12%|█▏        | 5/41 [02:23<17:20, 28.90s/it]Inference:  15%|█▍        | 6/41 [02:50<16:22, 28.07s/it]Inference:  17%|█▋        | 7/41 [03:17<15:43, 27.76s/it]Inference:  20%|█▉        | 8/41 [03:32<13:02, 23.72s/it]Inference:  22%|██▏       | 9/41 [04:00<13:17, 24.93s/it]Inference:  24%|██▍       | 10/41 [04:28<13:30, 26.15s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  27%|██▋       | 11/41 [04:55<13:12, 26.42s/it]Inference:  29%|██▉       | 12/41 [05:24<13:02, 26.98s/it]Inference:  32%|███▏      | 13/41 [05:57<13:30, 28.96s/it]Inference:  34%|███▍      | 14/41 [06:27<13:09, 29.24s/it]Inference:  37%|███▋      | 15/41 [06:46<11:18, 26.11s/it]Inference:  39%|███▉      | 16/41 [07:20<11:48, 28.36s/it]Inference:  41%|████▏     | 17/41 [07:47<11:12, 28.02s/it]Inference:  44%|████▍     | 18/41 [08:17<11:03, 28.83s/it]Inference:  46%|████▋     | 19/41 [08:49<10:54, 29.76s/it]Inference:  49%|████▉     | 20/41 [09:10<09:24, 26.90s/it]Inference:  51%|█████     | 21/41 [09:24<07:45, 23.26s/it]Inference:  54%|█████▎    | 22/41 [09:46<07:12, 22.79s/it]Inference:  56%|█████▌    | 23/41 [10:05<06:27, 21.51s/it]Inference:  59%|█████▊    | 24/41 [10:29<06:21, 22.47s/it]Inference:  61%|██████    | 25/41 [10:52<06:00, 22.54s/it]Inference:  63%|██████▎   | 26/41 [11:20<06:02, 24.14s/it]Inference:  66%|██████▌   | 27/41 [12:45<09:54, 42.49s/it]Inference:  68%|██████▊   | 28/41 [13:21<08:46, 40.51s/it]Inference:  71%|███████   | 29/41 [13:47<07:12, 36.06s/it]Inference:  73%|███████▎  | 30/41 [14:12<05:59, 32.66s/it]Inference:  76%|███████▌  | 31/41 [14:38<05:08, 30.90s/it]Inference:  78%|███████▊  | 32/41 [15:04<04:23, 29.31s/it]Inference:  80%|████████  | 33/41 [15:12<03:03, 22.97s/it]Inference:  83%|████████▎ | 34/41 [15:31<02:31, 21.71s/it]Inference:  85%|████████▌ | 35/41 [15:58<02:20, 23.35s/it]Inference:  88%|████████▊ | 36/41 [16:32<02:12, 26.46s/it]Inference:  90%|█████████ | 37/41 [16:57<01:44, 26.21s/it]Inference:  93%|█████████▎| 38/41 [17:19<01:14, 24.77s/it]Inference:  95%|█████████▌| 39/41 [17:40<00:47, 23.61s/it]Inference:  98%|█████████▊| 40/41 [18:07<00:24, 24.71s/it]Inference: 100%|██████████| 41/41 [19:33<00:00, 43.12s/it]Inference: 100%|██████████| 41/41 [19:33<00:00, 28.62s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/37 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:15<09:15, 15.42s/it]Inference:   5%|▌         | 2/37 [00:40<12:13, 20.97s/it]Inference:   8%|▊         | 3/37 [01:09<14:03, 24.80s/it]Inference:  11%|█         | 4/37 [01:37<14:14, 25.89s/it]Inference:  14%|█▎        | 5/37 [02:05<14:11, 26.61s/it]Inference:  16%|█▌        | 6/37 [02:27<12:57, 25.09s/it]Inference:  19%|█▉        | 7/37 [02:50<12:12, 24.40s/it]Inference:  22%|██▏       | 8/37 [03:25<13:28, 27.88s/it]Inference:  24%|██▍       | 9/37 [03:48<12:21, 26.47s/it]Inference:  27%|██▋       | 10/37 [04:10<11:13, 24.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  30%|██▉       | 11/37 [04:51<12:58, 29.94s/it]Inference:  32%|███▏      | 12/37 [05:10<11:05, 26.62s/it]Inference:  35%|███▌      | 13/37 [05:42<11:17, 28.22s/it]Inference:  38%|███▊      | 14/37 [06:12<11:00, 28.71s/it]Inference:  41%|████      | 15/37 [06:40<10:29, 28.60s/it]Inference:  43%|████▎     | 16/37 [07:02<09:19, 26.62s/it]Inference:  46%|████▌     | 17/37 [07:34<09:25, 28.29s/it]Inference:  49%|████▊     | 18/37 [08:06<09:14, 29.19s/it]Inference:  51%|█████▏    | 19/37 [08:38<09:04, 30.24s/it]Inference:  54%|█████▍    | 20/37 [09:09<08:36, 30.41s/it]Inference:  57%|█████▋    | 21/37 [09:37<07:53, 29.60s/it]Inference:  59%|█████▉    | 22/37 [10:05<07:15, 29.03s/it]Inference:  62%|██████▏   | 23/37 [10:37<07:02, 30.14s/it]Inference:  65%|██████▍   | 24/37 [11:02<06:11, 28.55s/it]Inference:  68%|██████▊   | 25/37 [11:35<05:58, 29.88s/it]Inference:  70%|███████   | 26/37 [12:02<05:19, 29.08s/it]Inference:  73%|███████▎  | 27/37 [12:18<04:09, 24.91s/it]Inference:  76%|███████▌  | 28/37 [12:40<03:38, 24.29s/it]Inference:  78%|███████▊  | 29/37 [13:09<03:25, 25.69s/it]Inference:  81%|████████  | 30/37 [13:37<03:03, 26.22s/it]Inference:  84%|████████▍ | 31/37 [14:09<02:47, 27.85s/it]Inference:  86%|████████▋ | 32/37 [14:45<02:31, 30.40s/it]Inference:  89%|████████▉ | 33/37 [15:15<02:00, 30.22s/it]Inference:  92%|█████████▏| 34/37 [15:22<01:09, 23.24s/it]Inference:  95%|█████████▍| 35/37 [15:51<00:50, 25.04s/it]Inference:  97%|█████████▋| 36/37 [16:14<00:24, 24.43s/it]Inference: 100%|██████████| 37/37 [16:38<00:00, 24.26s/it]Inference: 100%|██████████| 37/37 [16:38<00:00, 26.98s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/47 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:23<17:43, 23.12s/it]Inference:   4%|▍         | 2/47 [00:35<12:24, 16.55s/it]Inference:   6%|▋         | 3/47 [00:58<14:26, 19.70s/it]Inference:   9%|▊         | 4/47 [01:24<15:56, 22.25s/it]Inference:  11%|█         | 5/47 [01:46<15:30, 22.16s/it]Inference:  13%|█▎        | 6/47 [02:15<16:44, 24.50s/it]Inference:  15%|█▍        | 7/47 [02:47<17:56, 26.91s/it]Inference:  17%|█▋        | 8/47 [03:26<19:54, 30.63s/it]Inference:  19%|█▉        | 9/47 [03:37<15:29, 24.46s/it]Inference:  21%|██▏       | 10/47 [03:46<12:12, 19.80s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  23%|██▎       | 11/47 [04:24<15:18, 25.51s/it]Inference:  26%|██▌       | 12/47 [04:43<13:43, 23.53s/it]Inference:  28%|██▊       | 13/47 [05:00<12:10, 21.48s/it]Inference:  30%|██▉       | 14/47 [05:29<12:57, 23.56s/it]Inference:  32%|███▏      | 15/47 [05:54<12:50, 24.07s/it]Inference:  34%|███▍      | 16/47 [06:26<13:39, 26.42s/it]Inference:  36%|███▌      | 17/47 [06:44<11:57, 23.90s/it]Inference:  38%|███▊      | 18/47 [07:01<10:36, 21.96s/it]Inference:  40%|████      | 19/47 [07:28<10:52, 23.30s/it]Inference:  43%|████▎     | 20/47 [08:04<12:17, 27.32s/it]Inference:  45%|████▍     | 21/47 [08:31<11:48, 27.27s/it]Inference:  47%|████▋     | 22/47 [08:52<10:30, 25.21s/it]Inference:  49%|████▉     | 23/47 [09:21<10:30, 26.25s/it]Inference:  51%|█████     | 24/47 [09:53<10:46, 28.12s/it]Inference:  53%|█████▎    | 25/47 [10:20<10:08, 27.64s/it]Inference:  55%|█████▌    | 26/47 [10:57<10:40, 30.52s/it]Inference:  57%|█████▋    | 27/47 [11:35<10:54, 32.73s/it]Inference:  60%|█████▉    | 28/47 [11:57<09:22, 29.60s/it]Inference:  62%|██████▏   | 29/47 [12:18<08:05, 26.98s/it]Inference:  64%|██████▍   | 30/47 [12:40<07:14, 25.56s/it]Inference:  66%|██████▌   | 31/47 [13:24<08:18, 31.13s/it]Inference:  68%|██████▊   | 32/47 [13:53<07:36, 30.47s/it]Inference:  70%|███████   | 33/47 [14:10<06:10, 26.44s/it]Inference:  72%|███████▏  | 34/47 [14:31<05:20, 24.63s/it]Inference:  74%|███████▍  | 35/47 [14:45<04:17, 21.48s/it]Inference:  77%|███████▋  | 36/47 [15:05<03:53, 21.18s/it]Inference:  79%|███████▊  | 37/47 [15:58<05:07, 30.73s/it]Inference:  81%|████████  | 38/47 [16:11<03:47, 25.24s/it]Inference:  83%|████████▎ | 39/47 [16:34<03:16, 24.54s/it]Inference:  85%|████████▌ | 40/47 [16:50<02:34, 22.03s/it]Inference:  87%|████████▋ | 41/47 [17:22<02:30, 25.03s/it]Inference:  89%|████████▉ | 42/47 [17:41<01:56, 23.27s/it]Inference:  91%|█████████▏| 43/47 [18:07<01:36, 24.01s/it]Inference:  94%|█████████▎| 44/47 [18:42<01:21, 27.30s/it]Inference:  96%|█████████▌| 45/47 [19:02<00:50, 25.14s/it]Inference:  98%|█████████▊| 46/47 [19:33<00:27, 27.14s/it]Inference: 100%|██████████| 47/47 [19:43<00:00, 22.00s/it]Inference: 100%|██████████| 47/47 [19:43<00:00, 25.19s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/54 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:24<22:03, 24.97s/it]Inference:   4%|▎         | 2/54 [00:47<20:37, 23.80s/it]Inference:   6%|▌         | 3/54 [01:09<19:27, 22.89s/it]Inference:   7%|▋         | 4/54 [01:37<20:43, 24.88s/it]Inference:   9%|▉         | 5/54 [01:55<18:22, 22.51s/it]Inference:  11%|█         | 6/54 [02:15<17:09, 21.46s/it]Inference:  13%|█▎        | 7/54 [02:55<21:39, 27.65s/it]Inference:  15%|█▍        | 8/54 [03:18<20:02, 26.14s/it]Inference:  17%|█▋        | 9/54 [03:59<22:59, 30.66s/it]Inference:  19%|█▊        | 10/54 [04:36<23:58, 32.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  20%|██        | 11/54 [04:55<20:19, 28.36s/it]Inference:  22%|██▏       | 12/54 [05:10<17:11, 24.55s/it]Inference:  24%|██▍       | 13/54 [05:31<15:59, 23.41s/it]Inference:  26%|██▌       | 14/54 [05:52<15:05, 22.63s/it]Inference:  28%|██▊       | 15/54 [06:17<15:15, 23.47s/it]Inference:  30%|██▉       | 16/54 [06:45<15:37, 24.68s/it]Inference:  31%|███▏      | 17/54 [07:11<15:30, 25.15s/it]Inference:  33%|███▎      | 18/54 [07:39<15:34, 25.97s/it]Inference:  35%|███▌      | 19/54 [08:04<14:54, 25.55s/it]Inference:  37%|███▋      | 20/54 [08:42<16:37, 29.34s/it]Inference:  39%|███▉      | 21/54 [09:07<15:24, 28.00s/it]Inference:  41%|████      | 22/54 [09:27<13:46, 25.83s/it]Inference:  43%|████▎     | 23/54 [10:06<15:23, 29.78s/it]Inference:  44%|████▍     | 24/54 [10:30<13:55, 27.84s/it]Inference:  46%|████▋     | 25/54 [10:49<12:15, 25.35s/it]Inference:  48%|████▊     | 26/54 [11:07<10:46, 23.09s/it]Inference:  50%|█████     | 27/54 [11:37<11:21, 25.24s/it]Inference:  52%|█████▏    | 28/54 [12:05<11:17, 26.04s/it]Inference:  54%|█████▎    | 29/54 [12:17<09:05, 21.81s/it]Inference:  56%|█████▌    | 30/54 [12:34<08:10, 20.43s/it]Inference:  57%|█████▋    | 31/54 [12:48<07:00, 18.28s/it]Inference:  59%|█████▉    | 32/54 [13:13<07:27, 20.34s/it]Inference:  61%|██████    | 33/54 [13:47<08:36, 24.59s/it]Inference:  63%|██████▎   | 34/54 [14:01<07:05, 21.28s/it]Inference:  65%|██████▍   | 35/54 [14:37<08:07, 25.68s/it]Inference:  67%|██████▋   | 36/54 [15:01<07:36, 25.34s/it]Inference:  69%|██████▊   | 37/54 [15:22<06:48, 24.01s/it]Inference:  70%|███████   | 38/54 [16:00<07:27, 27.98s/it]Inference:  72%|███████▏  | 39/54 [16:22<06:34, 26.27s/it]Inference:  74%|███████▍  | 40/54 [16:38<05:26, 23.33s/it]Inference:  76%|███████▌  | 41/54 [17:08<05:28, 25.27s/it]Inference:  78%|███████▊  | 42/54 [17:37<05:15, 26.27s/it]Inference:  80%|███████▉  | 43/54 [18:07<05:02, 27.51s/it]Inference:  81%|████████▏ | 44/54 [18:46<05:08, 30.84s/it]Inference:  83%|████████▎ | 45/54 [19:12<04:24, 29.43s/it]Inference:  85%|████████▌ | 46/54 [19:30<03:28, 26.03s/it]Inference:  87%|████████▋ | 47/54 [20:00<03:11, 27.32s/it]Inference:  89%|████████▉ | 48/54 [20:22<02:34, 25.77s/it]Inference:  91%|█████████ | 49/54 [20:28<01:38, 19.67s/it]Inference:  93%|█████████▎| 50/54 [21:03<01:37, 24.42s/it]Inference:  94%|█████████▍| 51/54 [21:10<00:56, 18.97s/it]Inference:  96%|█████████▋| 52/54 [21:36<00:42, 21.29s/it]Inference:  98%|█████████▊| 53/54 [22:07<00:24, 24.12s/it]Inference: 100%|██████████| 54/54 [22:33<00:00, 24.78s/it]Inference: 100%|██████████| 54/54 [22:33<00:00, 25.07s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/33 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:26<13:55, 26.09s/it]Inference:   6%|▌         | 2/33 [00:49<12:33, 24.30s/it]Inference:   9%|▉         | 3/33 [01:18<13:22, 26.73s/it]Inference:  12%|█▏        | 4/33 [01:53<14:31, 30.05s/it]Inference:  15%|█▌        | 5/33 [02:18<13:07, 28.14s/it]Inference:  18%|█▊        | 6/33 [02:49<13:08, 29.22s/it]Inference:  21%|██        | 7/33 [03:14<11:58, 27.62s/it]Inference:  24%|██▍       | 8/33 [03:40<11:22, 27.32s/it]Inference:  27%|██▋       | 9/33 [04:16<11:59, 29.99s/it]Inference:  30%|███       | 10/33 [04:35<10:09, 26.49s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  33%|███▎      | 11/33 [04:57<09:09, 24.99s/it]Inference:  36%|███▋      | 12/33 [05:08<07:19, 20.92s/it]Inference:  39%|███▉      | 13/33 [05:32<07:16, 21.82s/it]Inference:  42%|████▏     | 14/33 [05:53<06:49, 21.57s/it]Inference:  45%|████▌     | 15/33 [06:16<06:38, 22.12s/it]Inference:  48%|████▊     | 16/33 [06:46<06:55, 24.42s/it]Inference:  52%|█████▏    | 17/33 [07:07<06:13, 23.33s/it]Inference:  55%|█████▍    | 18/33 [07:26<05:29, 21.93s/it]Inference:  58%|█████▊    | 19/33 [08:10<06:39, 28.54s/it]Inference:  61%|██████    | 20/33 [08:46<06:42, 30.94s/it]Inference:  64%|██████▎   | 21/33 [09:02<05:18, 26.53s/it]Inference:  67%|██████▋   | 22/33 [09:25<04:39, 25.44s/it]Inference:  70%|██████▉   | 23/33 [10:14<05:22, 32.30s/it]Inference:  73%|███████▎  | 24/33 [10:32<04:11, 27.98s/it]Inference:  76%|███████▌  | 25/33 [11:18<04:28, 33.52s/it]Inference:  79%|███████▉  | 26/33 [11:38<03:26, 29.44s/it]Inference:  82%|████████▏ | 27/33 [12:13<03:07, 31.24s/it]Inference:  85%|████████▍ | 28/33 [12:35<02:21, 28.29s/it]Inference:  88%|████████▊ | 29/33 [13:04<01:53, 28.49s/it]Inference:  91%|█████████ | 30/33 [13:12<01:07, 22.36s/it]Inference:  94%|█████████▍| 31/33 [14:01<01:01, 30.51s/it]Inference:  97%|█████████▋| 32/33 [14:31<00:30, 30.26s/it]Inference: 100%|██████████| 33/33 [15:04<00:00, 31.12s/it]Inference: 100%|██████████| 33/33 [15:04<00:00, 27.41s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:24<13:18, 24.20s/it]Inference:   6%|▌         | 2/34 [00:46<12:17, 23.05s/it]Inference:   9%|▉         | 3/34 [01:11<12:19, 23.86s/it]Inference:  12%|█▏        | 4/34 [01:26<10:08, 20.27s/it]Inference:  15%|█▍        | 5/34 [01:59<12:00, 24.86s/it]Inference:  18%|█▊        | 6/34 [02:29<12:33, 26.90s/it]Inference:  21%|██        | 7/34 [02:41<09:48, 21.78s/it]Inference:  24%|██▎       | 8/34 [02:46<07:11, 16.60s/it]Inference:  26%|██▋       | 9/34 [02:59<06:23, 15.33s/it]Inference:  29%|██▉       | 10/34 [03:21<06:57, 17.39s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [03:38<06:38, 17.34s/it]Inference:  35%|███▌      | 12/34 [04:07<07:37, 20.78s/it]Inference:  38%|███▊      | 13/34 [04:29<07:26, 21.24s/it]Inference:  41%|████      | 14/34 [04:58<07:54, 23.75s/it]Inference:  44%|████▍     | 15/34 [05:06<06:00, 18.95s/it]Inference:  47%|████▋     | 16/34 [05:18<05:02, 16.79s/it]Inference:  50%|█████     | 17/34 [05:44<05:31, 19.47s/it]Inference:  53%|█████▎    | 18/34 [06:27<07:06, 26.64s/it]Inference:  56%|█████▌    | 19/34 [06:55<06:46, 27.13s/it]Inference:  59%|█████▉    | 20/34 [07:12<05:35, 23.96s/it]Inference:  62%|██████▏   | 21/34 [07:25<04:28, 20.68s/it]Inference:  65%|██████▍   | 22/34 [08:04<05:14, 26.22s/it]Inference:  68%|██████▊   | 23/34 [08:31<04:49, 26.30s/it]Inference:  71%|███████   | 24/34 [08:54<04:13, 25.34s/it]Inference:  74%|███████▎  | 25/34 [09:11<03:25, 22.89s/it]Inference:  76%|███████▋  | 26/34 [09:37<03:12, 24.02s/it]Inference:  79%|███████▉  | 27/34 [09:55<02:34, 22.04s/it]Inference:  82%|████████▏ | 28/34 [10:29<02:33, 25.62s/it]Inference:  85%|████████▌ | 29/34 [10:55<02:08, 25.70s/it]Inference:  88%|████████▊ | 30/34 [11:19<01:41, 25.32s/it]Inference:  91%|█████████ | 31/34 [11:50<01:20, 26.90s/it]Inference:  94%|█████████▍| 32/34 [12:31<01:02, 31.16s/it]Inference:  97%|█████████▋| 33/34 [12:43<00:25, 25.46s/it]Inference: 100%|██████████| 34/34 [13:00<00:00, 23.00s/it]Inference: 100%|██████████| 34/34 [13:00<00:00, 22.96s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/34 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:32<17:59, 32.72s/it]Inference:   6%|▌         | 2/34 [01:09<18:46, 35.21s/it]Inference:   9%|▉         | 3/34 [01:39<16:55, 32.75s/it]Inference:  12%|█▏        | 4/34 [02:11<16:13, 32.46s/it]Inference:  15%|█▍        | 5/34 [02:22<11:59, 24.80s/it]Inference:  18%|█▊        | 6/34 [02:46<11:24, 24.44s/it]Inference:  21%|██        | 7/34 [03:02<09:43, 21.60s/it]Inference:  24%|██▎       | 8/34 [03:37<11:12, 25.88s/it]Inference:  26%|██▋       | 9/34 [04:03<10:52, 26.08s/it]Inference:  29%|██▉       | 10/34 [04:28<10:17, 25.73s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  32%|███▏      | 11/34 [04:46<08:53, 23.21s/it]Inference:  35%|███▌      | 12/34 [05:05<08:04, 22.01s/it]Inference:  38%|███▊      | 13/34 [05:32<08:14, 23.57s/it]Inference:  41%|████      | 14/34 [06:13<09:34, 28.70s/it]Inference:  44%|████▍     | 15/34 [06:20<07:00, 22.16s/it]Inference:  47%|████▋     | 16/34 [06:49<07:18, 24.36s/it]Inference:  50%|█████     | 17/34 [07:11<06:41, 23.60s/it]Inference:  53%|█████▎    | 18/34 [07:20<05:08, 19.30s/it]Inference:  56%|█████▌    | 19/34 [07:40<04:49, 19.32s/it]Inference:  59%|█████▉    | 20/34 [08:12<05:27, 23.37s/it]Inference:  62%|██████▏   | 21/34 [08:32<04:47, 22.14s/it]Inference:  65%|██████▍   | 22/34 [09:10<05:23, 26.98s/it]Inference:  68%|██████▊   | 23/34 [09:39<05:03, 27.56s/it]Inference:  71%|███████   | 24/34 [10:04<04:27, 26.73s/it]Inference:  74%|███████▎  | 25/34 [10:18<03:26, 22.89s/it]Inference:  76%|███████▋  | 26/34 [10:35<02:49, 21.15s/it]Inference:  79%|███████▉  | 27/34 [10:55<02:25, 20.79s/it]Inference:  82%|████████▏ | 28/34 [11:11<01:55, 19.32s/it]Inference:  85%|████████▌ | 29/34 [11:49<02:05, 25.14s/it]Inference:  88%|████████▊ | 30/34 [12:21<01:48, 27.12s/it]Inference:  91%|█████████ | 31/34 [12:46<01:19, 26.41s/it]Inference:  94%|█████████▍| 32/34 [13:29<01:03, 31.56s/it]Inference:  97%|█████████▋| 33/34 [13:40<00:25, 25.28s/it]Inference: 100%|██████████| 34/34 [14:06<00:00, 25.36s/it]Inference: 100%|██████████| 34/34 [14:06<00:00, 24.88s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:34<23:37, 34.56s/it]Inference:   5%|▍         | 2/42 [01:02<20:23, 30.58s/it]Inference:   7%|▋         | 3/42 [01:20<16:18, 25.09s/it]Inference:  10%|▉         | 4/42 [01:51<17:11, 27.14s/it]Inference:  12%|█▏        | 5/42 [02:14<15:47, 25.60s/it]Inference:  14%|█▍        | 6/42 [02:35<14:25, 24.03s/it]Inference:  17%|█▋        | 7/42 [03:11<16:27, 28.22s/it]Inference:  19%|█▉        | 8/42 [03:38<15:46, 27.85s/it]Inference:  21%|██▏       | 9/42 [04:07<15:22, 27.95s/it]Inference:  24%|██▍       | 10/42 [04:44<16:30, 30.96s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [05:17<16:16, 31.51s/it]Inference:  29%|██▊       | 12/42 [05:30<12:51, 25.70s/it]Inference:  31%|███       | 13/42 [05:55<12:24, 25.67s/it]Inference:  33%|███▎      | 14/42 [06:27<12:51, 27.56s/it]Inference:  36%|███▌      | 15/42 [07:05<13:47, 30.64s/it]Inference:  38%|███▊      | 16/42 [07:24<11:50, 27.34s/it]Inference:  40%|████      | 17/42 [07:49<11:02, 26.48s/it]Inference:  43%|████▎     | 18/42 [08:12<10:07, 25.31s/it]Inference:  45%|████▌     | 19/42 [08:32<09:09, 23.89s/it]Inference:  48%|████▊     | 20/42 [08:57<08:49, 24.08s/it]Inference:  50%|█████     | 21/42 [09:18<08:06, 23.15s/it]Inference:  52%|█████▏    | 22/42 [09:33<06:57, 20.90s/it]Inference:  55%|█████▍    | 23/42 [09:56<06:47, 21.44s/it]Inference:  57%|█████▋    | 24/42 [10:20<06:37, 22.10s/it]Inference:  60%|█████▉    | 25/42 [10:37<05:50, 20.60s/it]Inference:  62%|██████▏   | 26/42 [11:09<06:24, 24.06s/it]Inference:  64%|██████▍   | 27/42 [11:22<05:13, 20.90s/it]Inference:  67%|██████▋   | 28/42 [11:47<05:08, 22.05s/it]Inference:  69%|██████▉   | 29/42 [12:09<04:44, 21.89s/it]Inference:  71%|███████▏  | 30/42 [12:29<04:18, 21.50s/it]Inference:  74%|███████▍  | 31/42 [13:00<04:26, 24.24s/it]Inference:  76%|███████▌  | 32/42 [13:31<04:23, 26.40s/it]Inference:  79%|███████▊  | 33/42 [14:06<04:20, 28.96s/it]Inference:  81%|████████  | 34/42 [14:49<04:24, 33.04s/it]Inference:  83%|████████▎ | 35/42 [15:18<03:43, 31.88s/it]Inference:  86%|████████▌ | 36/42 [16:06<03:40, 36.80s/it]Inference:  88%|████████▊ | 37/42 [16:26<02:38, 31.61s/it]Inference:  90%|█████████ | 38/42 [16:58<02:06, 31.67s/it]Inference:  93%|█████████▎| 39/42 [17:22<01:28, 29.54s/it]Inference:  95%|█████████▌| 40/42 [17:45<00:55, 27.55s/it]Inference:  98%|█████████▊| 41/42 [18:13<00:27, 27.70s/it]Inference: 100%|██████████| 42/42 [18:53<00:00, 31.22s/it]Inference: 100%|██████████| 42/42 [18:53<00:00, 26.98s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Inference:   0%|          | 0/42 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:23<16:12, 23.72s/it]Inference:   5%|▍         | 2/42 [00:41<13:34, 20.35s/it]Inference:   7%|▋         | 3/42 [01:19<18:28, 28.41s/it]Inference:  10%|▉         | 4/42 [01:34<14:31, 22.94s/it]Inference:  12%|█▏        | 5/42 [01:50<12:36, 20.44s/it]Inference:  14%|█▍        | 6/42 [02:10<12:12, 20.33s/it]Inference:  17%|█▋        | 7/42 [02:35<12:46, 21.91s/it]Inference:  19%|█▉        | 8/42 [02:55<11:59, 21.16s/it]Inference:  21%|██▏       | 9/42 [03:20<12:18, 22.37s/it]Inference:  24%|██▍       | 10/42 [03:40<11:34, 21.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Inference:  26%|██▌       | 11/42 [04:04<11:35, 22.42s/it]Inference:  29%|██▊       | 12/42 [04:30<11:44, 23.50s/it]Inference:  31%|███       | 13/42 [04:55<11:35, 23.99s/it]Inference:  33%|███▎      | 14/42 [05:25<12:00, 25.73s/it]Inference:  36%|███▌      | 15/42 [05:51<11:36, 25.80s/it]Inference:  38%|███▊      | 16/42 [06:33<13:21, 30.85s/it]Inference:  40%|████      | 17/42 [07:00<12:22, 29.68s/it]Inference:  43%|████▎     | 18/42 [07:14<09:57, 24.88s/it]Inference:  45%|████▌     | 19/42 [07:38<09:23, 24.50s/it]Inference:  48%|████▊     | 20/42 [08:01<08:50, 24.14s/it]Inference:  50%|█████     | 21/42 [08:29<08:51, 25.32s/it]Inference:  52%|█████▏    | 22/42 [08:40<06:58, 20.90s/it]Inference:  55%|█████▍    | 23/42 [09:17<08:09, 25.75s/it]Inference:  57%|█████▋    | 24/42 [09:32<06:45, 22.54s/it]Inference:  60%|█████▉    | 25/42 [10:03<07:05, 25.04s/it]Inference:  62%|██████▏   | 26/42 [10:29<06:45, 25.37s/it]Inference:  64%|██████▍   | 27/42 [10:54<06:19, 25.30s/it]Inference:  67%|██████▋   | 28/42 [11:19<05:52, 25.19s/it]Inference:  69%|██████▉   | 29/42 [11:43<05:24, 24.94s/it]Inference:  71%|███████▏  | 30/42 [12:19<05:37, 28.13s/it]Inference:  74%|███████▍  | 31/42 [12:49<05:15, 28.70s/it]Inference:  76%|███████▌  | 32/42 [13:16<04:43, 28.33s/it]Inference:  79%|███████▊  | 33/42 [13:40<04:02, 26.96s/it]Inference:  81%|████████  | 34/42 [14:06<03:32, 26.58s/it]Inference:  83%|████████▎ | 35/42 [15:20<04:46, 40.96s/it]Inference:  86%|████████▌ | 36/42 [16:01<04:06, 41.02s/it]Inference:  88%|████████▊ | 37/42 [16:44<03:27, 41.56s/it]Inference:  90%|█████████ | 38/42 [17:17<02:36, 39.09s/it]Inference:  93%|█████████▎| 39/42 [18:16<02:14, 44.83s/it]Inference:  95%|█████████▌| 40/42 [19:04<01:31, 45.87s/it]Inference:  98%|█████████▊| 41/42 [19:39<00:42, 42.57s/it]Inference: 100%|██████████| 42/42 [20:27<00:00, 44.25s/it]Inference: 100%|██████████| 42/42 [20:27<00:00, 29.23s/it]
/home/namwoam/dl-final/llm/inference-reverse.py:113: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...        icl_type
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  prompt_reverse

[1 rows x 8 columns]
  dataset_name                                    path  ...  4_tile  5_tile
0       113-ss  ../dataset/gsat/113_social_studies.csv  ...   39.56   28.25
1       112-ss  ../dataset/gsat/112_social_studies.csv  ...   38.13   27.24
2       111-ss  ../dataset/gsat/111_social_studies.csv  ...   45.90   34.42
3       110-ss  ../dataset/gsat/110_social_studies.csv  ...   40.50   34.72
4       109-ss  ../dataset/gsat/109_social_studies.csv  ...   49.01   36.76

[5 rows x 7 columns]
Running dataset:113-ss on model:breeze-7b-icl_reverse
Running dataset:112-ss on model:breeze-7b-icl_reverse
Running dataset:111-ss on model:breeze-7b-icl_reverse
Running dataset:110-ss on model:breeze-7b-icl_reverse
Running dataset:109-ss on model:breeze-7b-icl_reverse
Running dataset:113-ch on model:breeze-7b-icl_reverse
Running dataset:112-ch on model:breeze-7b-icl_reverse
Running dataset:111-ch on model:breeze-7b-icl_reverse
Running dataset:110-ch on model:breeze-7b-icl_reverse
Running dataset:109-ch on model:breeze-7b-icl_reverse
