Inference:   3%|▎         | 1/33 [00:14<07:37, 14.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:25<06:23, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [00:38<06:24, 12.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [00:54<06:41, 13.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [01:24<09:17, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [01:35<07:35, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [01:44<06:08, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [02:12<07:46, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [02:36<08:06, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:02<08:24, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [03:13<06:52, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [03:35<06:51, 19.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [04:07<07:51, 23.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [04:47<09:01, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [05:18<08:42, 29.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [05:39<07:36, 26.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [05:59<06:36, 24.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [06:16<05:35, 22.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [06:34<04:55, 21.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [07:06<05:16, 24.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [07:24<04:27, 22.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [07:34<03:26, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [07:53<03:07, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [08:20<03:10, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [08:31<02:25, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [09:24<03:20, 28.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [10:24<03:48, 38.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [10:43<02:41, 32.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [11:15<02:08, 32.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [11:55<01:44, 34.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [12:17<01:01, 30.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [12:31<00:25, 25.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [13:03<00:00, 27.69s/it]Inference: 100%|██████████| 33/33 [13:03<00:00, 23.75s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:11<06:11, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:23<06:19, 11.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:51<09:57, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [00:54<06:28, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:09<06:31, 13.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [01:18<05:39, 12.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [01:36<06:16, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [02:03<07:52, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [02:19<07:17, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [02:23<05:17, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [03:24<10:39, 27.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [03:46<09:35, 26.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [04:11<09:03, 25.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [04:59<10:46, 32.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [05:11<08:17, 26.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [05:17<06:04, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [05:29<04:59, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [05:47<04:47, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [06:03<04:18, 17.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [06:18<03:50, 16.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [06:29<03:16, 15.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [06:45<03:04, 15.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [07:44<05:10, 28.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [08:29<05:34, 33.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [08:48<04:21, 29.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [08:58<03:07, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [09:21<02:42, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [09:40<02:12, 22.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [10:05<01:54, 22.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [10:34<01:38, 24.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [10:47<01:03, 21.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [11:19<00:48, 24.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [11:51<00:26, 26.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [12:06<00:00, 23.26s/it]Inference: 100%|██████████| 34/34 [12:06<00:00, 21.37s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/34 [00:13<07:17, 13.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/34 [00:27<07:27, 13.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/34 [00:52<09:52, 19.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/34 [01:25<12:09, 24.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 5/34 [01:36<09:28, 19.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/34 [02:02<10:07, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/34 [02:14<08:21, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▎       | 8/34 [02:28<07:27, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▋       | 9/34 [03:23<12:06, 29.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 10/34 [03:33<09:12, 23.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 11/34 [04:02<09:34, 24.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 12/34 [04:36<10:10, 27.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 13/34 [05:07<10:00, 28.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 14/34 [05:14<07:23, 22.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 15/34 [05:46<07:57, 25.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 16/34 [06:19<08:12, 27.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 17/34 [06:29<06:15, 22.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 18/34 [06:39<04:56, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 19/34 [07:03<05:01, 20.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 20/34 [07:25<04:53, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 21/34 [08:22<06:52, 31.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 22/34 [08:41<05:34, 27.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 23/34 [09:02<04:41, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 24/34 [09:18<03:48, 22.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▎  | 25/34 [09:40<03:24, 22.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▋  | 26/34 [10:07<03:11, 23.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 27/34 [10:43<03:11, 27.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 28/34 [11:43<03:44, 37.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 29/34 [12:17<03:01, 36.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 30/34 [12:46<02:16, 34.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 31/34 [13:16<01:38, 32.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 32/34 [14:02<01:13, 36.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 33/34 [14:21<00:31, 31.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 34/34 [14:43<00:00, 28.73s/it]Inference: 100%|██████████| 34/34 [14:43<00:00, 26.00s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:19<13:09, 19.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:51<17:48, 26.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [01:26<19:58, 30.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:45<16:25, 25.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [01:57<12:54, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [02:24<13:45, 22.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [02:46<13:14, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [02:59<11:04, 19.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [03:18<10:39, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:42<11:12, 21.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [04:14<12:27, 24.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:32<11:10, 22.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [05:03<12:07, 25.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [05:24<11:04, 23.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [05:51<11:07, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [06:11<10:09, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [06:47<11:15, 27.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [07:47<14:50, 37.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [08:10<12:33, 32.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [08:30<10:37, 28.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [08:44<08:35, 24.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [09:06<07:53, 23.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [09:55<09:55, 31.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [10:23<09:03, 30.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [12:17<15:40, 55.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [12:53<13:13, 49.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [13:07<09:43, 38.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [13:14<06:51, 29.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [13:38<06:02, 27.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [14:03<05:24, 27.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [14:24<04:36, 25.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [14:40<03:44, 22.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [15:01<03:16, 21.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [17:01<06:50, 51.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [17:20<04:51, 41.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [18:31<05:02, 50.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [19:14<04:00, 48.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [19:24<02:26, 36.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [19:51<01:41, 33.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [20:23<01:06, 33.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [20:59<00:34, 34.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [21:13<00:00, 27.93s/it]Inference: 100%|██████████| 42/42 [21:13<00:00, 30.31s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/42 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/42 [00:23<16:06, 23.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/42 [00:36<11:34, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/42 [01:23<19:55, 30.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/42 [01:42<16:33, 26.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/42 [02:16<17:58, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/42 [02:42<16:42, 27.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/42 [02:55<13:30, 23.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 8/42 [03:11<11:45, 20.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 9/42 [03:17<08:59, 16.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/42 [03:43<10:14, 19.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/42 [04:02<09:55, 19.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 12/42 [04:33<11:24, 22.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 13/42 [05:08<12:46, 26.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/42 [05:16<09:45, 20.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 15/42 [05:40<09:46, 21.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 16/42 [06:17<11:22, 26.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 17/42 [06:38<10:20, 24.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 18/42 [06:51<08:33, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 19/42 [07:04<07:10, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 20/42 [07:29<07:31, 20.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 21/42 [07:58<08:03, 23.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 22/42 [08:10<06:35, 19.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 23/42 [08:19<05:13, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 24/42 [08:35<04:58, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 25/42 [08:42<03:51, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 26/42 [08:57<03:43, 13.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 27/42 [09:43<05:54, 23.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 28/42 [10:09<05:39, 24.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 29/42 [10:34<05:20, 24.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 30/42 [10:45<04:05, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 31/42 [11:13<04:09, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 32/42 [11:38<03:52, 23.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 33/42 [11:57<03:20, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 34/42 [12:17<02:51, 21.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 35/42 [12:29<02:10, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 36/42 [13:06<02:24, 24.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 37/42 [14:16<03:10, 38.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 38/42 [15:06<02:45, 41.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 39/42 [15:30<01:48, 36.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 40/42 [15:51<01:03, 31.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 41/42 [16:29<00:33, 33.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 42/42 [17:53<00:00, 48.59s/it]Inference: 100%|██████████| 42/42 [17:53<00:00, 25.56s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Inference:   0%|          | 0/22 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/22 [00:36<12:50, 36.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 2/22 [01:12<12:05, 36.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 3/22 [01:49<11:35, 36.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 4/22 [02:11<09:13, 30.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 5/22 [02:43<08:51, 31.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 6/22 [03:05<07:27, 27.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 7/22 [03:34<07:06, 28.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 8/22 [04:34<08:56, 38.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 9/22 [11:05<32:13, 148.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 10/22 [11:16<21:14, 106.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 11/22 [11:39<14:47, 80.68s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 12/22 [12:11<10:59, 65.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 13/22 [12:42<08:16, 55.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 14/22 [13:07<06:10, 46.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 15/22 [13:47<05:09, 44.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 16/22 [14:30<04:23, 43.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 17/22 [14:57<03:13, 38.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 18/22 [15:15<02:10, 32.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 19/22 [15:38<01:29, 29.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 20/22 [16:08<00:59, 29.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 21/22 [16:47<00:32, 32.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 22/22 [17:09<00:00, 29.55s/it]Inference: 100%|██████████| 22/22 [17:09<00:00, 46.82s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Inference:   0%|          | 0/27 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   4%|▎         | 1/27 [00:29<12:41, 29.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 2/27 [00:50<10:17, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 3/27 [01:48<15:55, 39.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 4/27 [02:00<11:02, 28.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 5/27 [02:19<09:15, 25.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 6/27 [02:34<07:37, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 7/27 [03:10<08:50, 26.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 8/27 [03:27<07:22, 23.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 9/27 [03:57<07:41, 25.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 10/27 [04:33<08:09, 28.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 11/27 [05:30<09:58, 37.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 12/27 [06:07<09:17, 37.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 13/27 [06:22<07:08, 30.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 14/27 [07:07<07:34, 34.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 15/27 [07:44<07:06, 35.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 16/27 [08:23<06:41, 36.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 17/27 [08:41<05:11, 31.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 18/27 [08:53<03:46, 25.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 19/27 [09:09<03:00, 22.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 20/27 [09:40<02:54, 24.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 21/27 [10:07<02:34, 25.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 22/27 [10:22<01:51, 22.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 23/27 [10:38<01:21, 20.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 24/27 [11:11<01:13, 24.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 25/27 [11:26<00:43, 21.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 26/27 [11:55<00:23, 23.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 27/27 [12:10<00:00, 21.13s/it]Inference: 100%|██████████| 27/27 [12:10<00:00, 27.06s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Inference:   0%|          | 0/21 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   5%|▍         | 1/21 [00:23<07:56, 23.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 2/21 [00:49<07:50, 24.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 3/21 [01:48<12:08, 40.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 4/21 [02:17<10:09, 35.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 5/21 [08:53<44:11, 165.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▊       | 6/21 [09:30<30:33, 122.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 7/21 [10:04<21:44, 93.17s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 8/21 [10:28<15:24, 71.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 9/21 [10:39<10:27, 52.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 10/21 [10:54<07:30, 40.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 11/21 [11:13<05:43, 34.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 12/21 [11:46<05:04, 33.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 13/21 [12:07<03:58, 29.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 14/21 [12:48<03:52, 33.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████▏  | 15/21 [13:10<02:59, 29.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 16/21 [13:28<02:11, 26.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 17/21 [13:42<01:30, 22.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 18/21 [14:03<01:06, 22.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 19/21 [14:32<00:48, 24.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 20/21 [15:06<00:27, 27.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 21/21 [15:29<00:00, 25.94s/it]Inference: 100%|██████████| 21/21 [15:29<00:00, 44.27s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/39 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/39 [00:43<27:22, 43.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/39 [01:01<17:34, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/39 [01:35<18:34, 30.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|█         | 4/39 [01:57<16:06, 27.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 5/39 [02:31<16:59, 29.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 6/39 [02:56<15:29, 28.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 7/39 [03:16<13:38, 25.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 8/39 [03:34<11:58, 23.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 9/39 [04:40<18:16, 36.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 10/39 [05:04<15:48, 32.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 11/39 [05:25<13:32, 29.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███       | 12/39 [06:08<14:54, 33.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 13/39 [06:29<12:50, 29.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 14/39 [06:52<11:31, 27.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 15/39 [07:09<09:42, 24.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 16/39 [07:30<08:55, 23.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▎     | 17/39 [07:43<07:23, 20.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 18/39 [07:54<06:10, 17.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 19/39 [08:35<08:10, 24.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 20/39 [09:13<09:03, 28.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 21/39 [09:27<07:12, 24.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▋    | 22/39 [09:56<07:17, 25.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 23/39 [10:23<06:56, 26.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 24/39 [10:50<06:37, 26.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 25/39 [11:06<05:27, 23.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 26/39 [11:20<04:26, 20.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▉   | 27/39 [12:04<05:27, 27.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 28/39 [12:22<04:30, 24.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 29/39 [12:59<04:42, 28.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 30/39 [13:23<04:03, 27.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 31/39 [13:34<02:58, 22.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 32/39 [14:17<03:18, 28.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 33/39 [14:45<02:51, 28.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 34/39 [15:23<02:35, 31.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|████████▉ | 35/39 [15:32<01:38, 24.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 36/39 [16:08<01:23, 27.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 37/39 [16:18<00:45, 22.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 38/39 [16:56<00:27, 27.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 39/39 [17:12<00:00, 23.90s/it]Inference: 100%|██████████| 39/39 [17:12<00:00, 26.48s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
Inference:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/33 [00:12<06:50, 12.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 2/33 [00:31<08:24, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 3/33 [00:45<07:38, 15.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 4/33 [01:06<08:30, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▌        | 5/33 [01:29<09:09, 19.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  18%|█▊        | 6/33 [01:48<08:40, 19.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 7/33 [02:10<08:46, 20.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 8/33 [02:48<10:44, 25.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 9/33 [03:13<10:11, 25.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 10/33 [03:29<08:43, 22.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 11/33 [04:11<10:26, 28.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▋      | 12/33 [04:22<08:04, 23.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 13/33 [05:08<10:05, 30.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 14/33 [05:27<08:30, 26.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▌     | 15/33 [05:49<07:33, 25.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 16/33 [06:07<06:34, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 17/33 [06:43<07:13, 27.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▍    | 18/33 [07:13<06:58, 27.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 19/33 [07:36<06:09, 26.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 20/33 [08:04<05:48, 26.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▎   | 21/33 [08:26<05:03, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 22/33 [08:41<04:05, 22.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 23/33 [09:09<03:59, 23.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 24/33 [09:30<03:27, 23.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 25/33 [10:00<03:22, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 26/33 [10:16<02:36, 22.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  82%|████████▏ | 27/33 [10:38<02:13, 22.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▍ | 28/33 [10:46<01:30, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 29/33 [11:19<01:29, 22.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 30/33 [11:31<00:57, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 31/33 [12:01<00:45, 22.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 32/33 [12:26<00:23, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 33/33 [12:40<00:00, 20.37s/it]Inference: 100%|██████████| 33/33 [12:40<00:00, 23.03s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/43 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/43 [00:22<15:28, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/43 [00:42<14:18, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/43 [00:59<12:45, 19.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 4/43 [01:19<12:40, 19.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/43 [01:33<11:10, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▍        | 6/43 [01:56<11:57, 19.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▋        | 7/43 [02:12<10:57, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 8/43 [02:33<11:06, 19.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██        | 9/43 [02:42<09:05, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 10/43 [03:10<10:50, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 11/43 [04:00<15:32, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 12/43 [05:01<20:02, 38.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|███       | 13/43 [05:41<19:29, 39.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 14/43 [05:59<15:48, 32.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▍      | 15/43 [06:09<12:05, 25.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 16/43 [06:41<12:31, 27.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|███▉      | 17/43 [07:31<14:50, 34.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  42%|████▏     | 18/43 [07:48<12:08, 29.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 19/43 [08:07<10:26, 26.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 20/43 [08:12<07:37, 19.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 21/43 [08:30<07:00, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 22/43 [08:57<07:34, 21.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 23/43 [09:18<07:06, 21.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 24/43 [09:49<07:44, 24.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  58%|█████▊    | 25/43 [10:05<06:31, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|██████    | 26/43 [10:14<05:05, 17.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 27/43 [10:38<05:17, 19.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▌   | 28/43 [11:00<05:04, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 29/43 [11:25<05:06, 21.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|██████▉   | 30/43 [11:35<03:55, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 31/43 [12:07<04:26, 22.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 32/43 [12:20<03:35, 19.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 33/43 [13:03<04:26, 26.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▉  | 34/43 [13:26<03:50, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 35/43 [13:45<03:07, 23.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▎ | 36/43 [14:03<02:32, 21.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▌ | 37/43 [14:13<01:50, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 38/43 [14:34<01:36, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 39/43 [14:43<01:04, 16.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 40/43 [15:03<00:51, 17.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 41/43 [15:20<00:34, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 42/43 [15:41<00:18, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 43/43 [16:01<00:00, 18.80s/it]Inference: 100%|██████████| 43/43 [16:01<00:00, 22.36s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Inference:   0%|          | 0/41 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/41 [00:21<14:15, 21.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▍         | 2/41 [00:41<13:32, 20.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 3/41 [00:59<12:15, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  10%|▉         | 4/41 [01:51<19:53, 32.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  12%|█▏        | 5/41 [02:00<14:18, 23.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 6/41 [02:18<12:42, 21.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 7/41 [02:53<14:46, 26.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|█▉        | 8/41 [03:20<14:34, 26.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 9/41 [03:29<11:17, 21.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 10/41 [04:00<12:23, 23.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 11/41 [04:21<11:37, 23.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  29%|██▉       | 12/41 [04:52<12:16, 25.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 13/41 [05:10<10:49, 23.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 14/41 [05:27<09:37, 21.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 15/41 [05:57<10:26, 24.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 16/41 [06:06<08:02, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████▏     | 17/41 [06:16<06:37, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 18/41 [06:24<05:20, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 19/41 [06:38<05:08, 14.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 20/41 [06:46<04:17, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 21/41 [07:03<04:36, 13.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 22/41 [07:18<04:28, 14.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 23/41 [07:58<06:32, 21.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▊    | 24/41 [08:21<06:18, 22.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 25/41 [08:28<04:40, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 26/41 [09:49<09:09, 36.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 27/41 [10:25<08:30, 36.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 28/41 [10:38<06:21, 29.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  71%|███████   | 29/41 [10:52<04:57, 24.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 30/41 [11:04<03:50, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 31/41 [11:12<02:52, 17.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 32/41 [11:58<03:52, 25.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|████████  | 33/41 [12:05<02:41, 20.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 34/41 [12:28<02:26, 20.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 35/41 [13:01<02:26, 24.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  88%|████████▊ | 36/41 [13:20<01:54, 22.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  90%|█████████ | 37/41 [14:09<02:03, 30.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 38/41 [14:37<01:29, 29.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▌| 39/41 [15:10<01:01, 30.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 40/41 [15:23<00:25, 25.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 41/41 [15:48<00:00, 25.30s/it]Inference: 100%|██████████| 41/41 [15:48<00:00, 23.13s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]
Inference:   0%|          | 0/37 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   3%|▎         | 1/37 [00:20<12:20, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   5%|▌         | 2/37 [00:42<12:37, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   8%|▊         | 3/37 [01:17<15:37, 27.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 4/37 [01:40<14:13, 25.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  14%|█▎        | 5/37 [02:11<14:37, 27.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  16%|█▌        | 6/37 [02:17<10:28, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 7/37 [02:53<12:48, 25.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 8/37 [03:21<12:36, 26.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 9/37 [03:31<09:51, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  27%|██▋       | 10/37 [03:51<09:21, 20.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 11/37 [05:50<22:00, 50.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 12/37 [06:13<17:40, 42.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 13/37 [06:23<13:02, 32.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 14/37 [06:39<10:31, 27.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 15/37 [07:22<11:48, 32.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 16/37 [07:27<08:27, 24.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▌     | 17/37 [07:51<08:02, 24.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▊     | 18/37 [08:22<08:13, 26.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████▏    | 19/37 [08:42<07:16, 24.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▍    | 20/37 [09:00<06:19, 22.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 21/37 [09:18<05:38, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 22/37 [09:43<05:33, 22.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 23/37 [10:03<05:01, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 24/37 [10:20<04:24, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 25/37 [12:14<09:39, 48.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 26/37 [12:26<06:50, 37.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  73%|███████▎  | 27/37 [13:00<06:03, 36.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 28/37 [13:27<05:04, 33.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 29/37 [14:06<04:42, 35.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 30/37 [14:31<03:44, 32.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  84%|████████▍ | 31/37 [14:43<02:35, 25.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  86%|████████▋ | 32/37 [15:00<01:56, 23.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 33/37 [15:17<01:25, 21.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  92%|█████████▏| 34/37 [15:42<01:07, 22.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  95%|█████████▍| 35/37 [16:19<00:54, 27.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  97%|█████████▋| 36/37 [16:28<00:21, 21.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 37/37 [16:46<00:00, 20.52s/it]Inference: 100%|██████████| 37/37 [16:46<00:00, 27.21s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]
Inference:   0%|          | 0/47 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/47 [00:18<13:52, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▍         | 2/47 [00:28<10:06, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▋         | 3/47 [00:49<12:30, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▊         | 4/47 [01:22<16:47, 23.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 5/47 [01:55<18:37, 26.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 6/47 [03:54<39:41, 58.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 7/47 [04:14<30:29, 45.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 8/47 [04:30<23:34, 36.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▉        | 9/47 [04:50<19:46, 31.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  21%|██▏       | 10/47 [05:11<17:17, 28.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  23%|██▎       | 11/47 [05:40<16:59, 28.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 12/47 [05:49<13:07, 22.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 13/47 [06:38<17:14, 30.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 14/47 [07:18<18:18, 33.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  32%|███▏      | 15/47 [07:34<14:57, 28.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  34%|███▍      | 16/47 [07:50<12:42, 24.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  36%|███▌      | 17/47 [08:01<10:16, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  38%|███▊      | 18/47 [08:12<08:27, 17.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  40%|████      | 19/47 [08:25<07:29, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 20/47 [09:36<14:45, 32.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  45%|████▍     | 21/47 [09:45<11:00, 25.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  47%|████▋     | 22/47 [10:09<10:28, 25.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  49%|████▉     | 23/47 [10:26<09:04, 22.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  51%|█████     | 24/47 [10:41<07:49, 20.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  53%|█████▎    | 25/47 [10:47<05:53, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  55%|█████▌    | 26/47 [10:54<04:41, 13.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 27/47 [11:22<05:53, 17.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  60%|█████▉    | 28/47 [11:37<05:23, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  62%|██████▏   | 29/47 [12:02<05:46, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  64%|██████▍   | 30/47 [12:37<06:47, 23.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  66%|██████▌   | 31/47 [13:00<06:21, 23.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  68%|██████▊   | 32/47 [13:11<04:58, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 33/47 [14:05<07:00, 30.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 34/47 [14:23<05:45, 26.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 35/47 [14:35<04:26, 22.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  77%|███████▋  | 36/47 [15:21<05:21, 29.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  79%|███████▊  | 37/47 [15:33<03:59, 23.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████  | 38/47 [15:44<03:02, 20.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 39/47 [16:18<03:14, 24.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 40/47 [16:37<02:38, 22.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 41/47 [16:52<02:03, 20.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 42/47 [17:14<01:45, 21.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████▏| 43/47 [17:48<01:38, 24.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▎| 44/47 [18:11<01:12, 24.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▌| 45/47 [18:45<00:54, 27.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 46/47 [19:04<00:24, 24.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 47/47 [19:21<00:00, 22.44s/it]Inference: 100%|██████████| 47/47 [19:21<00:00, 24.71s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]
Inference:   0%|          | 0/54 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Inference:   2%|▏         | 1/54 [00:15<13:28, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   4%|▎         | 2/54 [00:39<17:45, 20.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   6%|▌         | 3/54 [00:50<13:35, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   7%|▋         | 4/54 [01:12<15:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:   9%|▉         | 5/54 [01:32<15:41, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  11%|█         | 6/54 [01:52<15:28, 19.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  13%|█▎        | 7/54 [02:16<16:27, 21.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  15%|█▍        | 8/54 [02:41<17:01, 22.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  17%|█▋        | 9/54 [02:50<13:36, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  19%|█▊        | 10/54 [03:03<11:58, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  20%|██        | 11/54 [03:16<11:06, 15.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  22%|██▏       | 12/54 [03:40<12:41, 18.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  24%|██▍       | 13/54 [03:58<12:17, 17.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  26%|██▌       | 14/54 [04:06<09:59, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  28%|██▊       | 15/54 [04:15<08:36, 13.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  30%|██▉       | 16/54 [04:23<07:21, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  31%|███▏      | 17/54 [04:38<07:46, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  33%|███▎      | 18/54 [05:30<14:38, 24.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  35%|███▌      | 19/54 [06:23<19:19, 33.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  37%|███▋      | 20/54 [06:53<18:15, 32.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  39%|███▉      | 21/54 [07:03<13:56, 25.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  41%|████      | 22/54 [07:12<10:57, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  43%|████▎     | 23/54 [07:45<12:28, 24.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  44%|████▍     | 24/54 [07:57<10:16, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  46%|████▋     | 25/54 [08:07<08:21, 17.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  48%|████▊     | 26/54 [08:24<08:06, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  50%|█████     | 27/54 [08:36<07:01, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  52%|█████▏    | 28/54 [09:00<07:50, 18.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  54%|█████▎    | 29/54 [09:10<06:38, 15.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  56%|█████▌    | 30/54 [09:19<05:32, 13.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  57%|█████▋    | 31/54 [09:39<05:57, 15.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  59%|█████▉    | 32/54 [09:48<05:01, 13.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  61%|██████    | 33/54 [10:03<04:51, 13.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  63%|██████▎   | 34/54 [10:11<04:06, 12.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  65%|██████▍   | 35/54 [11:04<07:45, 24.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  67%|██████▋   | 36/54 [11:18<06:23, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  69%|██████▊   | 37/54 [11:40<06:03, 21.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  70%|███████   | 38/54 [12:05<06:03, 22.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  72%|███████▏  | 39/54 [12:20<05:04, 20.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  74%|███████▍  | 40/54 [12:38<04:36, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  76%|███████▌  | 41/54 [13:07<04:51, 22.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  78%|███████▊  | 42/54 [13:26<04:17, 21.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  80%|███████▉  | 43/54 [13:43<03:38, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  81%|████████▏ | 44/54 [14:14<03:54, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  83%|████████▎ | 45/54 [14:33<03:18, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  85%|████████▌ | 46/54 [14:56<02:58, 22.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  87%|████████▋ | 47/54 [15:21<02:41, 23.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  89%|████████▉ | 48/54 [15:52<02:33, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  91%|█████████ | 49/54 [16:22<02:14, 26.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  93%|█████████▎| 50/54 [16:41<01:37, 24.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  94%|█████████▍| 51/54 [16:53<01:02, 20.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  96%|█████████▋| 52/54 [17:00<00:33, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference:  98%|█████████▊| 53/54 [17:07<00:13, 13.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Inference: 100%|██████████| 54/54 [17:43<00:00, 20.50s/it]Inference: 100%|██████████| 54/54 [17:43<00:00, 19.70s/it]
/home/namwoam/dl-final/llm/inference-lora.py:121: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  result_df.loc[:, "score"] = scores
                                           model  ...                lora_path
0  MediaTek-Research/Breeze-7B-32k-Instruct-v1_0  ...  ../finetune/is-all_gsat

[1 rows x 6 columns]
  dataset_name                             path  1_tile  ...  3_tile  4_tile  5_tile
0       113-ch  ../dataset/gsat/113_chinese.csv   63.48  ...   52.90   42.32   37.03
1       112-ch  ../dataset/gsat/112_chinese.csv   62.63  ...   52.19   41.75   36.54
2       111-ch  ../dataset/gsat/111_chinese.csv   64.08  ...   48.06   42.72   32.04
3       110-ch  ../dataset/gsat/110_chinese.csv   64.57  ...   53.81   43.05   37.67
4       109-ch  ../dataset/gsat/109_chinese.csv   65.65  ...   54.71   43.77   38.30

[5 rows x 7 columns]
Running dataset:113-ch on model:breeze-7b-is_gsat
Running dataset:112-ch on model:breeze-7b-is_gsat
Running dataset:111-ch on model:breeze-7b-is_gsat
Running dataset:110-ch on model:breeze-7b-is_gsat
Running dataset:109-ch on model:breeze-7b-is_gsat
Running dataset:113-ns on model:breeze-7b-is_gsat
Running dataset:112-ns on model:breeze-7b-is_gsat
Running dataset:111-ns on model:breeze-7b-is_gsat
Running dataset:110-ns on model:breeze-7b-is_gsat
Running dataset:109-ns on model:breeze-7b-is_gsat
Running dataset:113-ss on model:breeze-7b-is_gsat
Running dataset:112-ss on model:breeze-7b-is_gsat
Running dataset:111-ss on model:breeze-7b-is_gsat
Running dataset:110-ss on model:breeze-7b-is_gsat
Running dataset:109-ss on model:breeze-7b-is_gsat
