Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Instruction dataset size:459
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
Loaded lora weight from: cp-all_textbook/
  0%|          | 0/920 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/920 [00:14<3:40:07, 14.37s/it]  0%|          | 2/920 [00:27<3:29:59, 13.73s/it]  0%|          | 3/920 [00:41<3:27:26, 13.57s/it]  0%|          | 4/920 [00:54<3:26:31, 13.53s/it]  1%|          | 5/920 [01:07<3:25:48, 13.50s/it]  1%|          | 6/920 [01:21<3:24:43, 13.44s/it]  1%|          | 7/920 [01:34<3:24:20, 13.43s/it]  1%|          | 8/920 [01:48<3:24:25, 13.45s/it]  1%|          | 9/920 [02:01<3:23:08, 13.38s/it]  1%|          | 10/920 [02:14<3:21:06, 13.26s/it]  1%|          | 11/920 [02:27<3:21:17, 13.29s/it]  1%|▏         | 12/920 [02:41<3:21:54, 13.34s/it]  1%|▏         | 13/920 [02:54<3:21:34, 13.33s/it]  2%|▏         | 14/920 [03:07<3:19:35, 13.22s/it]  2%|▏         | 15/920 [03:20<3:17:25, 13.09s/it]  2%|▏         | 16/920 [03:33<3:19:09, 13.22s/it]  2%|▏         | 17/920 [03:47<3:19:50, 13.28s/it]  2%|▏         | 18/920 [04:00<3:20:00, 13.30s/it]  2%|▏         | 19/920 [04:12<3:14:56, 12.98s/it]  2%|▏         | 20/920 [04:25<3:15:10, 13.01s/it]  2%|▏         | 21/920 [04:38<3:14:48, 13.00s/it]  2%|▏         | 22/920 [04:51<3:13:38, 12.94s/it]  2%|▎         | 23/920 [05:04<3:12:51, 12.90s/it]  3%|▎         | 24/920 [05:17<3:13:09, 12.93s/it]  3%|▎         | 25/920 [05:30<3:14:36, 13.05s/it]  3%|▎         | 26/920 [05:43<3:14:53, 13.08s/it]  3%|▎         | 27/920 [05:57<3:15:31, 13.14s/it]  3%|▎         | 28/920 [06:10<3:18:00, 13.32s/it]Traceback (most recent call last):
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 133, in <module>
    main(model_id=args.model_id, dataset_path=args.dataset_paths, verbose=args.verbose,
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 114, in main
    trainer.train()
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 361, in train
    output = super().train(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 3238, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 3264, in compute_loss
    outputs = model(**inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 817, in forward
    return model_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 805, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1152, in forward
    logits = self.lm_head(hidden_states)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 9.69 MiB is free. Process 3287960 has 674.00 MiB memory in use. Process 3294938 has 596.00 MiB memory in use. Including non-PyTorch memory, this process has 22.09 GiB memory in use. Of the allocated memory 17.99 GiB is allocated by PyTorch, and 3.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  3%|▎         | 28/920 [06:22<3:23:15, 13.67s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Instruction dataset size:459
Traceback (most recent call last):
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 134, in <module>
    main(model_id=args.model_id, dataset_path=args.dataset_paths, verbose=args.verbose,
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 47, in main
    original_model = AutoModelForCausalLM.from_pretrained(model_id,
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    config = cls._autoset_attn_implementation(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1469, in _autoset_attn_implementation
    cls._check_and_enable_flash_attn_2(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1571, in _check_and_enable_flash_attn_2
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Instruction dataset size:459
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  5.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.97s/it]
Loaded lora weight from: cp-all_textbook/
  0%|          | 0/920 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/920 [00:14<3:45:21, 14.71s/it]  0%|          | 2/920 [00:27<3:30:09, 13.74s/it]  0%|          | 3/920 [00:40<3:24:54, 13.41s/it]  0%|          | 4/920 [00:54<3:25:48, 13.48s/it]  1%|          | 5/920 [01:07<3:23:56, 13.37s/it]  1%|          | 6/920 [01:20<3:23:57, 13.39s/it]  1%|          | 7/920 [01:33<3:19:05, 13.08s/it]  1%|          | 8/920 [01:46<3:19:58, 13.16s/it]Traceback (most recent call last):
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 134, in <module>
    main(model_id=args.model_id, dataset_path=args.dataset_paths, verbose=args.verbose,
  File "/home/namwoam/dl-final/finetune/./instruction-tuning.py", line 115, in main
    trainer.train()
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 361, in train
    output = super().train(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 3238, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/trainer.py", line 3264, in compute_loss
    outputs = model(**inputs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 817, in forward
    return model_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 805, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1152, in forward
    logits = self.lm_head(hidden_states)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 9.69 MiB is free. Process 3287960 has 674.00 MiB memory in use. Process 3294938 has 596.00 MiB memory in use. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 17.97 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  1%|          | 8/920 [01:48<3:26:45, 13.60s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Instruction dataset size:459
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:50<02:31, 50.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:33<01:32, 46.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:40<00:28, 28.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 18.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.08s/it]
Loaded lora weight from: cp-all_textbook/
  0%|          | 0/920 [00:00<?, ?it/s]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/920 [00:13<3:33:21, 13.93s/it]  0%|          | 2/920 [00:24<3:05:50, 12.15s/it]  0%|          | 3/920 [00:35<2:55:31, 11.48s/it]  0%|          | 4/920 [00:45<2:48:59, 11.07s/it]  1%|          | 5/920 [00:56<2:45:00, 10.82s/it]  1%|          | 6/920 [01:06<2:43:31, 10.73s/it]  1%|          | 7/920 [01:17<2:42:25, 10.67s/it]  1%|          | 8/920 [01:28<2:41:39, 10.63s/it]  1%|          | 9/920 [01:38<2:40:48, 10.59s/it]  1%|          | 10/920 [01:49<2:40:53, 10.61s/it]  1%|          | 11/920 [01:59<2:40:45, 10.61s/it]  1%|▏         | 12/920 [02:10<2:39:55, 10.57s/it]  1%|▏         | 13/920 [02:20<2:39:15, 10.54s/it]  2%|▏         | 14/920 [02:31<2:39:07, 10.54s/it]  2%|▏         | 15/920 [02:41<2:38:34, 10.51s/it]  2%|▏         | 16/920 [02:52<2:37:37, 10.46s/it]  2%|▏         | 17/920 [03:02<2:37:17, 10.45s/it]  2%|▏         | 18/920 [03:13<2:37:30, 10.48s/it]  2%|▏         | 19/920 [03:23<2:37:33, 10.49s/it]  2%|▏         | 20/920 [03:34<2:37:45, 10.52s/it]  2%|▏         | 21/920 [03:44<2:37:47, 10.53s/it]  2%|▏         | 22/920 [03:55<2:37:43, 10.54s/it]  2%|▎         | 23/920 [04:05<2:37:34, 10.54s/it]  3%|▎         | 24/920 [04:16<2:37:32, 10.55s/it]  3%|▎         | 25/920 [04:26<2:37:26, 10.55s/it]  3%|▎         | 26/920 [04:37<2:36:55, 10.53s/it]  3%|▎         | 27/920 [04:47<2:36:39, 10.53s/it]  3%|▎         | 28/920 [04:58<2:36:16, 10.51s/it]  3%|▎         | 29/920 [05:09<2:36:48, 10.56s/it]  3%|▎         | 30/920 [05:19<2:36:21, 10.54s/it]  3%|▎         | 31/920 [05:30<2:36:58, 10.59s/it]  3%|▎         | 32/920 [05:40<2:36:29, 10.57s/it]  4%|▎         | 33/920 [05:51<2:36:19, 10.57s/it]  4%|▎         | 34/920 [06:01<2:36:18, 10.58s/it]  4%|▍         | 35/920 [06:12<2:36:14, 10.59s/it]  4%|▍         | 36/920 [06:23<2:35:51, 10.58s/it]  4%|▍         | 37/920 [06:33<2:35:45, 10.58s/it]  4%|▍         | 38/920 [06:44<2:35:33, 10.58s/it]  4%|▍         | 39/920 [06:54<2:34:40, 10.53s/it]  4%|▍         | 40/920 [07:05<2:34:27, 10.53s/it]  4%|▍         | 41/920 [07:15<2:34:34, 10.55s/it]  5%|▍         | 42/920 [07:26<2:34:26, 10.55s/it]  5%|▍         | 43/920 [07:36<2:34:09, 10.55s/it]  5%|▍         | 44/920 [07:47<2:33:05, 10.49s/it]  5%|▍         | 45/920 [07:57<2:33:07, 10.50s/it]  5%|▌         | 46/920 [08:08<2:33:10, 10.52s/it]  5%|▌         | 47/920 [08:18<2:32:28, 10.48s/it]  5%|▌         | 48/920 [08:29<2:32:42, 10.51s/it]  5%|▌         | 49/920 [08:39<2:31:30, 10.44s/it]  5%|▌         | 50/920 [08:50<2:31:30, 10.45s/it]  6%|▌         | 51/920 [09:00<2:31:33, 10.46s/it]  6%|▌         | 52/920 [09:10<2:30:53, 10.43s/it]  6%|▌         | 53/920 [09:21<2:31:08, 10.46s/it]  6%|▌         | 54/920 [09:31<2:31:15, 10.48s/it]  6%|▌         | 55/920 [09:42<2:31:17, 10.49s/it]  6%|▌         | 56/920 [09:53<2:31:20, 10.51s/it]  6%|▌         | 57/920 [10:03<2:30:50, 10.49s/it]  6%|▋         | 58/920 [10:13<2:30:29, 10.47s/it]  6%|▋         | 59/920 [10:24<2:30:34, 10.49s/it]  7%|▋         | 60/920 [10:35<2:30:50, 10.52s/it]  7%|▋         | 61/920 [10:45<2:30:37, 10.52s/it]  7%|▋         | 62/920 [10:56<2:30:39, 10.54s/it]  7%|▋         | 63/920 [11:06<2:30:50, 10.56s/it]  7%|▋         | 64/920 [11:17<2:30:35, 10.56s/it]  7%|▋         | 65/920 [11:27<2:30:09, 10.54s/it]  7%|▋         | 66/920 [11:38<2:30:14, 10.56s/it]  7%|▋         | 67/920 [11:49<2:30:15, 10.57s/it]  7%|▋         | 68/920 [11:59<2:29:56, 10.56s/it]  8%|▊         | 69/920 [12:10<2:29:19, 10.53s/it]  8%|▊         | 70/920 [12:20<2:29:08, 10.53s/it]  8%|▊         | 71/920 [12:31<2:29:10, 10.54s/it]  8%|▊         | 72/920 [12:41<2:28:37, 10.52s/it]  8%|▊         | 73/920 [12:52<2:28:47, 10.54s/it]  8%|▊         | 74/920 [13:02<2:28:50, 10.56s/it]  8%|▊         | 75/920 [13:13<2:28:31, 10.55s/it]  8%|▊         | 76/920 [13:23<2:28:37, 10.57s/it]  8%|▊         | 77/920 [13:34<2:28:52, 10.60s/it]  8%|▊         | 78/920 [13:45<2:28:28, 10.58s/it]  9%|▊         | 79/920 [13:55<2:28:31, 10.60s/it]  9%|▊         | 80/920 [14:06<2:28:01, 10.57s/it]  9%|▉         | 81/920 [14:16<2:27:38, 10.56s/it]  9%|▉         | 82/920 [14:27<2:27:20, 10.55s/it]  9%|▉         | 83/920 [14:37<2:27:11, 10.55s/it]  9%|▉         | 84/920 [14:48<2:26:02, 10.48s/it]  9%|▉         | 85/920 [14:58<2:25:21, 10.45s/it]  9%|▉         | 86/920 [15:09<2:25:35, 10.47s/it]  9%|▉         | 87/920 [15:19<2:26:06, 10.52s/it] 10%|▉         | 88/920 [15:30<2:26:12, 10.54s/it] 10%|▉         | 89/920 [15:40<2:26:32, 10.58s/it] 10%|▉         | 90/920 [15:51<2:26:35, 10.60s/it] 10%|▉         | 91/920 [16:02<2:26:24, 10.60s/it] 10%|█         | 92/920 [16:12<2:25:58, 10.58s/it] 10%|█         | 93/920 [16:23<2:25:35, 10.56s/it] 10%|█         | 94/920 [16:33<2:25:18, 10.55s/it] 10%|█         | 95/920 [16:44<2:24:54, 10.54s/it] 10%|█         | 96/920 [16:54<2:24:19, 10.51s/it] 11%|█         | 97/920 [17:05<2:24:05, 10.51s/it] 11%|█         | 98/920 [17:15<2:24:17, 10.53s/it] 11%|█         | 99/920 [17:26<2:24:09, 10.54s/it] 11%|█         | 100/920 [17:36<2:23:58, 10.53s/it]                                                    11%|█         | 100/920 [17:36<2:23:58, 10.53s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 11%|█         | 101/920 [17:49<2:30:32, 11.03s/it] 11%|█         | 102/920 [17:59<2:27:55, 10.85s/it] 11%|█         | 103/920 [18:10<2:26:46, 10.78s/it] 11%|█▏        | 104/920 [18:20<2:26:09, 10.75s/it] 11%|█▏        | 105/920 [18:31<2:25:05, 10.68s/it] 12%|█▏        | 106/920 [18:42<2:24:54, 10.68s/it] 12%|█▏        | 107/920 [18:52<2:24:05, 10.63s/it] 12%|█▏        | 108/920 [19:03<2:23:39, 10.61s/it] 12%|█▏        | 109/920 [19:13<2:23:24, 10.61s/it] 12%|█▏        | 110/920 [19:24<2:22:30, 10.56s/it] 12%|█▏        | 111/920 [19:34<2:22:27, 10.57s/it] 12%|█▏        | 112/920 [19:45<2:22:35, 10.59s/it] 12%|█▏        | 113/920 [19:55<2:21:34, 10.53s/it] 12%|█▏        | 114/920 [20:06<2:21:21, 10.52s/it] 12%|█▎        | 115/920 [20:16<2:21:27, 10.54s/it] 13%|█▎        | 116/920 [20:27<2:21:34, 10.57s/it] 13%|█▎        | 117/920 [20:38<2:21:39, 10.58s/it] 13%|█▎        | 118/920 [20:48<2:21:21, 10.58s/it] 13%|█▎        | 119/920 [20:59<2:20:39, 10.54s/it] 13%|█▎        | 120/920 [21:09<2:20:55, 10.57s/it] 13%|█▎        | 121/920 [21:20<2:20:57, 10.58s/it] 13%|█▎        | 122/920 [21:30<2:20:54, 10.59s/it] 13%|█▎        | 123/920 [21:41<2:20:50, 10.60s/it] 13%|█▎        | 124/920 [21:52<2:20:45, 10.61s/it] 14%|█▎        | 125/920 [22:02<2:20:00, 10.57s/it] 14%|█▎        | 126/920 [22:13<2:19:33, 10.55s/it] 14%|█▍        | 127/920 [22:23<2:19:40, 10.57s/it] 14%|█▍        | 128/920 [22:34<2:19:52, 10.60s/it] 14%|█▍        | 129/920 [22:45<2:19:42, 10.60s/it] 14%|█▍        | 130/920 [22:55<2:19:03, 10.56s/it] 14%|█▍        | 131/920 [23:06<2:19:04, 10.58s/it] 14%|█▍        | 132/920 [23:16<2:19:02, 10.59s/it] 14%|█▍        | 133/920 [23:27<2:18:59, 10.60s/it] 15%|█▍        | 134/920 [23:38<2:18:54, 10.60s/it] 15%|█▍        | 135/920 [23:48<2:18:31, 10.59s/it] 15%|█▍        | 136/920 [23:59<2:18:03, 10.57s/it] 15%|█▍        | 137/920 [24:09<2:17:54, 10.57s/it] 15%|█▌        | 138/920 [24:20<2:17:57, 10.59s/it] 15%|█▌        | 139/920 [24:30<2:18:08, 10.61s/it] 15%|█▌        | 140/920 [24:41<2:17:36, 10.58s/it] 15%|█▌        | 141/920 [24:52<2:17:45, 10.61s/it] 15%|█▌        | 142/920 [25:02<2:17:29, 10.60s/it] 16%|█▌        | 143/920 [25:13<2:17:09, 10.59s/it] 16%|█▌        | 144/920 [25:23<2:17:07, 10.60s/it] 16%|█▌        | 145/920 [25:34<2:16:51, 10.60s/it] 16%|█▌        | 146/920 [25:45<2:16:48, 10.61s/it] 16%|█▌        | 147/920 [25:55<2:15:37, 10.53s/it] 16%|█▌        | 148/920 [26:05<2:15:07, 10.50s/it] 16%|█▌        | 149/920 [26:16<2:15:27, 10.54s/it] 16%|█▋        | 150/920 [26:27<2:15:18, 10.54s/it] 16%|█▋        | 151/920 [26:37<2:15:20, 10.56s/it] 17%|█▋        | 152/920 [26:48<2:15:14, 10.57s/it] 17%|█▋        | 153/920 [26:58<2:15:21, 10.59s/it] 17%|█▋        | 154/920 [27:09<2:14:55, 10.57s/it] 17%|█▋        | 155/920 [27:20<2:15:03, 10.59s/it] 17%|█▋        | 156/920 [27:30<2:14:58, 10.60s/it] 17%|█▋        | 157/920 [27:41<2:14:47, 10.60s/it] 17%|█▋        | 158/920 [27:51<2:14:50, 10.62s/it] 17%|█▋        | 159/920 [28:02<2:14:30, 10.60s/it] 17%|█▋        | 160/920 [28:13<2:14:01, 10.58s/it] 18%|█▊        | 161/920 [28:23<2:13:46, 10.57s/it] 18%|█▊        | 162/920 [28:34<2:13:43, 10.59s/it] 18%|█▊        | 163/920 [28:44<2:13:33, 10.59s/it] 18%|█▊        | 164/920 [28:55<2:13:33, 10.60s/it] 18%|█▊        | 165/920 [29:06<2:13:21, 10.60s/it] 18%|█▊        | 166/920 [29:16<2:13:18, 10.61s/it] 18%|█▊        | 167/920 [29:27<2:13:06, 10.61s/it] 18%|█▊        | 168/920 [29:37<2:13:05, 10.62s/it] 18%|█▊        | 169/920 [29:48<2:12:55, 10.62s/it] 18%|█▊        | 170/920 [29:59<2:12:52, 10.63s/it] 19%|█▊        | 171/920 [30:09<2:12:26, 10.61s/it] 19%|█▊        | 172/920 [30:20<2:12:09, 10.60s/it] 19%|█▉        | 173/920 [30:31<2:12:45, 10.66s/it] 19%|█▉        | 174/920 [30:41<2:12:55, 10.69s/it] 19%|█▉        | 175/920 [30:52<2:12:22, 10.66s/it] 19%|█▉        | 176/920 [31:03<2:11:53, 10.64s/it] 19%|█▉        | 177/920 [31:13<2:11:44, 10.64s/it] 19%|█▉        | 178/920 [31:24<2:12:02, 10.68s/it] 19%|█▉        | 179/920 [31:35<2:11:13, 10.63s/it] 20%|█▉        | 180/920 [31:45<2:10:13, 10.56s/it] 20%|█▉        | 181/920 [31:56<2:10:39, 10.61s/it] 20%|█▉        | 182/920 [32:06<2:10:46, 10.63s/it] 20%|█▉        | 183/920 [32:17<2:10:23, 10.62s/it] 20%|██        | 184/920 [32:28<2:10:18, 10.62s/it] 20%|██        | 185/920 [32:38<2:09:38, 10.58s/it] 20%|██        | 186/920 [32:49<2:09:40, 10.60s/it] 20%|██        | 187/920 [32:59<2:09:44, 10.62s/it] 20%|██        | 188/920 [33:10<2:09:37, 10.62s/it] 21%|██        | 189/920 [33:21<2:09:18, 10.61s/it] 21%|██        | 190/920 [33:31<2:09:07, 10.61s/it] 21%|██        | 191/920 [33:42<2:09:02, 10.62s/it] 21%|██        | 192/920 [33:52<2:08:44, 10.61s/it] 21%|██        | 193/920 [34:03<2:08:53, 10.64s/it] 21%|██        | 194/920 [34:14<2:08:37, 10.63s/it] 21%|██        | 195/920 [34:24<2:08:44, 10.65s/it] 21%|██▏       | 196/920 [34:35<2:08:14, 10.63s/it] 21%|██▏       | 197/920 [34:46<2:08:10, 10.64s/it] 22%|██▏       | 198/920 [34:56<2:08:10, 10.65s/it] 22%|██▏       | 199/920 [35:07<2:07:56, 10.65s/it] 22%|██▏       | 200/920 [35:18<2:08:02, 10.67s/it]                                                    22%|██▏       | 200/920 [35:18<2:08:02, 10.67s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 22%|██▏       | 201/920 [35:30<2:12:35, 11.06s/it] 22%|██▏       | 202/920 [35:40<2:11:05, 10.95s/it] 22%|██▏       | 203/920 [35:51<2:09:48, 10.86s/it] 22%|██▏       | 204/920 [36:02<2:08:51, 10.80s/it] 22%|██▏       | 205/920 [36:12<2:08:15, 10.76s/it] 22%|██▏       | 206/920 [36:23<2:07:49, 10.74s/it] 22%|██▎       | 207/920 [36:34<2:07:20, 10.72s/it] 23%|██▎       | 208/920 [36:44<2:07:14, 10.72s/it] 23%|██▎       | 209/920 [36:55<2:06:00, 10.63s/it] 23%|██▎       | 210/920 [37:05<2:05:00, 10.56s/it] 23%|██▎       | 211/920 [37:16<2:05:41, 10.64s/it] 23%|██▎       | 212/920 [37:27<2:05:35, 10.64s/it] 23%|██▎       | 213/920 [37:37<2:05:47, 10.68s/it] 23%|██▎       | 214/920 [37:48<2:05:33, 10.67s/it] 23%|██▎       | 215/920 [37:59<2:05:11, 10.65s/it] 23%|██▎       | 216/920 [38:09<2:05:05, 10.66s/it] 24%|██▎       | 217/920 [38:20<2:04:50, 10.66s/it] 24%|██▎       | 218/920 [38:31<2:04:37, 10.65s/it] 24%|██▍       | 219/920 [38:41<2:04:33, 10.66s/it] 24%|██▍       | 220/920 [38:52<2:04:28, 10.67s/it] 24%|██▍       | 221/920 [39:03<2:04:36, 10.70s/it] 24%|██▍       | 222/920 [39:14<2:04:23, 10.69s/it] 24%|██▍       | 223/920 [39:24<2:04:02, 10.68s/it] 24%|██▍       | 224/920 [39:35<2:03:55, 10.68s/it] 24%|██▍       | 225/920 [39:46<2:03:36, 10.67s/it] 25%|██▍       | 226/920 [39:56<2:03:17, 10.66s/it] 25%|██▍       | 227/920 [40:07<2:03:17, 10.67s/it] 25%|██▍       | 228/920 [40:17<2:02:57, 10.66s/it] 25%|██▍       | 229/920 [40:28<2:03:02, 10.68s/it] 25%|██▌       | 230/920 [40:39<2:03:00, 10.70s/it] 25%|██▌       | 231/920 [40:50<2:02:40, 10.68s/it] 25%|██▌       | 232/920 [41:00<2:02:22, 10.67s/it] 25%|██▌       | 233/920 [41:11<2:02:10, 10.67s/it] 25%|██▌       | 234/920 [41:22<2:01:51, 10.66s/it] 26%|██▌       | 235/920 [41:32<2:01:15, 10.62s/it] 26%|██▌       | 236/920 [41:43<2:00:56, 10.61s/it] 26%|██▌       | 237/920 [41:53<2:00:53, 10.62s/it] 26%|██▌       | 238/920 [42:04<2:00:44, 10.62s/it] 26%|██▌       | 239/920 [42:15<2:00:43, 10.64s/it] 26%|██▌       | 240/920 [42:25<2:00:28, 10.63s/it] 26%|██▌       | 241/920 [42:36<1:59:54, 10.60s/it] 26%|██▋       | 242/920 [42:46<2:00:07, 10.63s/it] 26%|██▋       | 243/920 [42:57<2:00:13, 10.65s/it] 27%|██▋       | 244/920 [43:08<1:59:25, 10.60s/it] 27%|██▋       | 245/920 [43:18<1:59:24, 10.61s/it] 27%|██▋       | 246/920 [43:29<1:59:28, 10.64s/it] 27%|██▋       | 247/920 [43:40<1:59:38, 10.67s/it] 27%|██▋       | 248/920 [43:50<1:59:37, 10.68s/it] 27%|██▋       | 249/920 [44:01<1:59:19, 10.67s/it] 27%|██▋       | 250/920 [44:12<1:59:17, 10.68s/it] 27%|██▋       | 251/920 [44:22<1:58:29, 10.63s/it] 27%|██▋       | 252/920 [44:33<1:58:31, 10.65s/it] 28%|██▊       | 253/920 [44:44<1:58:38, 10.67s/it] 28%|██▊       | 254/920 [44:54<1:58:00, 10.63s/it] 28%|██▊       | 255/920 [45:05<1:57:16, 10.58s/it] 28%|██▊       | 256/920 [45:15<1:57:11, 10.59s/it] 28%|██▊       | 257/920 [45:26<1:57:19, 10.62s/it] 28%|██▊       | 258/920 [45:36<1:56:41, 10.58s/it] 28%|██▊       | 259/920 [45:47<1:56:49, 10.60s/it] 28%|██▊       | 260/920 [45:58<1:56:51, 10.62s/it] 28%|██▊       | 261/920 [46:09<1:57:01, 10.65s/it] 28%|██▊       | 262/920 [46:19<1:56:50, 10.65s/it] 29%|██▊       | 263/920 [46:30<1:56:12, 10.61s/it] 29%|██▊       | 264/920 [46:40<1:56:16, 10.64s/it] 29%|██▉       | 265/920 [46:51<1:56:20, 10.66s/it] 29%|██▉       | 266/920 [47:02<1:56:12, 10.66s/it] 29%|██▉       | 267/920 [47:13<1:56:20, 10.69s/it] 29%|██▉       | 268/920 [47:23<1:56:05, 10.68s/it] 29%|██▉       | 269/920 [47:34<1:56:06, 10.70s/it] 29%|██▉       | 270/920 [47:45<1:55:56, 10.70s/it] 29%|██▉       | 271/920 [47:55<1:55:20, 10.66s/it] 30%|██▉       | 272/920 [48:06<1:54:21, 10.59s/it] 30%|██▉       | 273/920 [48:16<1:54:13, 10.59s/it] 30%|██▉       | 274/920 [48:27<1:54:00, 10.59s/it] 30%|██▉       | 275/920 [48:38<1:54:22, 10.64s/it] 30%|███       | 276/920 [48:48<1:54:21, 10.65s/it] 30%|███       | 277/920 [48:59<1:54:05, 10.65s/it] 30%|███       | 278/920 [49:10<1:54:06, 10.67s/it] 30%|███       | 279/920 [49:20<1:54:11, 10.69s/it] 30%|███       | 280/920 [49:31<1:54:07, 10.70s/it] 31%|███       | 281/920 [49:42<1:54:03, 10.71s/it] 31%|███       | 282/920 [49:53<1:54:18, 10.75s/it] 31%|███       | 283/920 [50:03<1:53:51, 10.72s/it] 31%|███       | 284/920 [50:14<1:53:42, 10.73s/it] 31%|███       | 285/920 [50:25<1:53:23, 10.71s/it] 31%|███       | 286/920 [50:35<1:53:23, 10.73s/it] 31%|███       | 287/920 [50:46<1:53:15, 10.73s/it] 31%|███▏      | 288/920 [50:57<1:52:56, 10.72s/it] 31%|███▏      | 289/920 [51:08<1:52:39, 10.71s/it] 32%|███▏      | 290/920 [51:18<1:52:25, 10.71s/it] 32%|███▏      | 291/920 [51:29<1:52:28, 10.73s/it] 32%|███▏      | 292/920 [51:40<1:52:16, 10.73s/it] 32%|███▏      | 293/920 [51:50<1:51:43, 10.69s/it] 32%|███▏      | 294/920 [52:01<1:51:11, 10.66s/it] 32%|███▏      | 295/920 [52:12<1:51:15, 10.68s/it] 32%|███▏      | 296/920 [52:23<1:51:27, 10.72s/it] 32%|███▏      | 297/920 [52:33<1:51:05, 10.70s/it] 32%|███▏      | 298/920 [52:44<1:50:56, 10.70s/it] 32%|███▎      | 299/920 [52:55<1:50:44, 10.70s/it] 33%|███▎      | 300/920 [53:05<1:50:33, 10.70s/it]                                                    33%|███▎      | 300/920 [53:05<1:50:33, 10.70s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 33%|███▎      | 301/920 [53:17<1:55:02, 11.15s/it] 33%|███▎      | 302/920 [53:28<1:53:41, 11.04s/it] 33%|███▎      | 303/920 [53:39<1:52:43, 10.96s/it] 33%|███▎      | 304/920 [53:50<1:51:04, 10.82s/it] 33%|███▎      | 305/920 [54:00<1:50:23, 10.77s/it] 33%|███▎      | 306/920 [54:11<1:50:12, 10.77s/it] 33%|███▎      | 307/920 [54:22<1:50:07, 10.78s/it] 33%|███▎      | 308/920 [54:32<1:49:36, 10.75s/it] 34%|███▎      | 309/920 [54:43<1:49:19, 10.74s/it] 34%|███▎      | 310/920 [54:54<1:49:19, 10.75s/it] 34%|███▍      | 311/920 [55:05<1:49:00, 10.74s/it] 34%|███▍      | 312/920 [55:15<1:48:22, 10.70s/it] 34%|███▍      | 313/920 [55:26<1:48:13, 10.70s/it] 34%|███▍      | 314/920 [55:37<1:48:00, 10.69s/it] 34%|███▍      | 315/920 [55:47<1:47:57, 10.71s/it] 34%|███▍      | 316/920 [55:58<1:47:50, 10.71s/it] 34%|███▍      | 317/920 [56:09<1:47:34, 10.70s/it] 35%|███▍      | 318/920 [56:19<1:47:24, 10.71s/it] 35%|███▍      | 319/920 [56:30<1:47:06, 10.69s/it] 35%|███▍      | 320/920 [56:41<1:46:49, 10.68s/it] 35%|███▍      | 321/920 [56:51<1:45:55, 10.61s/it] 35%|███▌      | 322/920 [57:02<1:45:23, 10.57s/it] 35%|███▌      | 323/920 [57:12<1:44:52, 10.54s/it] 35%|███▌      | 324/920 [57:23<1:44:57, 10.57s/it] 35%|███▌      | 325/920 [57:34<1:45:16, 10.62s/it] 35%|███▌      | 326/920 [57:44<1:44:48, 10.59s/it] 36%|███▌      | 327/920 [57:55<1:45:00, 10.63s/it] 36%|███▌      | 328/920 [58:06<1:45:08, 10.66s/it] 36%|███▌      | 329/920 [58:16<1:45:05, 10.67s/it] 36%|███▌      | 330/920 [58:27<1:44:15, 10.60s/it] 36%|███▌      | 331/920 [58:37<1:44:16, 10.62s/it] 36%|███▌      | 332/920 [58:48<1:44:04, 10.62s/it] 36%|███▌      | 333/920 [58:59<1:44:09, 10.65s/it] 36%|███▋      | 334/920 [59:09<1:44:12, 10.67s/it] 36%|███▋      | 335/920 [59:20<1:43:59, 10.67s/it] 37%|███▋      | 336/920 [59:31<1:44:19, 10.72s/it] 37%|███▋      | 337/920 [59:42<1:44:05, 10.71s/it] 37%|███▋      | 338/920 [59:52<1:43:53, 10.71s/it] 37%|███▋      | 339/920 [1:00:03<1:43:41, 10.71s/it] 37%|███▋      | 340/920 [1:00:14<1:43:38, 10.72s/it] 37%|███▋      | 341/920 [1:00:24<1:42:50, 10.66s/it] 37%|███▋      | 342/920 [1:00:35<1:42:55, 10.69s/it] 37%|███▋      | 343/920 [1:00:46<1:42:48, 10.69s/it] 37%|███▋      | 344/920 [1:00:57<1:43:07, 10.74s/it] 38%|███▊      | 345/920 [1:01:07<1:42:57, 10.74s/it] 38%|███▊      | 346/920 [1:01:18<1:42:42, 10.74s/it] 38%|███▊      | 347/920 [1:01:29<1:42:31, 10.74s/it] 38%|███▊      | 348/920 [1:01:40<1:42:24, 10.74s/it] 38%|███▊      | 349/920 [1:01:50<1:41:56, 10.71s/it] 38%|███▊      | 350/920 [1:02:01<1:41:55, 10.73s/it] 38%|███▊      | 351/920 [1:02:12<1:41:18, 10.68s/it] 38%|███▊      | 352/920 [1:02:22<1:41:21, 10.71s/it] 38%|███▊      | 353/920 [1:02:33<1:41:11, 10.71s/it] 38%|███▊      | 354/920 [1:02:44<1:41:02, 10.71s/it] 39%|███▊      | 355/920 [1:02:54<1:40:50, 10.71s/it] 39%|███▊      | 356/920 [1:03:05<1:40:24, 10.68s/it] 39%|███▉      | 357/920 [1:03:16<1:40:20, 10.69s/it] 39%|███▉      | 358/920 [1:03:26<1:39:54, 10.67s/it] 39%|███▉      | 359/920 [1:03:37<1:39:38, 10.66s/it] 39%|███▉      | 360/920 [1:03:48<1:39:26, 10.65s/it] 39%|███▉      | 361/920 [1:03:58<1:39:31, 10.68s/it] 39%|███▉      | 362/920 [1:04:09<1:39:27, 10.69s/it] 39%|███▉      | 363/920 [1:04:20<1:38:55, 10.66s/it] 40%|███▉      | 364/920 [1:04:30<1:38:52, 10.67s/it] 40%|███▉      | 365/920 [1:04:41<1:38:45, 10.68s/it] 40%|███▉      | 366/920 [1:04:52<1:38:34, 10.68s/it] 40%|███▉      | 367/920 [1:05:02<1:38:19, 10.67s/it] 40%|████      | 368/920 [1:05:13<1:38:11, 10.67s/it] 40%|████      | 369/920 [1:05:24<1:38:06, 10.68s/it] 40%|████      | 370/920 [1:05:34<1:37:58, 10.69s/it] 40%|████      | 371/920 [1:05:45<1:37:50, 10.69s/it] 40%|████      | 372/920 [1:05:56<1:37:26, 10.67s/it] 41%|████      | 373/920 [1:06:07<1:37:38, 10.71s/it] 41%|████      | 374/920 [1:06:17<1:36:59, 10.66s/it] 41%|████      | 375/920 [1:06:28<1:36:40, 10.64s/it] 41%|████      | 376/920 [1:06:39<1:37:00, 10.70s/it] 41%|████      | 377/920 [1:06:49<1:36:51, 10.70s/it] 41%|████      | 378/920 [1:07:00<1:36:43, 10.71s/it] 41%|████      | 379/920 [1:07:11<1:36:29, 10.70s/it] 41%|████▏     | 380/920 [1:07:21<1:36:30, 10.72s/it] 41%|████▏     | 381/920 [1:07:32<1:36:24, 10.73s/it] 42%|████▏     | 382/920 [1:07:43<1:36:25, 10.75s/it] 42%|████▏     | 383/920 [1:07:54<1:36:09, 10.74s/it] 42%|████▏     | 384/920 [1:08:05<1:36:13, 10.77s/it] 42%|████▏     | 385/920 [1:08:15<1:35:51, 10.75s/it] 42%|████▏     | 386/920 [1:08:26<1:35:58, 10.78s/it] 42%|████▏     | 387/920 [1:08:37<1:35:41, 10.77s/it] 42%|████▏     | 388/920 [1:08:48<1:35:24, 10.76s/it] 42%|████▏     | 389/920 [1:08:58<1:35:06, 10.75s/it] 42%|████▏     | 390/920 [1:09:09<1:34:53, 10.74s/it] 42%|████▎     | 391/920 [1:09:20<1:34:44, 10.75s/it] 43%|████▎     | 392/920 [1:09:30<1:34:09, 10.70s/it] 43%|████▎     | 393/920 [1:09:41<1:33:56, 10.70s/it] 43%|████▎     | 394/920 [1:09:52<1:33:51, 10.71s/it] 43%|████▎     | 395/920 [1:10:02<1:33:23, 10.67s/it] 43%|████▎     | 396/920 [1:10:13<1:33:17, 10.68s/it] 43%|████▎     | 397/920 [1:10:24<1:33:03, 10.68s/it] 43%|████▎     | 398/920 [1:10:34<1:32:56, 10.68s/it] 43%|████▎     | 399/920 [1:10:45<1:32:44, 10.68s/it] 43%|████▎     | 400/920 [1:10:56<1:32:34, 10.68s/it]                                                      43%|████▎     | 400/920 [1:10:56<1:32:34, 10.68s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 44%|████▎     | 401/920 [1:11:09<1:37:39, 11.29s/it] 44%|████▎     | 402/920 [1:11:19<1:35:48, 11.10s/it] 44%|████▍     | 403/920 [1:11:30<1:34:47, 11.00s/it] 44%|████▍     | 404/920 [1:11:41<1:33:49, 10.91s/it] 44%|████▍     | 405/920 [1:11:51<1:33:23, 10.88s/it] 44%|████▍     | 406/920 [1:12:02<1:32:54, 10.85s/it] 44%|████▍     | 407/920 [1:12:13<1:32:29, 10.82s/it] 44%|████▍     | 408/920 [1:12:24<1:31:59, 10.78s/it] 44%|████▍     | 409/920 [1:12:34<1:31:31, 10.75s/it] 45%|████▍     | 410/920 [1:12:45<1:31:04, 10.71s/it] 45%|████▍     | 411/920 [1:12:56<1:30:58, 10.72s/it] 45%|████▍     | 412/920 [1:13:06<1:30:46, 10.72s/it] 45%|████▍     | 413/920 [1:13:17<1:30:23, 10.70s/it] 45%|████▌     | 414/920 [1:13:28<1:29:52, 10.66s/it] 45%|████▌     | 415/920 [1:13:38<1:29:39, 10.65s/it] 45%|████▌     | 416/920 [1:13:49<1:29:51, 10.70s/it] 45%|████▌     | 417/920 [1:14:00<1:29:39, 10.70s/it] 45%|████▌     | 418/920 [1:14:10<1:29:10, 10.66s/it] 46%|████▌     | 419/920 [1:14:21<1:28:51, 10.64s/it] 46%|████▌     | 420/920 [1:14:32<1:28:58, 10.68s/it] 46%|████▌     | 421/920 [1:14:43<1:29:01, 10.70s/it] 46%|████▌     | 422/920 [1:14:53<1:28:47, 10.70s/it] 46%|████▌     | 423/920 [1:15:04<1:28:53, 10.73s/it] 46%|████▌     | 424/920 [1:15:15<1:28:38, 10.72s/it] 46%|████▌     | 425/920 [1:15:25<1:28:31, 10.73s/it] 46%|████▋     | 426/920 [1:15:36<1:28:22, 10.73s/it] 46%|████▋     | 427/920 [1:15:47<1:28:08, 10.73s/it] 47%|████▋     | 428/920 [1:15:58<1:28:04, 10.74s/it] 47%|████▋     | 429/920 [1:16:08<1:27:53, 10.74s/it] 47%|████▋     | 430/920 [1:16:19<1:27:41, 10.74s/it] 47%|████▋     | 431/920 [1:16:30<1:27:32, 10.74s/it] 47%|████▋     | 432/920 [1:16:40<1:26:57, 10.69s/it] 47%|████▋     | 433/920 [1:16:51<1:26:56, 10.71s/it] 47%|████▋     | 434/920 [1:17:02<1:26:54, 10.73s/it] 47%|████▋     | 435/920 [1:17:13<1:26:14, 10.67s/it] 47%|████▋     | 436/920 [1:17:23<1:26:15, 10.69s/it] 48%|████▊     | 437/920 [1:17:34<1:26:17, 10.72s/it] 48%|████▊     | 438/920 [1:17:45<1:26:25, 10.76s/it] 48%|████▊     | 439/920 [1:17:56<1:26:02, 10.73s/it] 48%|████▊     | 440/920 [1:18:06<1:26:02, 10.76s/it] 48%|████▊     | 441/920 [1:18:17<1:25:23, 10.70s/it] 48%|████▊     | 442/920 [1:18:28<1:25:10, 10.69s/it] 48%|████▊     | 443/920 [1:18:38<1:25:13, 10.72s/it] 48%|████▊     | 444/920 [1:18:49<1:24:53, 10.70s/it] 48%|████▊     | 445/920 [1:19:00<1:24:46, 10.71s/it] 48%|████▊     | 446/920 [1:19:11<1:24:45, 10.73s/it] 49%|████▊     | 447/920 [1:19:21<1:24:27, 10.71s/it] 49%|████▊     | 448/920 [1:19:32<1:24:20, 10.72s/it] 49%|████▉     | 449/920 [1:19:43<1:24:12, 10.73s/it] 49%|████▉     | 450/920 [1:19:53<1:23:53, 10.71s/it] 49%|████▉     | 451/920 [1:20:04<1:23:40, 10.71s/it] 49%|████▉     | 452/920 [1:20:15<1:23:36, 10.72s/it] 49%|████▉     | 453/920 [1:20:26<1:23:30, 10.73s/it] 49%|████▉     | 454/920 [1:20:36<1:23:16, 10.72s/it] 49%|████▉     | 455/920 [1:20:47<1:23:08, 10.73s/it] 50%|████▉     | 456/920 [1:20:58<1:22:54, 10.72s/it] 50%|████▉     | 457/920 [1:21:08<1:22:29, 10.69s/it] 50%|████▉     | 458/920 [1:21:19<1:22:13, 10.68s/it] 50%|████▉     | 459/920 [1:21:30<1:22:19, 10.72s/it] 50%|█████     | 460/920 [1:21:40<1:21:47, 10.67s/it] 50%|█████     | 461/920 [1:21:51<1:21:34, 10.66s/it] 50%|█████     | 462/920 [1:22:02<1:21:27, 10.67s/it] 50%|█████     | 463/920 [1:22:12<1:21:14, 10.67s/it] 50%|█████     | 464/920 [1:22:23<1:21:06, 10.67s/it] 51%|█████     | 465/920 [1:22:34<1:21:00, 10.68s/it] 51%|█████     | 466/920 [1:22:44<1:20:42, 10.67s/it] 51%|█████     | 467/920 [1:22:55<1:20:38, 10.68s/it] 51%|█████     | 468/920 [1:23:06<1:20:17, 10.66s/it] 51%|█████     | 469/920 [1:23:16<1:20:19, 10.69s/it] 51%|█████     | 470/920 [1:23:27<1:20:09, 10.69s/it] 51%|█████     | 471/920 [1:23:38<1:19:40, 10.65s/it] 51%|█████▏    | 472/920 [1:23:48<1:19:39, 10.67s/it] 51%|█████▏    | 473/920 [1:23:59<1:19:58, 10.73s/it] 52%|█████▏    | 474/920 [1:24:10<1:19:51, 10.74s/it] 52%|█████▏    | 475/920 [1:24:21<1:19:21, 10.70s/it] 52%|█████▏    | 476/920 [1:24:31<1:19:11, 10.70s/it] 52%|█████▏    | 477/920 [1:24:42<1:18:47, 10.67s/it] 52%|█████▏    | 478/920 [1:24:53<1:18:39, 10.68s/it] 52%|█████▏    | 479/920 [1:25:03<1:18:28, 10.68s/it] 52%|█████▏    | 480/920 [1:25:14<1:18:23, 10.69s/it] 52%|█████▏    | 481/920 [1:25:25<1:18:13, 10.69s/it] 52%|█████▏    | 482/920 [1:25:35<1:18:03, 10.69s/it] 52%|█████▎    | 483/920 [1:25:46<1:18:00, 10.71s/it] 53%|█████▎    | 484/920 [1:25:57<1:17:52, 10.72s/it] 53%|█████▎    | 485/920 [1:26:08<1:17:22, 10.67s/it] 53%|█████▎    | 486/920 [1:26:18<1:17:10, 10.67s/it] 53%|█████▎    | 487/920 [1:26:29<1:17:19, 10.72s/it] 53%|█████▎    | 488/920 [1:26:40<1:17:05, 10.71s/it] 53%|█████▎    | 489/920 [1:26:50<1:16:59, 10.72s/it] 53%|█████▎    | 490/920 [1:27:01<1:16:41, 10.70s/it] 53%|█████▎    | 491/920 [1:27:12<1:16:03, 10.64s/it] 53%|█████▎    | 492/920 [1:27:22<1:15:43, 10.62s/it] 54%|█████▎    | 493/920 [1:27:33<1:15:33, 10.62s/it] 54%|█████▎    | 494/920 [1:27:43<1:15:26, 10.63s/it] 54%|█████▍    | 495/920 [1:27:54<1:15:31, 10.66s/it] 54%|█████▍    | 496/920 [1:28:05<1:15:32, 10.69s/it] 54%|█████▍    | 497/920 [1:28:16<1:15:44, 10.74s/it] 54%|█████▍    | 498/920 [1:28:26<1:15:24, 10.72s/it] 54%|█████▍    | 499/920 [1:28:37<1:15:03, 10.70s/it] 54%|█████▍    | 500/920 [1:28:48<1:14:59, 10.71s/it]                                                      54%|█████▍    | 500/920 [1:28:48<1:14:59, 10.71s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 54%|█████▍    | 501/920 [1:29:01<1:19:13, 11.35s/it] 55%|█████▍    | 502/920 [1:29:11<1:17:29, 11.12s/it] 55%|█████▍    | 503/920 [1:29:22<1:16:26, 11.00s/it] 55%|█████▍    | 504/920 [1:29:33<1:15:56, 10.95s/it] 55%|█████▍    | 505/920 [1:29:44<1:15:19, 10.89s/it] 55%|█████▌    | 506/920 [1:29:54<1:14:55, 10.86s/it] 55%|█████▌    | 507/920 [1:30:05<1:14:18, 10.80s/it] 55%|█████▌    | 508/920 [1:30:16<1:13:53, 10.76s/it] 55%|█████▌    | 509/920 [1:30:26<1:13:40, 10.75s/it] 55%|█████▌    | 510/920 [1:30:37<1:13:25, 10.74s/it] 56%|█████▌    | 511/920 [1:30:48<1:13:14, 10.74s/it] 56%|█████▌    | 512/920 [1:30:59<1:13:01, 10.74s/it] 56%|█████▌    | 513/920 [1:31:09<1:12:52, 10.74s/it] 56%|█████▌    | 514/920 [1:31:20<1:12:13, 10.67s/it] 56%|█████▌    | 515/920 [1:31:31<1:12:18, 10.71s/it] 56%|█████▌    | 516/920 [1:31:41<1:12:04, 10.70s/it] 56%|█████▌    | 517/920 [1:31:52<1:11:50, 10.70s/it] 56%|█████▋    | 518/920 [1:32:03<1:11:33, 10.68s/it] 56%|█████▋    | 519/920 [1:32:13<1:11:33, 10.71s/it] 57%|█████▋    | 520/920 [1:32:24<1:11:31, 10.73s/it] 57%|█████▋    | 521/920 [1:32:35<1:11:25, 10.74s/it] 57%|█████▋    | 522/920 [1:32:46<1:11:10, 10.73s/it] 57%|█████▋    | 523/920 [1:32:56<1:10:56, 10.72s/it] 57%|█████▋    | 524/920 [1:33:07<1:10:50, 10.73s/it] 57%|█████▋    | 525/920 [1:33:18<1:10:31, 10.71s/it] 57%|█████▋    | 526/920 [1:33:28<1:10:02, 10.67s/it] 57%|█████▋    | 527/920 [1:33:39<1:10:03, 10.70s/it] 57%|█████▋    | 528/920 [1:33:50<1:09:54, 10.70s/it] 57%|█████▊    | 529/920 [1:34:01<1:09:52, 10.72s/it] 58%|█████▊    | 530/920 [1:34:11<1:09:40, 10.72s/it] 58%|█████▊    | 531/920 [1:34:22<1:09:33, 10.73s/it] 58%|█████▊    | 532/920 [1:34:33<1:09:22, 10.73s/it] 58%|█████▊    | 533/920 [1:34:44<1:09:12, 10.73s/it] 58%|█████▊    | 534/920 [1:34:54<1:09:02, 10.73s/it] 58%|█████▊    | 535/920 [1:35:05<1:08:43, 10.71s/it] 58%|█████▊    | 536/920 [1:35:16<1:08:33, 10.71s/it] 58%|█████▊    | 537/920 [1:35:26<1:08:21, 10.71s/it] 58%|█████▊    | 538/920 [1:35:37<1:08:17, 10.73s/it] 59%|█████▊    | 539/920 [1:35:48<1:07:59, 10.71s/it] 59%|█████▊    | 540/920 [1:35:58<1:07:45, 10.70s/it] 59%|█████▉    | 541/920 [1:36:09<1:07:39, 10.71s/it] 59%|█████▉    | 542/920 [1:36:20<1:07:37, 10.73s/it] 59%|█████▉    | 543/920 [1:36:31<1:07:26, 10.73s/it] 59%|█████▉    | 544/920 [1:36:41<1:07:18, 10.74s/it] 59%|█████▉    | 545/920 [1:36:52<1:07:05, 10.73s/it] 59%|█████▉    | 546/920 [1:37:03<1:07:00, 10.75s/it] 59%|█████▉    | 547/920 [1:37:14<1:06:45, 10.74s/it] 60%|█████▉    | 548/920 [1:37:24<1:06:37, 10.75s/it] 60%|█████▉    | 549/920 [1:37:35<1:06:31, 10.76s/it] 60%|█████▉    | 550/920 [1:37:46<1:06:21, 10.76s/it] 60%|█████▉    | 551/920 [1:37:57<1:06:08, 10.76s/it] 60%|██████    | 552/920 [1:38:07<1:05:53, 10.74s/it] 60%|██████    | 553/920 [1:38:18<1:05:39, 10.73s/it] 60%|██████    | 554/920 [1:38:29<1:05:24, 10.72s/it] 60%|██████    | 555/920 [1:38:40<1:05:13, 10.72s/it] 60%|██████    | 556/920 [1:38:50<1:05:05, 10.73s/it] 61%|██████    | 557/920 [1:39:01<1:04:53, 10.73s/it] 61%|██████    | 558/920 [1:39:12<1:04:45, 10.73s/it] 61%|██████    | 559/920 [1:39:23<1:04:36, 10.74s/it] 61%|██████    | 560/920 [1:39:33<1:04:18, 10.72s/it] 61%|██████    | 561/920 [1:39:44<1:03:55, 10.69s/it] 61%|██████    | 562/920 [1:39:55<1:03:46, 10.69s/it] 61%|██████    | 563/920 [1:40:05<1:03:27, 10.67s/it] 61%|██████▏   | 564/920 [1:40:16<1:03:29, 10.70s/it] 61%|██████▏   | 565/920 [1:40:27<1:03:11, 10.68s/it] 62%|██████▏   | 566/920 [1:40:37<1:03:07, 10.70s/it] 62%|██████▏   | 567/920 [1:40:48<1:02:58, 10.70s/it] 62%|██████▏   | 568/920 [1:40:59<1:02:47, 10.70s/it] 62%|██████▏   | 569/920 [1:41:09<1:02:39, 10.71s/it] 62%|██████▏   | 570/920 [1:41:20<1:02:28, 10.71s/it] 62%|██████▏   | 571/920 [1:41:31<1:02:22, 10.72s/it] 62%|██████▏   | 572/920 [1:41:42<1:02:15, 10.73s/it] 62%|██████▏   | 573/920 [1:41:52<1:02:01, 10.73s/it] 62%|██████▏   | 574/920 [1:42:03<1:01:54, 10.74s/it] 62%|██████▎   | 575/920 [1:42:14<1:01:38, 10.72s/it] 63%|██████▎   | 576/920 [1:42:24<1:01:14, 10.68s/it] 63%|██████▎   | 577/920 [1:42:35<1:01:07, 10.69s/it] 63%|██████▎   | 578/920 [1:42:46<1:01:00, 10.70s/it] 63%|██████▎   | 579/920 [1:42:57<1:01:04, 10.75s/it] 63%|██████▎   | 580/920 [1:43:07<1:00:51, 10.74s/it] 63%|██████▎   | 581/920 [1:43:18<1:00:41, 10.74s/it] 63%|██████▎   | 582/920 [1:43:29<1:00:42, 10.78s/it] 63%|██████▎   | 583/920 [1:43:40<1:00:19, 10.74s/it] 63%|██████▎   | 584/920 [1:43:50<1:00:08, 10.74s/it] 64%|██████▎   | 585/920 [1:44:01<59:59, 10.74s/it]   64%|██████▎   | 586/920 [1:44:12<59:44, 10.73s/it] 64%|██████▍   | 587/920 [1:44:23<59:32, 10.73s/it] 64%|██████▍   | 588/920 [1:44:33<59:05, 10.68s/it] 64%|██████▍   | 589/920 [1:44:44<58:51, 10.67s/it] 64%|██████▍   | 590/920 [1:44:55<58:40, 10.67s/it] 64%|██████▍   | 591/920 [1:45:05<58:40, 10.70s/it] 64%|██████▍   | 592/920 [1:45:16<58:31, 10.71s/it] 64%|██████▍   | 593/920 [1:45:27<58:02, 10.65s/it] 65%|██████▍   | 594/920 [1:45:37<57:57, 10.67s/it] 65%|██████▍   | 595/920 [1:45:48<57:51, 10.68s/it] 65%|██████▍   | 596/920 [1:45:59<57:32, 10.65s/it] 65%|██████▍   | 597/920 [1:46:09<57:27, 10.67s/it] 65%|██████▌   | 598/920 [1:46:20<57:31, 10.72s/it] 65%|██████▌   | 599/920 [1:46:31<57:15, 10.70s/it] 65%|██████▌   | 600/920 [1:46:41<57:06, 10.71s/it]                                                    65%|██████▌   | 600/920 [1:46:41<57:06, 10.71s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 65%|██████▌   | 601/920 [1:46:54<59:08, 11.13s/it] 65%|██████▌   | 602/920 [1:47:04<58:25, 11.02s/it] 66%|██████▌   | 603/920 [1:47:15<57:29, 10.88s/it] 66%|██████▌   | 604/920 [1:47:26<56:59, 10.82s/it] 66%|██████▌   | 605/920 [1:47:36<56:31, 10.77s/it] 66%|██████▌   | 606/920 [1:47:47<56:12, 10.74s/it] 66%|██████▌   | 607/920 [1:47:58<56:01, 10.74s/it] 66%|██████▌   | 608/920 [1:48:08<55:49, 10.74s/it] 66%|██████▌   | 609/920 [1:48:19<55:36, 10.73s/it] 66%|██████▋   | 610/920 [1:48:30<55:24, 10.72s/it] 66%|██████▋   | 611/920 [1:48:41<55:21, 10.75s/it] 67%|██████▋   | 612/920 [1:48:51<55:15, 10.76s/it] 67%|██████▋   | 613/920 [1:49:02<55:04, 10.76s/it] 67%|██████▋   | 614/920 [1:49:13<54:43, 10.73s/it] 67%|██████▋   | 615/920 [1:49:24<54:31, 10.73s/it] 67%|██████▋   | 616/920 [1:49:34<54:20, 10.73s/it] 67%|██████▋   | 617/920 [1:49:45<54:04, 10.71s/it] 67%|██████▋   | 618/920 [1:49:56<53:59, 10.73s/it] 67%|██████▋   | 619/920 [1:50:06<53:49, 10.73s/it] 67%|██████▋   | 620/920 [1:50:17<53:36, 10.72s/it] 68%|██████▊   | 621/920 [1:50:28<53:17, 10.70s/it] 68%|██████▊   | 622/920 [1:50:38<53:06, 10.69s/it] 68%|██████▊   | 623/920 [1:50:49<53:07, 10.73s/it] 68%|██████▊   | 624/920 [1:51:00<52:52, 10.72s/it] 68%|██████▊   | 625/920 [1:51:11<52:28, 10.67s/it] 68%|██████▊   | 626/920 [1:51:21<52:20, 10.68s/it] 68%|██████▊   | 627/920 [1:51:32<52:18, 10.71s/it] 68%|██████▊   | 628/920 [1:51:43<51:54, 10.67s/it] 68%|██████▊   | 629/920 [1:51:53<51:30, 10.62s/it] 68%|██████▊   | 630/920 [1:52:04<51:19, 10.62s/it] 69%|██████▊   | 631/920 [1:52:14<51:08, 10.62s/it] 69%|██████▊   | 632/920 [1:52:25<51:07, 10.65s/it] 69%|██████▉   | 633/920 [1:52:36<51:08, 10.69s/it] 69%|██████▉   | 634/920 [1:52:47<51:03, 10.71s/it] 69%|██████▉   | 635/920 [1:52:57<50:50, 10.70s/it] 69%|██████▉   | 636/920 [1:53:08<50:38, 10.70s/it] 69%|██████▉   | 637/920 [1:53:19<50:29, 10.70s/it] 69%|██████▉   | 638/920 [1:53:29<50:16, 10.70s/it] 69%|██████▉   | 639/920 [1:53:40<50:10, 10.71s/it] 70%|██████▉   | 640/920 [1:53:51<50:07, 10.74s/it] 70%|██████▉   | 641/920 [1:54:02<49:52, 10.73s/it] 70%|██████▉   | 642/920 [1:54:12<49:43, 10.73s/it] 70%|██████▉   | 643/920 [1:54:23<49:31, 10.73s/it] 70%|███████   | 644/920 [1:54:34<49:19, 10.72s/it] 70%|███████   | 645/920 [1:54:45<49:11, 10.73s/it] 70%|███████   | 646/920 [1:54:55<48:37, 10.65s/it] 70%|███████   | 647/920 [1:55:06<48:27, 10.65s/it] 70%|███████   | 648/920 [1:55:16<48:19, 10.66s/it] 71%|███████   | 649/920 [1:55:27<48:02, 10.64s/it] 71%|███████   | 650/920 [1:55:38<47:58, 10.66s/it] 71%|███████   | 651/920 [1:55:48<47:52, 10.68s/it] 71%|███████   | 652/920 [1:55:59<47:42, 10.68s/it] 71%|███████   | 653/920 [1:56:10<47:36, 10.70s/it] 71%|███████   | 654/920 [1:56:20<47:28, 10.71s/it] 71%|███████   | 655/920 [1:56:31<47:19, 10.72s/it] 71%|███████▏  | 656/920 [1:56:42<47:11, 10.73s/it] 71%|███████▏  | 657/920 [1:56:53<47:02, 10.73s/it] 72%|███████▏  | 658/920 [1:57:03<46:54, 10.74s/it] 72%|███████▏  | 659/920 [1:57:14<46:44, 10.74s/it] 72%|███████▏  | 660/920 [1:57:25<46:33, 10.74s/it] 72%|███████▏  | 661/920 [1:57:36<46:28, 10.76s/it] 72%|███████▏  | 662/920 [1:57:47<46:14, 10.75s/it] 72%|███████▏  | 663/920 [1:57:57<46:03, 10.75s/it] 72%|███████▏  | 664/920 [1:58:08<45:49, 10.74s/it] 72%|███████▏  | 665/920 [1:58:19<45:30, 10.71s/it] 72%|███████▏  | 666/920 [1:58:29<45:22, 10.72s/it] 72%|███████▎  | 667/920 [1:58:40<45:09, 10.71s/it] 73%|███████▎  | 668/920 [1:58:51<45:00, 10.72s/it] 73%|███████▎  | 669/920 [1:59:02<44:55, 10.74s/it] 73%|███████▎  | 670/920 [1:59:12<44:42, 10.73s/it] 73%|███████▎  | 671/920 [1:59:23<44:32, 10.73s/it] 73%|███████▎  | 672/920 [1:59:34<44:11, 10.69s/it] 73%|███████▎  | 673/920 [1:59:44<44:02, 10.70s/it] 73%|███████▎  | 674/920 [1:59:55<43:39, 10.65s/it] 73%|███████▎  | 675/920 [2:00:06<43:29, 10.65s/it] 73%|███████▎  | 676/920 [2:00:16<43:23, 10.67s/it] 74%|███████▎  | 677/920 [2:00:27<43:17, 10.69s/it] 74%|███████▎  | 678/920 [2:00:38<43:13, 10.72s/it] 74%|███████▍  | 679/920 [2:00:48<43:04, 10.72s/it] 74%|███████▍  | 680/920 [2:00:59<42:54, 10.73s/it] 74%|███████▍  | 681/920 [2:01:10<42:39, 10.71s/it] 74%|███████▍  | 682/920 [2:01:21<42:24, 10.69s/it] 74%|███████▍  | 683/920 [2:01:31<42:19, 10.71s/it] 74%|███████▍  | 684/920 [2:01:42<42:14, 10.74s/it] 74%|███████▍  | 685/920 [2:01:53<42:03, 10.74s/it] 75%|███████▍  | 686/920 [2:02:04<41:58, 10.76s/it] 75%|███████▍  | 687/920 [2:02:14<41:43, 10.74s/it] 75%|███████▍  | 688/920 [2:02:25<41:32, 10.74s/it] 75%|███████▍  | 689/920 [2:02:36<41:23, 10.75s/it] 75%|███████▌  | 690/920 [2:02:47<41:12, 10.75s/it] 75%|███████▌  | 691/920 [2:02:57<41:08, 10.78s/it] 75%|███████▌  | 692/920 [2:03:08<40:56, 10.77s/it] 75%|███████▌  | 693/920 [2:03:19<40:43, 10.77s/it] 75%|███████▌  | 694/920 [2:03:30<40:33, 10.77s/it] 76%|███████▌  | 695/920 [2:03:40<40:18, 10.75s/it] 76%|███████▌  | 696/920 [2:03:51<40:06, 10.74s/it] 76%|███████▌  | 697/920 [2:04:02<39:52, 10.73s/it] 76%|███████▌  | 698/920 [2:04:13<39:42, 10.73s/it] 76%|███████▌  | 699/920 [2:04:23<39:29, 10.72s/it] 76%|███████▌  | 700/920 [2:04:34<39:19, 10.72s/it]                                                    76%|███████▌  | 700/920 [2:04:34<39:19, 10.72s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 76%|███████▌  | 701/920 [2:04:46<40:41, 11.15s/it] 76%|███████▋  | 702/920 [2:04:57<39:56, 11.00s/it] 76%|███████▋  | 703/920 [2:05:08<39:26, 10.90s/it] 77%|███████▋  | 704/920 [2:05:18<39:01, 10.84s/it] 77%|███████▋  | 705/920 [2:05:29<38:34, 10.77s/it] 77%|███████▋  | 706/920 [2:05:40<38:21, 10.75s/it] 77%|███████▋  | 707/920 [2:05:50<38:03, 10.72s/it] 77%|███████▋  | 708/920 [2:06:01<37:52, 10.72s/it] 77%|███████▋  | 709/920 [2:06:12<37:46, 10.74s/it] 77%|███████▋  | 710/920 [2:06:22<37:27, 10.70s/it] 77%|███████▋  | 711/920 [2:06:33<37:12, 10.68s/it] 77%|███████▋  | 712/920 [2:06:44<37:07, 10.71s/it] 78%|███████▊  | 713/920 [2:06:54<36:58, 10.72s/it] 78%|███████▊  | 714/920 [2:07:05<36:48, 10.72s/it] 78%|███████▊  | 715/920 [2:07:16<36:37, 10.72s/it] 78%|███████▊  | 716/920 [2:07:27<36:25, 10.71s/it] 78%|███████▊  | 717/920 [2:07:37<36:13, 10.71s/it] 78%|███████▊  | 718/920 [2:07:48<36:07, 10.73s/it] 78%|███████▊  | 719/920 [2:07:59<35:56, 10.73s/it] 78%|███████▊  | 720/920 [2:08:10<35:46, 10.73s/it] 78%|███████▊  | 721/920 [2:08:20<35:39, 10.75s/it] 78%|███████▊  | 722/920 [2:08:31<35:26, 10.74s/it] 79%|███████▊  | 723/920 [2:08:42<35:17, 10.75s/it] 79%|███████▊  | 724/920 [2:08:53<35:03, 10.73s/it] 79%|███████▉  | 725/920 [2:09:03<34:54, 10.74s/it] 79%|███████▉  | 726/920 [2:09:14<34:41, 10.73s/it] 79%|███████▉  | 727/920 [2:09:25<34:30, 10.73s/it] 79%|███████▉  | 728/920 [2:09:36<34:25, 10.76s/it] 79%|███████▉  | 729/920 [2:09:46<34:11, 10.74s/it] 79%|███████▉  | 730/920 [2:09:57<34:00, 10.74s/it] 79%|███████▉  | 731/920 [2:10:08<33:49, 10.74s/it] 80%|███████▉  | 732/920 [2:10:18<33:39, 10.74s/it] 80%|███████▉  | 733/920 [2:10:29<33:28, 10.74s/it] 80%|███████▉  | 734/920 [2:10:40<33:11, 10.71s/it] 80%|███████▉  | 735/920 [2:10:51<33:02, 10.72s/it] 80%|████████  | 736/920 [2:11:01<32:54, 10.73s/it] 80%|████████  | 737/920 [2:11:12<32:46, 10.75s/it] 80%|████████  | 738/920 [2:11:23<32:35, 10.74s/it] 80%|████████  | 739/920 [2:11:34<32:22, 10.73s/it] 80%|████████  | 740/920 [2:11:44<32:14, 10.75s/it] 81%|████████  | 741/920 [2:11:55<32:03, 10.74s/it] 81%|████████  | 742/920 [2:12:06<31:49, 10.73s/it] 81%|████████  | 743/920 [2:12:16<31:31, 10.69s/it] 81%|████████  | 744/920 [2:12:27<31:09, 10.62s/it] 81%|████████  | 745/920 [2:12:37<31:02, 10.64s/it] 81%|████████  | 746/920 [2:12:48<30:55, 10.66s/it] 81%|████████  | 747/920 [2:12:59<30:49, 10.69s/it] 81%|████████▏ | 748/920 [2:13:10<30:39, 10.69s/it] 81%|████████▏ | 749/920 [2:13:20<30:28, 10.69s/it] 82%|████████▏ | 750/920 [2:13:31<30:19, 10.70s/it] 82%|████████▏ | 751/920 [2:13:42<30:04, 10.68s/it] 82%|████████▏ | 752/920 [2:13:52<29:57, 10.70s/it] 82%|████████▏ | 753/920 [2:14:03<29:47, 10.70s/it] 82%|████████▏ | 754/920 [2:14:14<29:39, 10.72s/it] 82%|████████▏ | 755/920 [2:14:25<29:29, 10.72s/it] 82%|████████▏ | 756/920 [2:14:35<29:19, 10.73s/it] 82%|████████▏ | 757/920 [2:14:46<28:57, 10.66s/it] 82%|████████▏ | 758/920 [2:14:57<28:52, 10.69s/it] 82%|████████▎ | 759/920 [2:15:07<28:42, 10.70s/it] 83%|████████▎ | 760/920 [2:15:18<28:35, 10.72s/it] 83%|████████▎ | 761/920 [2:15:29<28:27, 10.74s/it] 83%|████████▎ | 762/920 [2:15:40<28:17, 10.74s/it] 83%|████████▎ | 763/920 [2:15:50<28:06, 10.74s/it] 83%|████████▎ | 764/920 [2:16:01<27:54, 10.74s/it] 83%|████████▎ | 765/920 [2:16:12<27:40, 10.72s/it] 83%|████████▎ | 766/920 [2:16:22<27:27, 10.70s/it] 83%|████████▎ | 767/920 [2:16:33<27:22, 10.73s/it] 83%|████████▎ | 768/920 [2:16:44<27:13, 10.75s/it] 84%|████████▎ | 769/920 [2:16:55<27:05, 10.77s/it] 84%|████████▎ | 770/920 [2:17:06<26:49, 10.73s/it] 84%|████████▍ | 771/920 [2:17:16<26:39, 10.73s/it] 84%|████████▍ | 772/920 [2:17:27<26:26, 10.72s/it] 84%|████████▍ | 773/920 [2:17:37<26:06, 10.66s/it] 84%|████████▍ | 774/920 [2:17:48<26:01, 10.70s/it] 84%|████████▍ | 775/920 [2:17:59<25:53, 10.71s/it] 84%|████████▍ | 776/920 [2:18:10<25:43, 10.72s/it] 84%|████████▍ | 777/920 [2:18:20<25:31, 10.71s/it] 85%|████████▍ | 778/920 [2:18:31<25:19, 10.70s/it] 85%|████████▍ | 779/920 [2:18:42<25:08, 10.70s/it] 85%|████████▍ | 780/920 [2:18:52<24:53, 10.67s/it] 85%|████████▍ | 781/920 [2:19:03<24:39, 10.64s/it] 85%|████████▌ | 782/920 [2:19:14<24:32, 10.67s/it] 85%|████████▌ | 783/920 [2:19:25<24:27, 10.71s/it] 85%|████████▌ | 784/920 [2:19:35<24:13, 10.69s/it] 85%|████████▌ | 785/920 [2:19:46<24:08, 10.73s/it] 85%|████████▌ | 786/920 [2:19:57<23:54, 10.71s/it] 86%|████████▌ | 787/920 [2:20:07<23:46, 10.72s/it] 86%|████████▌ | 788/920 [2:20:18<23:35, 10.72s/it] 86%|████████▌ | 789/920 [2:20:29<23:25, 10.73s/it] 86%|████████▌ | 790/920 [2:20:40<23:11, 10.70s/it] 86%|████████▌ | 791/920 [2:20:50<22:56, 10.67s/it] 86%|████████▌ | 792/920 [2:21:01<22:50, 10.71s/it] 86%|████████▌ | 793/920 [2:21:12<22:37, 10.69s/it] 86%|████████▋ | 794/920 [2:21:22<22:26, 10.69s/it] 86%|████████▋ | 795/920 [2:21:33<22:17, 10.70s/it] 87%|████████▋ | 796/920 [2:21:44<22:08, 10.71s/it] 87%|████████▋ | 797/920 [2:21:54<21:59, 10.73s/it] 87%|████████▋ | 798/920 [2:22:05<21:45, 10.70s/it] 87%|████████▋ | 799/920 [2:22:16<21:38, 10.73s/it] 87%|████████▋ | 800/920 [2:22:27<21:26, 10.72s/it]                                                    87%|████████▋ | 800/920 [2:22:27<21:26, 10.72s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 87%|████████▋ | 801/920 [2:22:39<22:07, 11.16s/it] 87%|████████▋ | 802/920 [2:22:50<21:42, 11.04s/it] 87%|████████▋ | 803/920 [2:23:00<21:22, 10.96s/it] 87%|████████▋ | 804/920 [2:23:11<20:59, 10.86s/it] 88%|████████▊ | 805/920 [2:23:22<20:43, 10.81s/it] 88%|████████▊ | 806/920 [2:23:32<20:30, 10.79s/it] 88%|████████▊ | 807/920 [2:23:43<20:18, 10.78s/it] 88%|████████▊ | 808/920 [2:23:54<20:08, 10.79s/it] 88%|████████▊ | 809/920 [2:24:05<20:03, 10.84s/it] 88%|████████▊ | 810/920 [2:24:16<19:49, 10.81s/it] 88%|████████▊ | 811/920 [2:24:26<19:35, 10.78s/it] 88%|████████▊ | 812/920 [2:24:37<19:15, 10.70s/it] 88%|████████▊ | 813/920 [2:24:47<19:01, 10.67s/it] 88%|████████▊ | 814/920 [2:24:58<18:46, 10.62s/it] 89%|████████▊ | 815/920 [2:25:09<18:32, 10.60s/it] 89%|████████▊ | 816/920 [2:25:19<18:27, 10.65s/it] 89%|████████▉ | 817/920 [2:25:30<18:19, 10.67s/it] 89%|████████▉ | 818/920 [2:25:41<18:10, 10.69s/it] 89%|████████▉ | 819/920 [2:25:52<18:01, 10.70s/it] 89%|████████▉ | 820/920 [2:26:02<17:52, 10.73s/it] 89%|████████▉ | 821/920 [2:26:13<17:41, 10.72s/it] 89%|████████▉ | 822/920 [2:26:24<17:30, 10.72s/it] 89%|████████▉ | 823/920 [2:26:34<17:20, 10.73s/it] 90%|████████▉ | 824/920 [2:26:45<17:13, 10.77s/it] 90%|████████▉ | 825/920 [2:26:56<17:03, 10.78s/it] 90%|████████▉ | 826/920 [2:27:07<16:52, 10.77s/it] 90%|████████▉ | 827/920 [2:27:18<16:39, 10.75s/it] 90%|█████████ | 828/920 [2:27:28<16:27, 10.74s/it] 90%|█████████ | 829/920 [2:27:39<16:17, 10.74s/it] 90%|█████████ | 830/920 [2:27:50<16:05, 10.73s/it] 90%|█████████ | 831/920 [2:28:00<15:55, 10.73s/it] 90%|█████████ | 832/920 [2:28:11<15:46, 10.76s/it] 91%|█████████ | 833/920 [2:28:22<15:35, 10.75s/it] 91%|█████████ | 834/920 [2:28:33<15:24, 10.75s/it] 91%|█████████ | 835/920 [2:28:43<15:13, 10.74s/it] 91%|█████████ | 836/920 [2:28:54<15:01, 10.73s/it] 91%|█████████ | 837/920 [2:29:05<14:50, 10.73s/it] 91%|█████████ | 838/920 [2:29:16<14:39, 10.73s/it] 91%|█████████ | 839/920 [2:29:26<14:27, 10.71s/it] 91%|█████████▏| 840/920 [2:29:37<14:17, 10.72s/it] 91%|█████████▏| 841/920 [2:29:48<14:08, 10.74s/it] 92%|█████████▏| 842/920 [2:29:59<13:57, 10.73s/it] 92%|█████████▏| 843/920 [2:30:09<13:48, 10.76s/it] 92%|█████████▏| 844/920 [2:30:20<13:37, 10.75s/it] 92%|█████████▏| 845/920 [2:30:31<13:25, 10.74s/it] 92%|█████████▏| 846/920 [2:30:42<13:13, 10.72s/it] 92%|█████████▏| 847/920 [2:30:52<13:03, 10.73s/it] 92%|█████████▏| 848/920 [2:31:03<12:53, 10.74s/it] 92%|█████████▏| 849/920 [2:31:14<12:43, 10.75s/it] 92%|█████████▏| 850/920 [2:31:25<12:31, 10.74s/it] 92%|█████████▎| 851/920 [2:31:35<12:22, 10.76s/it] 93%|█████████▎| 852/920 [2:31:46<12:10, 10.75s/it] 93%|█████████▎| 853/920 [2:31:57<11:58, 10.72s/it] 93%|█████████▎| 854/920 [2:32:07<11:48, 10.74s/it] 93%|█████████▎| 855/920 [2:32:18<11:38, 10.74s/it] 93%|█████████▎| 856/920 [2:32:29<11:28, 10.75s/it] 93%|█████████▎| 857/920 [2:32:40<11:16, 10.74s/it] 93%|█████████▎| 858/920 [2:32:50<11:05, 10.73s/it] 93%|█████████▎| 859/920 [2:33:01<10:53, 10.72s/it] 93%|█████████▎| 860/920 [2:33:12<10:43, 10.73s/it] 94%|█████████▎| 861/920 [2:33:23<10:32, 10.72s/it] 94%|█████████▎| 862/920 [2:33:33<10:21, 10.72s/it] 94%|█████████▍| 863/920 [2:33:44<10:11, 10.73s/it] 94%|█████████▍| 864/920 [2:33:55<10:01, 10.74s/it] 94%|█████████▍| 865/920 [2:34:05<09:49, 10.73s/it] 94%|█████████▍| 866/920 [2:34:16<09:37, 10.70s/it] 94%|█████████▍| 867/920 [2:34:27<09:24, 10.66s/it] 94%|█████████▍| 868/920 [2:34:37<09:15, 10.68s/it] 94%|█████████▍| 869/920 [2:34:48<09:04, 10.68s/it] 95%|█████████▍| 870/920 [2:34:59<08:55, 10.71s/it] 95%|█████████▍| 871/920 [2:35:10<08:44, 10.71s/it] 95%|█████████▍| 872/920 [2:35:20<08:35, 10.73s/it] 95%|█████████▍| 873/920 [2:35:31<08:26, 10.77s/it] 95%|█████████▌| 874/920 [2:35:42<08:15, 10.77s/it] 95%|█████████▌| 875/920 [2:35:53<08:04, 10.77s/it] 95%|█████████▌| 876/920 [2:36:03<07:52, 10.74s/it] 95%|█████████▌| 877/920 [2:36:14<07:40, 10.71s/it] 95%|█████████▌| 878/920 [2:36:25<07:30, 10.74s/it] 96%|█████████▌| 879/920 [2:36:36<07:20, 10.74s/it] 96%|█████████▌| 880/920 [2:36:46<07:09, 10.73s/it] 96%|█████████▌| 881/920 [2:36:57<06:58, 10.73s/it] 96%|█████████▌| 882/920 [2:37:08<06:47, 10.72s/it] 96%|█████████▌| 883/920 [2:37:18<06:35, 10.70s/it] 96%|█████████▌| 884/920 [2:37:29<06:25, 10.72s/it] 96%|█████████▌| 885/920 [2:37:40<06:15, 10.73s/it] 96%|█████████▋| 886/920 [2:37:51<06:04, 10.73s/it] 96%|█████████▋| 887/920 [2:38:01<05:54, 10.73s/it] 97%|█████████▋| 888/920 [2:38:12<05:44, 10.76s/it] 97%|█████████▋| 889/920 [2:38:23<05:33, 10.76s/it] 97%|█████████▋| 890/920 [2:38:34<05:22, 10.75s/it] 97%|█████████▋| 891/920 [2:38:44<05:11, 10.74s/it] 97%|█████████▋| 892/920 [2:38:55<05:00, 10.73s/it] 97%|█████████▋| 893/920 [2:39:06<04:50, 10.76s/it] 97%|█████████▋| 894/920 [2:39:17<04:39, 10.73s/it] 97%|█████████▋| 895/920 [2:39:27<04:27, 10.71s/it] 97%|█████████▋| 896/920 [2:39:38<04:17, 10.73s/it] 98%|█████████▊| 897/920 [2:39:49<04:06, 10.72s/it] 98%|█████████▊| 898/920 [2:39:59<03:54, 10.64s/it] 98%|█████████▊| 899/920 [2:40:10<03:42, 10.62s/it] 98%|█████████▊| 900/920 [2:40:21<03:33, 10.66s/it]                                                    98%|█████████▊| 900/920 [2:40:21<03:33, 10.66s/it]/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 98%|█████████▊| 901/920 [2:40:33<03:30, 11.09s/it] 98%|█████████▊| 902/920 [2:40:43<03:17, 10.96s/it] 98%|█████████▊| 903/920 [2:40:54<03:05, 10.90s/it] 98%|█████████▊| 904/920 [2:41:05<02:53, 10.86s/it] 98%|█████████▊| 905/920 [2:41:16<02:42, 10.82s/it] 98%|█████████▊| 906/920 [2:41:26<02:31, 10.82s/it] 99%|█████████▊| 907/920 [2:41:37<02:20, 10.80s/it] 99%|█████████▊| 908/920 [2:41:48<02:09, 10.79s/it] 99%|█████████▉| 909/920 [2:41:59<01:58, 10.76s/it] 99%|█████████▉| 910/920 [2:42:09<01:47, 10.75s/it] 99%|█████████▉| 911/920 [2:42:20<01:36, 10.75s/it] 99%|█████████▉| 912/920 [2:42:31<01:25, 10.74s/it] 99%|█████████▉| 913/920 [2:42:41<01:15, 10.73s/it] 99%|█████████▉| 914/920 [2:42:52<01:04, 10.72s/it] 99%|█████████▉| 915/920 [2:43:03<00:53, 10.73s/it]100%|█████████▉| 916/920 [2:43:14<00:42, 10.73s/it]100%|█████████▉| 917/920 [2:43:24<00:32, 10.73s/it]100%|█████████▉| 918/920 [2:43:35<00:21, 10.74s/it]100%|█████████▉| 919/920 [2:43:46<00:10, 10.73s/it]100%|██████████| 920/920 [2:43:57<00:00, 10.74s/it]                                                   100%|██████████| 920/920 [2:43:57<00:00, 10.74s/it]100%|██████████| 920/920 [2:43:57<00:00, 10.69s/it]
/home/namwoam/miniforge3/envs/dl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 2.3997, 'grad_norm': 1.6168519258499146, 'learning_rate': 2.757847533632287e-05, 'epoch': 2.14}
{'loss': 1.5237, 'grad_norm': 3.183507204055786, 'learning_rate': 2.42152466367713e-05, 'epoch': 4.27}
{'loss': 1.1097, 'grad_norm': 2.0663468837738037, 'learning_rate': 2.0852017937219733e-05, 'epoch': 6.41}
{'loss': 0.8114, 'grad_norm': 2.8198471069335938, 'learning_rate': 1.7488789237668162e-05, 'epoch': 8.55}
{'loss': 0.6016, 'grad_norm': 2.935614824295044, 'learning_rate': 1.4125560538116594e-05, 'epoch': 10.68}
{'loss': 0.4717, 'grad_norm': 2.9803237915039062, 'learning_rate': 1.0762331838565022e-05, 'epoch': 12.82}
{'loss': 0.3736, 'grad_norm': 3.495419979095459, 'learning_rate': 7.399103139013453e-06, 'epoch': 14.96}
{'loss': 0.309, 'grad_norm': 2.9727730751037598, 'learning_rate': 4.0358744394618836e-06, 'epoch': 17.09}
{'loss': 0.2752, 'grad_norm': 2.821988821029663, 'learning_rate': 6.726457399103139e-07, 'epoch': 19.23}
{'train_runtime': 9837.1289, 'train_samples_per_second': 0.951, 'train_steps_per_second': 0.094, 'train_loss': 0.8618715249973795, 'epoch': 19.66}
